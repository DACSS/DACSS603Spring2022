[
  {
    "path": "posts/httpsrpubscommollyhackbarthdacss-603-homework-4/",
    "title": "DACSS 603 Homework 4",
    "description": "Homework 4 for DACSS 603",
    "author": [
      {
        "name": "Molly Hackbarth",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\n\nContents\nPART 1\nQuestion 1\nA\nB\nC\nD\nE\n\nQuestion 2\nA\nB\n\nQuestion 3\n\nPART 2 (Final Project)\n1\n2\n3\n\n\nPART 1\n(Data file: house.selling.price.2 from smss R package) For the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.\nQuestion 1\n(SMSS 14.3, 14.4, merged & modified) Price\n\n\n#create matrix\ntab <- matrix(c(1, 0.899, 0.590, 0.714,  0.357,\n                0.899, 1, 0.669,  0.662, 0.176,\n                0.590, 0.699,  1, 0.334, 0.267,\n                0.714, 0.662,  0.334, 1, 0.182,\n                0.357, 0.176, 0.267, 0.182, 1), ncol=5, nrow = 5)\n\n#define column names and row names of matrix\ncolnames(tab) <- c('Price', 'Size', 'Bed', 'Bath', 'New')\nrownames(tab) <- c('Price', 'Size', 'Bed', 'Bath', 'New')\n\n#convert matrix to table \ntab <- as.table(tab)\n\n#view table \ntab\n\n\n      Price  Size   Bed  Bath   New\nPrice 1.000 0.899 0.590 0.714 0.357\nSize  0.899 1.000 0.699 0.662 0.176\nBed   0.590 0.669 1.000 0.334 0.267\nBath  0.714 0.662 0.334 1.000 0.182\nNew   0.357 0.176 0.267 0.182 1.000\n\ntab2 <- matrix(c(-41.795, 64.761,-2.766, 19.203, 18.984,\n                 12.104, 5.630, 3.960, 5.650, 3.873, \n                 -3.453, 11.504, -0.698,  3.399, 4.902,\n                 0.001, 0, 0.487, 0.001, 0.00000), ncol=4, nrow = 5)\n\n#define column names and row names of matrix\ncolnames(tab2) <- c('Estimate', 'Std. Error', 't value', 'Pr(> | t| )')\nrownames(tab2) <- c('Intercept', 'Size', 'Bed', 'Bath', 'New')\n\n#convert matrix to table \ntab2 <- as.table(tab2)\n\n#view table \ntab2\n\n\n          Estimate Std. Error t value Pr(> | t| )\nIntercept  -41.795     12.104  -3.453       0.001\nSize        64.761      5.630  11.504       0.000\nBed         -2.766      3.960  -0.698       0.487\nBath        19.203      5.650   3.399       0.001\nNew         18.984      3.873   4.902       0.000\n\nA\nWith these four predictors,\nA. For backward elimination, which variable would be deleted first? Why?\nFor backwards elimination the first variable to be deleted would be BED. This is because BED has the largest p-value. Compared to the variables SIZE, BATH, and NEW, the variable BED has the largest p-value and would fail to reject the null hypothesis that BED is statistically significant in determining the selling price of the home.\nB\nB. For forward selection, which variable would be added first? Why?\nFor forwards selection we would add the variable SIZE. Due to the p-values for SIZE and NEW both being 0.000 we will look to the correlation matrix. When looking at the correlation matrix we see SIZE has a higher correlation to selling price at .90 out of 1 compared to NEW being .36 out of 1.\nC\nC. Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\nI believe the variable BEDS has a large p-value in the multiple regression model because the model may be too complex with too many variables. If there are too many variables we are unable to theorize about causality or the relationship between variables. The sample size being very small also can be a factor in having a high p-value and a high correlation. When we run the model we see that there are only 93 observations, which is a very small sample size.\nD\nD. Using software with these four predictors, find the model that would be selected using each criterion: a. R2 b. Adjusted R2 c. PRESS d. AIC e. BIC\nThe model for R2 and Adjusted R2 would be a multiple regression model using the function lm() and summary().\nIn order to find the PRESS we would use our multiple regression model “house” in the function PRESS(). To use the PRESS() we will used the created function.\nIn order to find the AIC we would use our multiple regression model “house” in the function AIC()\nIn order to find the BIC we would use our multiple regression model “house” in the function BIC()\n\n\ndata(\"house.selling.price.2\")\nhouse <- lm(P ~ Be + Ba + New + S, data = house.selling.price.2)\n\nsummary(house)\n\n\n\nCall:\nlm(formula = P ~ Be + Ba + New + S, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\nS             64.761      5.630  11.504  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\nAIC(house)\n\n\n[1] 790.6225\n\nBIC(house)\n\n\n[1] 805.8181\n\nPRESS <- function(linear.model) {\n    pr <- residuals(linear.model)/(1 - lm.influence(linear.model)$hat)\n    sum(pr^2)\n}\n\nfit <- lm(P ~ Be + Ba + New + S, data = house.selling.price.2)\nPRESS(fit)\n\n\n[1] 28390.22\n\nE\nE. Explain which model you prefer and why.\nI prefer the AIC and BIC models as they both penalize the addition of new variables. Additionally they both want the lowest value possible to have a better-fit (parsimonious) model. This makes it easy to rerun test with multiple variables to see when adding a new variable would increase the score instead of decreasing it. Seeing the AIC or BIC score increase also lets us know when we should stop adding new variables. Additionally running both the AIC and BIC allow us to receive two scores and pick the lower score of the two.\nQuestion 2\n(Data file: trees from base R) From the documentation: “This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\nA\nA. fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\nTo do the multiple regression model I used the function lm().\n\n\ntree <- lm(Volume ~ Girth + Height, data=trees)\n\n\n\nB\nB. Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\nTo run the diagnostic plots I will use the function plot(). I will also use the function par() to align the graphs.\n\n\npar(mfrow = c(2,3)); plot(tree, which = 1:6)\n\n\n\n\nBased on the plots I believe that the following regression assumptions are violated for residuals vs fitted plot:\nLinearity: The residuals should form roughly a straight line.\nConstant variance: The residuals should form roughly a horizontal band.\nBoth of these assumptions are violated in the plot.\nI believe the following regression assumptions for Scale-Location plot are also violated:\nConstant variance: The residuals should form roughly a horizontal band with equally spread points.\nThe red line is approximately horizontal\nBoth of these assumptions are violated in the plot.\nIn addition the Cook’s Distance vs Leverage plot shows a highly influential data point of data item 31.\nQuestion 3\n(inspired by ALR 9.16) (Data file: florida in alr R package) In the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\nTo run a linear regression I used lm(), for regression diagnostic plots I used plot() along with par() to align the graphics.\n\n\ndata(\"florida\")\n\nvote <- lm(Buchanan ~ Bush, data=florida)\n\npar(mfrow = c(2,3)); plot(vote, which = 1:6)\n\n\n\n\nAfter looking at the diagnostic plots, Palm Beach County is an outlier. The Palm Beach residual is the only residuals that violates the regression assumptions for most of the plots. One exception is the Dade residual also appears to violate some of the assumptions.\nIn the Residuals vs Fitted plot Palm Beach County’s residual violates the linearity assumption by being far away from the other residuals. It is much higher up.\nIn the Normal Q-Q plot Palm Beach County’s residual violates the normality assumption by not falling around the line.\nIn the Scale-Location plot Palm Beach County’s residual violates the homoskedasticity by the spreading wider and further than the rest of the residuals.\nIn the Residuals vs Leverage plot Palm Beach County’s residual violates the influential observation by having the residual being outside the red dashed line. Thus Palm Beach County can be influential against a regression line.\nIn the Cook’s Distance plot, the Cook’s distance for the Palm Beach County observation is larger than 1 and also larger than 4/67 (.06). This indicates that Palm Beach County is an outlier.\nPART 2 (Final Project)\n1\n1. What is your research question for the final project?\nMy research question: Is age statistically significant to views on homosexual sexual relationships, same sex marriage, and abortions when including if you’re a democrat or republican?\n2\n2. What is your hypothesis (i.e. an answer to the research question) that you want to test?\nMy Hypothesis to be tested: My hypothesis to be tested is that age is not statistically significant in predicting views about homosexual sexual relationships, homosexual marriage, and abortions.\n3\n3. Present some exploratory analysis. In particular:\nA\na. Numerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables).\nBelow I have numerically summarized the variables. However due to the variables ABANY, HOMOSEX, MARHOMO, and PARTY ID being categorical variables, the summary does not explain as much.\n\n\n# Check if \"drat\" package is in your library and if it's not this will download for you\nif (!require(\"drat\")) {\n    install.packages(\"drat\")\n    library(\"drat\")\n}\n\n# Finds the repository that \"drat\" is in\ndrat::addRepo(\"kjhealy\")\n\n# Installs the package \"gssr\" and use library function\nif (!require(\"gssr\")) {\n    install.packages(\"gssr\")\n    library(\"gssr\")\n}\n\n\ndata(gss_doc)\ngss21 <- gss_get_yr(2021)\n\ngssmh <- gss21 %>% \n  dplyr::select(age, partyid, homosex, marhomo, abany) %>% \n  dplyr::filter_at(vars(age, partyid),\n            all_vars(!is.na(.))) %>% \n  dplyr::filter(partyid == 5 | partyid <=1 | partyid == 6)\n\ninfo <- gss_get_marginals(varnames = c(\"age\", \"partyid\", \n                                       \"homosex\", \"marhomo\", \"abany\"))\n\npaged_table(info)\n\n\n\n\n{\"columns\":[{\"label\":[\"variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"cases\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"range\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"percent\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"value\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"label\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"age\",\"2\":\"Total Cases:\",\"3\":\"64,814 (Range of valid codes: 18-89)\",\"4\":\"AGE\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"16.1\",\"6\":\"10378\",\"7\":\"0\",\"8\":\"STRONG DEMOCRAT\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"20.6\",\"6\":\"13294\",\"7\":\"1\",\"8\":\"NOT STR DEMOCRAT\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"12.1\",\"6\":\"7792\",\"7\":\"2\",\"8\":\"IND,NEAR DEM\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"15.4\",\"6\":\"9888\",\"7\":\"3\",\"8\":\"INDEPENDENT\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"8.9\",\"6\":\"5721\",\"7\":\"4\",\"8\":\"IND,NEAR REP\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"15.4\",\"6\":\"9933\",\"7\":\"5\",\"8\":\"NOT STR REPUBLICAN\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"9.8\",\"6\":\"6318\",\"7\":\"6\",\"8\":\"STRONG REPUBLICAN\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"1.7\",\"6\":\"1072\",\"7\":\"7\",\"8\":\"OTHER PARTY\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"NA\",\"6\":\"11\",\"7\":\"8\",\"8\":\"DK\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"NA\",\"6\":\"407\",\"7\":\"9\",\"8\":\"NA\"},{\"1\":\"partyid\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"PARTYID\",\"5\":\"100.0\",\"6\":\"64814\",\"7\":\"NA\",\"8\":\"Total\"},{\"1\":\"abany\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"ABANY\",\"5\":\"41.4\",\"6\":\"15234\",\"7\":\"1\",\"8\":\"YES\"},{\"1\":\"abany\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"ABANY\",\"5\":\"58.6\",\"6\":\"21560\",\"7\":\"2\",\"8\":\"NO\"},{\"1\":\"abany\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"ABANY\",\"5\":\"NA\",\"6\":\"26398\",\"7\":\"0\",\"8\":\"IAP\"},{\"1\":\"abany\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"ABANY\",\"5\":\"NA\",\"6\":\"1341\",\"7\":\"8\",\"8\":\"DK\"},{\"1\":\"abany\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"ABANY\",\"5\":\"NA\",\"6\":\"281\",\"7\":\"9\",\"8\":\"NA\"},{\"1\":\"abany\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"ABANY\",\"5\":\"100.0\",\"6\":\"64814\",\"7\":\"NA\",\"8\":\"Total\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"62.2\",\"6\":\"23469\",\"7\":\"1\",\"8\":\"ALWAYS WRONG\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"4.7\",\"6\":\"1756\",\"7\":\"2\",\"8\":\"ALMST ALWAYS WRG\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"6.8\",\"6\":\"2554\",\"7\":\"3\",\"8\":\"SOMETIMES WRONG\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"26.2\",\"6\":\"9880\",\"7\":\"4\",\"8\":\"NOT WRONG AT ALL\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"0.2\",\"6\":\"82\",\"7\":\"5\",\"8\":\"OTHER\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"NA\",\"6\":\"25042\",\"7\":\"0\",\"8\":\"IAP\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"NA\",\"6\":\"1743\",\"7\":\"8\",\"8\":\"DK\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"NA\",\"6\":\"288\",\"7\":\"9\",\"8\":\"NA\"},{\"1\":\"homosex\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"HOMOSEX\",\"5\":\"100.0\",\"6\":\"64814\",\"7\":\"NA\",\"8\":\"Total\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"23.0\",\"6\":\"3093\",\"7\":\"1\",\"8\":\"STRONGLY AGREE\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"22.0\",\"6\":\"2966\",\"7\":\"2\",\"8\":\"AGREE\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"12.5\",\"6\":\"1684\",\"7\":\"3\",\"8\":\"NEITHER AGREE NOR DISAGREE\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"15.5\",\"6\":\"2086\",\"7\":\"4\",\"8\":\"DISAGREE\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"27.0\",\"6\":\"3635\",\"7\":\"5\",\"8\":\"STRONGLY DISAGREE\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"NA\",\"6\":\"51062\",\"7\":\"0\",\"8\":\"IAP\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"NA\",\"6\":\"202\",\"7\":\"8\",\"8\":\"CANT CHOOSE\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"NA\",\"6\":\"86\",\"7\":\"9\",\"8\":\"NA\"},{\"1\":\"marhomo\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"MARHOMO\",\"5\":\"100.0\",\"6\":\"64814\",\"7\":\"NA\",\"8\":\"Total\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\nsummary(gssmh)\n\n\n      age           partyid        homosex         marhomo     \n Min.   :18.00   Min.   :0.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.:40.00   1st Qu.:0.00   1st Qu.:1.000   1st Qu.:1.000  \n Median :55.00   Median :1.00   Median :4.000   Median :2.000  \n Mean   :53.81   Mean   :2.46   Mean   :3.059   Mean   :2.139  \n 3rd Qu.:67.00   3rd Qu.:5.00   3rd Qu.:4.000   3rd Qu.:3.000  \n Max.   :89.00   Max.   :6.00   Max.   :4.000   Max.   :5.000  \n                                NA's   :708     NA's   :687    \n     abany      \n Min.   :1.000  \n 1st Qu.:1.000  \n Median :1.000  \n Mean   :1.409  \n 3rd Qu.:2.000  \n Max.   :2.000  \n NA's   :1368   \n\nFor the variable AGE we can see the minimum age is 18, the maximum age is 89 and the mean age is about 54.\nFor the variable PARTY ID we can see the minimum is 0 (Strongly Democrat), the maximum is 6 (Strongly Republican), and the mean is 2.46 which would be leaning close to “Not Strongly Democratic”. In order to do a study on only Democrats and Republicans, Independent participants were removed (2-4).\nFor the variable HOMOSEX (Question: “219. What about sexual relations between two adults of the same sex?”) we can see the minimum is 1 (Always Wrong), the maximum is 4 (Not Wrong at All) and the mean is 3.06 which would be leaning close to “Sometimes Wrong”.\nFor the variable MARHOMO (Question: “1280. Do you agree or disagree? j. Homosexual couples should have the right to marry one another.”) we can see the minimum is 1 (Strongly Agree), the maximum is 5 (Strongly Disagree) and the mean is 2.14 which would be leaning close to “Agree”.\nFor the variable ABANY (Question: “206. Please tell me whether or not you think it should be possible for a pregnant woman to obtain a legal abortion if: g. The woman wants it for any reason?”) we can see the minimum is 1 (Yes), the maximum is 2 (No) and the mean is 1.41 which would be leaning slightly closer to “Yes”.\nI also ran the following linear models below.\n\n\nhomosexpa <- lm(homosex ~ partyid + age + partyid*age, data = gssmh)\n\nmarhomopa <- lm(marhomo ~ partyid + age + partyid*age, data = gssmh)\n\nabandypa <- lm(abany ~ partyid + age + partyid*age, data = gssmh)\n\nstargazer(homosexpa, marhomopa, abandypa, type = 'text', \n          dep.var.labels = c('Homosexual Sex Relationships', \n                             'Homosexual Marriage', 'Abortion'),\n          covariate.labels = c('Party ID', 'Age', 'Party ID: Age')\n          )\n\n\n\n==================================================================================================\n                                                 Dependent variable:                              \n                    ------------------------------------------------------------------------------\n                    Homosexual Sex Relationships    Homosexual Marriage           Abortion        \n                                (1)                         (2)                      (3)          \n--------------------------------------------------------------------------------------------------\nParty ID                     -0.179***                   0.199***                 0.098***        \n                              (0.044)                     (0.044)                  (0.022)        \n                                                                                                  \nAge                          -0.010***                   0.011***                  0.002*         \n                              (0.002)                     (0.002)                  (0.001)        \n                                                                                                  \nParty ID: Age                 -0.0001                      0.001                   -0.0004        \n                              (0.001)                     (0.001)                 (0.0004)        \n                                                                                                  \nConstant                      4.025***                   0.978***                 1.089***        \n                              (0.139)                     (0.140)                  (0.072)        \n                                                                                                  \n--------------------------------------------------------------------------------------------------\nObservations                   1,416                       1,437                     756          \nR2                             0.153                       0.234                    0.170         \nAdjusted R2                    0.151                       0.232                    0.167         \nResidual Std. Error      1.209 (df = 1412)           1.220 (df = 1433)        0.449 (df = 752)    \nF Statistic           84.900*** (df = 3; 1412)   145.879*** (df = 3; 1433) 51.506*** (df = 3; 752)\n==================================================================================================\nNote:                                                                  *p<0.1; **p<0.05; ***p<0.01\n\nFor these summaries, I used stargazer and a 5% significance level for the analyses.\nFor homosexual sexual relationships, the p-value for variable for AGE has 3 asterisks. From the function stargazer() we can conclude that the p-value was between 0 and 0.01. With the p-value being under the .05 significance level we reject the null hypothesis that age is not statistically significant in predicting views about homosexual sexual relationships.\nFor homosexual marriage, the p-value for the variable AGE has 3 asterisks. From the function stargazer() we can conclude that the p-value was between 0 and 0.01. With the p-value being under the .05 significance level we reject the null hypothesis that age is not statistically significant in predicting views about homosexual marriage.\nFor views on abortion, the p-value for the variable AGE has 1 asterisks. From the function stargazer() we can conclude that the p-value was between 0.05 and 0.1. With the p-value being above the .05 significance level we fail to reject the null hypothesis that age is not statistically significant in predicting views about abortion.\nOverall we can see that for views on homosexual sexual relationships, homosexual marriage, and abortion that the variable PARTY ID has 3 asterisks. From the function stargazer() we can conclude that the p-value is less than .01. With the p-value being below the .05 significance level. Thus in relation to views on homosexual sexual relationships, homosexual marriage, and abortion PARTY ID has a significant impact.\nComparatively when we look at the interaction term PARTY ID * AGE, the interaction term has no asterisks. From the function stargazer() we can conclude that the p-value was above 0.1. With the p-value being above the .05 significance level. Thus in relation to views on homosexual sexual relationships, homosexual marriage, and abortion PARTY ID * AGE has an insignificant impact.\nTo test further I made a dummy variable where you are either a democrat (0) or republican (1). I then performed lm() again.\n\n\ngssmh$dr <- ifelse(gssmh$partyid <=3, 0, 1)\n\nhomosexpa2 <- lm(homosex ~ dr + age + dr*age , data = gssmh)\n\nmarhomopa2 <- lm(marhomo ~ dr + age + dr*age, data = gssmh)\n\nabandypa2 <- lm(abany ~ dr + age + dr*age, data = gssmh)\n\nstargazer(homosexpa2, marhomopa2, abandypa2, type = 'text', \n          dep.var.labels = c('Homosexual Sex Relationships', \n                             'Homosexual Marriage', 'Abortion'), \n          covariate.labels = c('Party ID', 'Age', 'Party ID: Age')\n          )\n\n\n\n==================================================================================================\n                                                 Dependent variable:                              \n                    ------------------------------------------------------------------------------\n                    Homosexual Sex Relationships    Homosexual Marriage           Abortion        \n                                (1)                         (2)                      (3)          \n--------------------------------------------------------------------------------------------------\nParty ID                     -0.862***                   0.913***                 0.466***        \n                              (0.227)                     (0.229)                  (0.114)        \n                                                                                                  \nAge                          -0.009***                   0.010***                   0.002         \n                              (0.002)                     (0.002)                  (0.001)        \n                                                                                                  \nParty ID: Age                  -0.001                      0.006                   -0.001         \n                              (0.004)                     (0.004)                  (0.002)        \n                                                                                                  \nConstant                      3.915***                   1.118***                 1.153***        \n                              (0.128)                     (0.129)                  (0.066)        \n                                                                                                  \n--------------------------------------------------------------------------------------------------\nObservations                   1,416                       1,437                     756          \nR2                             0.144                       0.221                    0.164         \nAdjusted R2                    0.142                       0.220                    0.161         \nResidual Std. Error      1.215 (df = 1412)           1.230 (df = 1433)        0.451 (df = 752)    \nF Statistic           79.241*** (df = 3; 1412)   135.741*** (df = 3; 1433) 49.127*** (df = 3; 752)\n==================================================================================================\nNote:                                                                  *p<0.1; **p<0.05; ***p<0.01\n\nFor homosexual sexual relationships, the p-value for the variable AGE has 3 asterisks. From the function stargazer() we can conclude that the p-value was between 0 and 0.01. With the p-value being under the .05 significance level we reject the null hypothesis that age is not statistically significant in predicting views about homosexual sexual relationships.\nFor homosexual marriage, the p-value for the variable AGE has 3 asterisks. From the function stargazer() we can conclude that the p-value was between 0 and 0.01. With the p-value being under the .05 significance level we reject the null hypothesis that age is not statistically significant in predicting views about homosexual marriage.\nFor views on abortion, the p-value for the variable AGE has no asterisks. From the function stargazer() we can conclude that the p-value was above 0.1. With the p-value being above the .05 significance level we fail to reject the null hypothesis that age is not statistically significant in predicting views about abortion.\nOverall we can see that for views on homosexual sexual relationships, homosexual marriage, and abortion that the variable PARTY ID has 3 asterisks. From the function stargazer() we can conclude that the p-value was below 0.1. With the p-value being below the .05 significance level. Thus in relation to views on homosexual sexual relationships, homosexual marriage, and abortion PARTY ID has a significant impact.\nComparatively when we look at the interaction term PARTY ID * AGE, the interaction term has no asterisks. From the function stargazer() we can conclude that the p-value was above 0.1. With the p-value being above the .05 significance level. Thus in relation to views on homosexual sexual relationships, homosexual marriage, and abortion PARTY ID* AGE has an insignificant impact.\nB\nb. Plot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots.\nBelow is the three plots I will use for my final project. The size of each bubble is based on the count of people’s age and the response picked for the variable (HOMOSEX, MARHOMO, ABANY). The color shows the dummy variable DR where 0 is “DEMOCRAT” and 1 is “REPUBLICAN”.\n\n\ngssmhhs <- gssmh%>%group_by(age, homosex)%>%mutate(count=n())\ngssmhms <- gssmh%>%group_by(age, marhomo)%>%mutate(count=n())\ngssmhab <- gssmh%>%group_by(age, abany)%>%mutate(count=n())\n\n\n\n\n\np1 <- ggplot(gssmhhs, aes(x=age, y=homosex, \n                           color=dr)) +\n  geom_point() +\n   labs(title= \"Age compared to attitudes of homosexual relationships\",\n        x= \"Age of participant\", \n        y = \"Attitudes towards homosexual relationships\") +\n   theme_minimal()\n\np1 + geom_point(aes(size = count))\n\n\n\n\n\nThis graph shows:\nx-axis: Age of the respondent\ny-axis: Attitudes towards homosexual sexual relationships with the mimimum 1 (Always Wrong) and the maximum is 4 (Not Wrong at All)\nfill colors: Party ID where 0 is “Democrat” and 1 is “Republican”\nCount: The size of each bubble is based on the count of people’s age and the response picked for HOMOSEX\nThis is a graph is showing the attitudes towards homosexual sexual relationships by age, count and party id\n\n\n\np2 <- ggplot(gssmhms, aes(x=age, y=marhomo, \n                           color=dr)) +\n  geom_point() +\n   labs(title= \"Age compared to attitudes of homosexual marriage\",\n        x= \"Age of participant\", \n        y = \"Attitudes towards homosexual marriage\") +\n   theme_minimal()\n\np2 + geom_point(aes(size = count))\n\n\n\n\n\nThis graph shows:\nx-axis: Age of the respondent\ny-axis: Attitudes towards homosexual marriage with the mimimum 1 (Strongly Agree) and the maximum is 5 (Strongly Disagree)\nfill colors: Party ID where 0 is “Democrat” and 1 is “Republican”\nCount: The size of each bubble is based on the count of people’s age and the response picked for MARHOMO\nThis is a graph is showing the attitudes towards homosexual marriage by age, count and party id\n\n\n\np3 <- ggplot(gssmhab, aes(x=age, y=abany, color=dr)) +\n  geom_point() +\n   labs(title= \"Age compared to attitudes on abortion\",\n        x= \"Age of participant\", \n        y = \"Attitudes towards abortion\") +\n  scale_y_continuous(breaks=c(0,1,2)) +\n   theme_minimal()\n\np3 + geom_point(aes(size = count))\n\n\n\n\n\nThis graph shows:\nx-axis: Age of the respondent\ny-axis: Attitudes towards abortion with the mimimum 1 (Yes) and the maximum is 2 (No)\nfill colors: Party ID where 0 is “Democrat” and 1 is “Republican”\nCount: The size of each bubble is based on the count of people’s age and the response picked for ABANY\nThis is a graph is showing the attitudes towards abortion by age, count and party id\n\n\n\n\n",
    "preview": "posts/httpsrpubscommollyhackbarthdacss-603-homework-4/distill-preview.png",
    "last_modified": "2022-04-15T17:21:28-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomektracy890062/",
    "title": "HOMEWORK 4 Part II",
    "description": "DACSS 603, Spring 2022",
    "author": [
      {
        "name": "Erin Tracy",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\r\nPART II (Final Project)\r\n1. What is your research question for the final project?\r\nIs there a relationship between level of education (profile_educ5) and how one rates Planned Parenthood (ftpp)?\r\nInformation source: ANES 2020 Social Media Study\r\n\r\n\r\n#my_data <- read_excel(path= \"C:\\\\Users\\\\erink\\\\OneDrive\\\\Documents\\\\2020 Social Media.xlxs.xlsx\") \r\n#my_data2<- read_csv(file = \"C:\\Users\\erink\\OneDrive\\Documents\\2020Social_Media.csv\")\r\n\r\nmy_data2<- read_csv(file = \"C:\\\\Users\\\\erink\\\\OneDrive\\\\Documents\\\\2020Social_Media.csv\")\r\nhead(my_data2)\r\n\r\n\r\n# A tibble: 6 x 13\r\n  caseid profile_gender profile_age profile_racethnicity profile_educ5\r\n   <dbl>          <dbl>       <dbl>                <dbl>         <dbl>\r\n1   3824              2          32                    3             1\r\n2    235              2          63                    1             3\r\n3   1286              1          41                    1             3\r\n4   4981              1          52                    2             5\r\n5   1183              2          67                    1             3\r\n6   3158              1          46                    2             4\r\n# ... with 8 more variables: profile_marital <dbl>,\r\n#   profile_income <dbl>, profile_region4 <dbl>,\r\n#   profile_region9 <dbl>, profile_metro <dbl>, profile_relig <dbl>,\r\n#   profile_born <dbl>, ftpp <dbl>\r\n\r\n2. What is your hypothesis (i.e. an answer to the research question) that you want to test?\r\nI expect that those with higher levels of education are more likely to rate Planned Parenthood highly.\r\n3. Present some exploratory analysis. In particular: a. Numerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables).\r\n\r\n\r\nsummary(my_data2)\r\n\r\n\r\n     caseid     profile_gender   profile_age   profile_racethnicity\r\n Min.   :   1   Min.   :1.000   Min.   :18.0   Min.   :1.000       \r\n 1st Qu.:1459   1st Qu.:1.000   1st Qu.:36.0   1st Qu.:1.000       \r\n Median :2916   Median :1.000   Median :51.0   Median :1.000       \r\n Mean   :2915   Mean   :1.495   Mean   :50.5   Mean   :1.636       \r\n 3rd Qu.:4371   3rd Qu.:2.000   3rd Qu.:65.0   3rd Qu.:2.000       \r\n Max.   :5830   Max.   :2.000   Max.   :80.0   Max.   :4.000       \r\n profile_educ5   profile_marital profile_income  profile_region4\r\n Min.   :1.000   Min.   :1.000   Min.   : 1.00   Min.   :1.000  \r\n 1st Qu.:3.000   1st Qu.:1.000   1st Qu.: 7.00   1st Qu.:2.000  \r\n Median :3.000   Median :1.000   Median :11.00   Median :3.000  \r\n Mean   :3.397   Mean   :2.549   Mean   :10.36   Mean   :2.671  \r\n 3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:14.00   3rd Qu.:4.000  \r\n Max.   :5.000   Max.   :6.000   Max.   :18.00   Max.   :4.000  \r\n profile_region9 profile_metro    profile_relig     profile_born    \r\n Min.   :1.00    Min.   :0.0000   Min.   :-7.000   Min.   :-7.0000  \r\n 1st Qu.:3.00    1st Qu.:1.0000   1st Qu.: 1.000   1st Qu.:-1.0000  \r\n Median :5.00    Median :1.0000   Median : 9.000   Median : 1.0000  \r\n Mean   :5.23    Mean   :0.8313   Mean   : 6.706   Mean   : 0.8106  \r\n 3rd Qu.:8.00    3rd Qu.:1.0000   3rd Qu.:12.000   3rd Qu.: 2.0000  \r\n Max.   :9.00    Max.   :1.0000   Max.   :14.000   Max.   : 2.0000  \r\n      ftpp       \r\n Min.   : -7.00  \r\n 1st Qu.: 28.00  \r\n Median : 61.00  \r\n Mean   : 56.98  \r\n 3rd Qu.: 88.00  \r\n Max.   :100.00  \r\n\r\nI am mostly interested in level of education and rating of Planned Parenthood. The dataset initially had 521 variables. I removed most, but I did also keep the following variables: gender, age, race, marital status, income, region and religion, so that I can see how some of those variables factor into my research question.\r\nb. Plot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\r\n\r\n\r\ninstall.packages(\"ggplot2\")\r\nlibrary(ggplot2)\r\n\r\npairs(~profile_educ5 + profile_relig + ftpp, data=my_data2)\r\n\r\n\r\n\r\nggplot(data = my_data2) +\r\n  geom_point(mapping = aes(x = ftpp, y = profile_educ5))\r\n\r\n\r\n\r\nggplot(data =my_data2) +\r\n  geom_smooth(mapping = aes(x = ftpp, y = profile_educ5))\r\n\r\n\r\n\r\nggplot(data = my_data2)+ \r\n  geom_bar(mapping = aes(x = profile_educ5))\r\n\r\n\r\n\r\ninstall.packages(\"lsr\")\r\n\r\n\r\npackage 'lsr' successfully unpacked and MD5 sums checked\r\n\r\nThe downloaded binary packages are in\r\n    C:\\Users\\erink\\AppData\\Local\\Temp\\RtmpwN4Fqm\\downloaded_packages\r\n\r\nlibrary(lsr)\r\n\r\ninstall.packages(\"PerformanceAnalytics\")\r\n\r\n\r\npackage 'PerformanceAnalytics' successfully unpacked and MD5 sums checked\r\n\r\nThe downloaded binary packages are in\r\n    C:\\Users\\erink\\AppData\\Local\\Temp\\RtmpwN4Fqm\\downloaded_packages\r\n\r\nlibrary(PerformanceAnalytics)\r\n\r\nchart.Correlation(my_data2,histogram=TRUE)\r\n\r\n\r\n\r\n\r\nThe geom point model isn’t very useful at the moment, I will have to make some adjustments to be able to read that model. The geom smooth model does show that people who rated Planned Parenthood very highly do tend to have high levels of education, but there is also another peak among people who rated Planned Parenthood with a low score.\r\nI have to review best models for this type of data, since there are only 5 options for level of education. I also want to explore the chart.correlation model further as I dig further into this project.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomektracy890062/distill-preview.png",
    "last_modified": "2022-04-15T17:21:56-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomektracy890107/",
    "title": "HOMEWORK 4 Part I",
    "description": "DACSS 603, Spring 2022",
    "author": [
      {
        "name": "Erin Tracy",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\r\nPART I\r\nQuestion 1\r\n\r\n\r\ninstall.packages(\"smss\")\r\n\r\n\r\npackage 'smss' successfully unpacked and MD5 sums checked\r\n\r\nThe downloaded binary packages are in\r\n    C:\\Users\\erink\\AppData\\Local\\Temp\\RtmpUtOWzQ\\downloaded_packages\r\n\r\nlibrary(\"smss\")\r\ndata(\"house.selling.price.2\")\r\ninstall.packages('plyr', repos = \"http://cran.us.r-project.org\")\r\n\r\n\r\npackage 'plyr' successfully unpacked and MD5 sums checked\r\n\r\nThe downloaded binary packages are in\r\n    C:\\Users\\erink\\AppData\\Local\\Temp\\RtmpUtOWzQ\\downloaded_packages\r\n\r\nlibrary(plyr)\r\n\r\n\r\n\r\nFor the house.selling.price.2 data the tables show a correlation matrix and a model fit using four predictors of selling price.\r\n\r\n\r\nhouse<- house.selling.price.2\r\nhead(house)\r\n\r\n\r\n      P    S Be Ba New\r\n1  48.5 1.10  3  1   0\r\n2  55.0 1.01  3  2   0\r\n3  68.0 1.45  3  2   0\r\n4 137.0 2.40  3  3   0\r\n5 309.4 3.30  4  3   1\r\n6  17.5 0.40  1  1   0\r\n\r\nsummary(house)\r\n\r\n\r\n       P                S              Be              Ba       \r\n Min.   : 17.50   Min.   :0.40   Min.   :1.000   Min.   :1.000  \r\n 1st Qu.: 72.90   1st Qu.:1.33   1st Qu.:3.000   1st Qu.:2.000  \r\n Median : 96.00   Median :1.57   Median :3.000   Median :2.000  \r\n Mean   : 99.53   Mean   :1.65   Mean   :3.183   Mean   :1.957  \r\n 3rd Qu.:115.00   3rd Qu.:1.98   3rd Qu.:4.000   3rd Qu.:2.000  \r\n Max.   :309.40   Max.   :3.85   Max.   :5.000   Max.   :3.000  \r\n      New        \r\n Min.   :0.0000  \r\n 1st Qu.:0.0000  \r\n Median :0.0000  \r\n Mean   :0.3011  \r\n 3rd Qu.:1.0000  \r\n Max.   :1.0000  \r\n\r\nlm(P~ S +Be + Ba + New, data = house)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = P ~ S + Be + Ba + New, data = house)\r\n\r\nCoefficients:\r\n(Intercept)            S           Be           Ba          New  \r\n    -41.795       64.761       -2.766       19.203       18.984  \r\n\r\ncor<- cor(x=house)\r\ncor\r\n\r\n\r\n            P         S        Be        Ba       New\r\nP   1.0000000 0.8988136 0.5902675 0.7136960 0.3565540\r\nS   0.8988136 1.0000000 0.6691137 0.6624828 0.1762879\r\nBe  0.5902675 0.6691137 1.0000000 0.3337966 0.2672091\r\nBa  0.7136960 0.6624828 0.3337966 1.0000000 0.1820651\r\nNew 0.3565540 0.1762879 0.2672091 0.1820651 1.0000000\r\n\r\n#correlate(house)\r\n\r\n\r\n\r\nWith these four predictors,\r\nA. For backward elimination, which variable would be deleted first? Why?\r\n\r\n\r\nlm(P~., data = house)|>summary()\r\n\r\n\r\n\r\nCall:\r\nlm(formula = P ~ ., data = house)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-36.212  -9.546   1.277   9.406  71.953 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\r\nS             64.761      5.630  11.504  < 2e-16 ***\r\nBe            -2.766      3.960  -0.698 0.486763    \r\nBa            19.203      5.650   3.399 0.001019 ** \r\nNew           18.984      3.873   4.902  4.3e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 16.36 on 88 degrees of freedom\r\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \r\nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\r\n\r\nWe would delete Bedrooms (Be) because it has the highest P Value.\r\nB. For forward selection, which variable would be added first? Why?\r\nS, size would be added first because that is the variable that is the most significant (has the smallest P Value)\r\nC. Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\r\nI think that there are too many other factors that might affect the price of a house with just one bedroom or many bedrooms, this variable alone isn’t significant.\r\nD. Using software with these four predictors, find the model that would be selected using each criterion:\r\nR2\r\nAdjusted R2\r\n\r\n\r\nlm(P~., data = house)|>summary()\r\n\r\n\r\n\r\nCall:\r\nlm(formula = P ~ ., data = house)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-36.212  -9.546   1.277   9.406  71.953 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\r\nS             64.761      5.630  11.504  < 2e-16 ***\r\nBe            -2.766      3.960  -0.698 0.486763    \r\nBa            19.203      5.650   3.399 0.001019 ** \r\nNew           18.984      3.873   4.902  4.3e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 16.36 on 88 degrees of freedom\r\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \r\nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\r\n\r\nlm() model includes R2 and Adjusted R-Squared\r\nPRESS\r\n\r\n\r\nPRESS <- function(linear.model) {\r\n    pr <- residuals(linear.model)/(1 - lm.influence(linear.model)$hat)\r\n    sum(pr^2)\r\n}\r\n\r\nfit <- lm( P ~ Be + Ba + S + New , data = house)\r\nPRESS(fit)\r\n\r\n\r\n[1] 28390.22\r\n\r\nUsed the linear.model function to find PRESS (thank you Piazza.)\r\nAIC\r\n\r\n\r\nAIC(lm(P~., data = house))\r\n\r\n\r\n[1] 790.6225\r\n\r\nBIC\r\n\r\n\r\nBIC(lm(P~., data = house))\r\n\r\n\r\n[1] 805.8181\r\n\r\nE. Explain which model you prefer and why. I prefer the multiple regression model, which seems to provide the most information.\r\nQuestion 2\r\n(Data file: trees from base R) From the documentation: “This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.”\r\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\r\nA. fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\r\n\r\n\r\ndata(trees)\r\nsummary(trees)\r\n\r\n\r\n     Girth           Height       Volume     \r\n Min.   : 8.30   Min.   :63   Min.   :10.20  \r\n 1st Qu.:11.05   1st Qu.:72   1st Qu.:19.40  \r\n Median :12.90   Median :76   Median :24.20  \r\n Mean   :13.25   Mean   :76   Mean   :30.17  \r\n 3rd Qu.:15.25   3rd Qu.:80   3rd Qu.:37.30  \r\n Max.   :20.60   Max.   :87   Max.   :77.00  \r\n\r\nhead(trees)\r\n\r\n\r\n  Girth Height Volume\r\n1   8.3     70   10.3\r\n2   8.6     65   10.3\r\n3   8.8     63   10.2\r\n4  10.5     72   16.4\r\n5  10.7     81   18.8\r\n6  10.8     83   19.7\r\n\r\nfit<- lm(Volume~ Girth + Height, data = trees)\r\nfit\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Volume ~ Girth + Height, data = trees)\r\n\r\nCoefficients:\r\n(Intercept)        Girth       Height  \r\n   -57.9877       4.7082       0.3393  \r\n\r\nB. Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\r\n\r\n\r\npar(mfrow=c(2,2)) \r\nplot(fit)\r\n\r\n\r\n\r\npar(mfrow=c(1,1))\r\n\r\n\r\n\r\nResiduals vs Fitted: I don’t see a horizontal line with equally spread residuals. Assumptions are violated\r\nNormal Q-Q: Appears that residuals are normally distributed.\r\nScale-Location: I do not see a horizontal line with equally spread points, suggesting Heteroscedasticity?\r\nQuestion 3\r\n(inspired by ALR 9.16)\r\n(Data file: florida in alr R package)\r\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\r\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\r\n\r\n\r\ninstall.packages(\"alr4\")\r\n\r\n\r\npackage 'alr4' successfully unpacked and MD5 sums checked\r\n\r\nThe downloaded binary packages are in\r\n    C:\\Users\\erink\\AppData\\Local\\Temp\\RtmpUtOWzQ\\downloaded_packages\r\n\r\nlibrary(alr4)\r\n\r\ndata(florida)\r\n\r\nflorida\r\n\r\n\r\n               Gore   Bush Buchanan\r\nALACHUA       47300  34062      262\r\nBAKER          2392   5610       73\r\nBAY           18850  38637      248\r\nBRADFORD       3072   5413       65\r\nBREVARD       97318 115185      570\r\nBROWARD      386518 177279      789\r\nCALHOUN        2155   2873       90\r\nCHARLOTTE     29641  35419      182\r\nCITRUS        25501  29744      270\r\nCLAY          14630  41745      186\r\nCOLLIER       29905  60426      122\r\nCOLUMBIA       7047  10964       89\r\nDADE         328702 289456      561\r\nDE SOTO        3322   4256       36\r\nDIXIE          1825   2698       29\r\nDUVAL        107680 152082      650\r\nESCAMBIA      40958  73029      504\r\nFLAGLER       13891  12608       83\r\nFRANKLIN       2042   2448       33\r\nGADSDEN        9565   4750       39\r\nGILCHRIST      1910   3300       29\r\nGLADES         1420   1840        9\r\nGULF           2389   3546       71\r\nHAMILTON       1718   2153       24\r\nHARDEE         2341   3764       30\r\nHENDRY         3239   4743       22\r\nHERNANDO      32644  30646      242\r\nHIGHLANDS     14152  20196       99\r\nHILLSBOROUGH 166581 176967      836\r\nHOLMES         2154   4985       76\r\nINDIAN RIVER  19769  28627      105\r\nJACKSON        6868   9138      102\r\nJEFFERSON      3038   2481       29\r\nLAFAYETTE       788   1669       10\r\nLAKE          36555  49963      289\r\nLEE           73560 106141      305\r\nLEON          61425  39053      282\r\nLEVY           5403   6860       67\r\nLIBERTY        1011   1316       39\r\nMADISON        3011   3038       29\r\nMANATEE       49169  57948      272\r\nMARION        44648  55135      563\r\nMARTIN        26619  33864      108\r\nMONROE        16483  16059       47\r\nNASSAU         6952  16404       90\r\nOKALOOSA      16924  52043      267\r\nOKEECHOBEE     4588   5058       43\r\nORANGE       140115 134476      446\r\nOSCEOLA       28177  26216      145\r\nPALM BEACH   268945 152846     3407\r\nPASCO         69550  68581      570\r\nPINELLAS     199660 184312     1010\r\nPOLK          74977  90101      538\r\nPUTNAM        12091  13439      147\r\nST. JOHNS     19482  39497      229\r\nST. LUCIE     41559  34705      124\r\nSANTA ROSA    12795  36248      311\r\nSARASOTA      72854  83100      305\r\nSEMINOLE      58888  75293      194\r\nSUMTER         9634  12126      114\r\nSUWANNEE       4084   8014      108\r\nTAYLOR         2647   4051       27\r\nUNION          1399   2326       26\r\nVOLUSIA       97063  82214      396\r\nWAKULLA        3835   4511       46\r\nWALTON         5637  12176      120\r\nWASHINGTON     2796   4983       88\r\n\r\nhead(florida)\r\n\r\n\r\n           Gore   Bush Buchanan\r\nALACHUA   47300  34062      262\r\nBAKER      2392   5610       73\r\nBAY       18850  38637      248\r\nBRADFORD   3072   5413       65\r\nBREVARD   97318 115185      570\r\nBROWARD  386518 177279      789\r\n\r\nsummary(florida)\r\n\r\n\r\n      Gore             Bush           Buchanan     \r\n Min.   :   788   Min.   :  1316   Min.   :   9.0  \r\n 1st Qu.:  3055   1st Qu.:  4746   1st Qu.:  46.5  \r\n Median : 14152   Median : 20196   Median : 114.0  \r\n Mean   : 43341   Mean   : 43356   Mean   : 258.5  \r\n 3rd Qu.: 45974   3rd Qu.: 56542   3rd Qu.: 285.5  \r\n Max.   :386518   Max.   :289456   Max.   :3407.0  \r\n\r\n?florida\r\n\r\nlm(Buchanan~., data= florida)|>summary()\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Buchanan ~ ., data = florida)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-913.70  -65.68  -36.96   21.41 2204.20 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept) 82.3532929 52.1819372   1.578  0.11945   \r\nGore         0.0043013  0.0013309   3.232  0.00194 **\r\nBush        -0.0002379  0.0017476  -0.136  0.89216   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 330.7 on 64 degrees of freedom\r\nMultiple R-squared:  0.4747,    Adjusted R-squared:  0.4582 \r\nF-statistic: 28.91 on 2 and 64 DF,  p-value: 1.132e-09\r\n\r\ninstalled.packages(\"ggplot2\")\r\n\r\n\r\n     Package LibPath Version Priority Depends Imports LinkingTo\r\n     Suggests Enhances License License_is_FOSS License_restricts_use\r\n     OS_type Archs MD5sum NeedsCompilation Built\r\n\r\nlibrary(ggplot2)\r\n\r\nggplot(data = florida) +\r\n  geom_point(mapping = aes(x = Buchanan, y = Bush))\r\n\r\n\r\n\r\n\r\nYes, I can visually see from the plots that there is an outlier (above 3000 for Buchanan), and when I check that plot in the data, I can see that it is Palm Beach.\r\nSee separate File for PART II\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomektracy890107/distill-preview.png",
    "last_modified": "2022-04-15T17:22:01-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomofyalcinhw3-sol/",
    "title": "Homework 3 | Solutions",
    "description": "Solutions to Homework 3",
    "author": [
      {
        "name": "Omer Yalcin",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\nQuestion 1\nA\nLet’s write a function that expresses the outcome as a function of \\(x_1\\) and \\(x_2\\)\n\n\ny_hat <- function(x1, x2) {-10536 + 53.8*x1 + 2.84*x2}\n\n\n\nThe predicted selling price for a home of 1240 square feet on a lot of 18,000 square feet is:\n\n\npredicted <- y_hat(x1 = 1240, x2 = 18000)\ncat('$', predicted, sep = '')\n\n\n$107296\n\nThe actual price is $145,000. Thus, the residual is:\n\n\ncat(145000 - predicted)\n\n\n37704\n\nThe residual is positive. The model underpredicts the selling price for this house, arguably by a lot.\nB\nFor a fixed lot size, the house selling price is predicted to increase by $53.8, because that’s the coefficient of the size of home variable when lot size is also in the model, thus controlling for (or holding fixed) the latter.\nC\nOne-square-foot increase in home size is associated with an increase in price of $53.8. One-square-foot increase in lot size is associated with an increase in price of $2.84. To have an impact of a one-square foot increase in home size, which $53.8, lot size would have to increase by 53.8/2.84, which is an increase of about 18.94 square-feet.\nQuestion 2\nLoad the data:\n\n\ndata(salary)\n\n\n\nA\nBecause the question says “without regard to any other variable but sex,” we can run a simple linear regression model with sex as the only explanatory variable. This is equivalent to doing a two-sample t-test for salary for the Male and Female groups\n\n\nsummary(lm(salary ~ sex, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\nThe coefficient -3340 for the sexFemale variable suggests that female faculty makes on average $3340 less than their male colleagues in this college. In other words, the difference in mean is 3340. The variable is statistically significant at the 10% level, but not at the more widely accepted 5% level.\nB\nFor the model with all predictors, here is what the confidence intervals look like:\n\n\nlm(salary ~ ., data = salary) |>\n  confint() \n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\nThe 95% confidence interval for the sexFemale variable is (-697, 3031). This suggests an confidence interval between $697 less or $3031 more salary for female faculty relative to male faculty, controlling for other variables.\nC\n\n\nsummary(lm(salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\ndegree: The degree variable is not statistically significant at any conventionally accepted level. It is a dummy variable, so the coefficient suggests that PhDs make $1389 more compared to those with Masters controlling for everything else.\nrank The model takes rank as an categorical variable, ignoring its order. The most common practice for ordered categorical variables like rank is to either treat them as just a regular categorical variable or as a numeric variable. The latter is most acceptable when the variable has lots of levels and/or the distance between each level can be reasonably thought of as equal. In this case, because there are only three levels (one more than what a dummy variable has), it makes sense to accept this as a regular categorical variable.\nGiven this, the rankAssoc category suggests that Associate Professors make $5292 more than Assistant Professors, the base (reference) level. rankProf suggests full professors make $11118 more than Assistant Professors. Both variables are statistically significant.\nIf we wanted to test for the statistical significance of the rank variable as a whole, rather than for the individual dummy variables, we would need to do a partial F-test to compare the model with all the variables to the one without any rank dummies. The easiest way to do this is:\n\n\nfit1 <- lm(salary ~ ., data = salary)\nfit2 <- lm(salary ~ . -rank, data = salary)\n# see here: https://www.youtube.com/watch?v=G_obrpV70QQ\nanova(fit1, fit2)\n\n\nAnalysis of Variance Table\n\nModel 1: salary ~ degree + rank + sex + year + ysdeg\nModel 2: salary ~ (degree + rank + sex + year + ysdeg) - rank\n  Res.Df       RSS Df  Sum of Sq     F    Pr(>F)    \n1     45 258858365                                  \n2     47 658649047 -2 -399790682 34.75 7.485e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe very small p-value (Pr(>F)) suggests that the rank variable is significant as a whole.\nsex: As we saw with confidence intervals, this variable is now not statistically significant at conventional levels. The coefficient suggests female faculty make $1166 more after everything is controlled, but interpreting coefficients when the effect is insignificant is not very meaningful.\nyear: This variable is statistically significant. It suggests that every additional in current rank is associated with $476 more salary.\nysdeg: The variable is insignificant. The coefficient would suggest that every additional year that passes since degree is associated with $124 less salary\nD\n\n\nsalary$rank <- relevel(salary$rank, ref = 'Prof')\nsummary(lm(salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nThe coefficients are now for the two dummy variables that represent the two categories other than the reference category, which is now Prof. rankAsst being -11118 means Assistant Professors make 11118 less than Full Professors, controlling for other variables. rankAssoc being -5826 means Associate Professors make 5826 less than Full Professors, controlling for other variables. The same information in the previous model is presented in a different way.\nE\n\n\nsummary(lm(salary ~ . -rank, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ . - rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\nExcluding rank does cause the sign of the sex variable to flip to negative, but it is still not statistically significant.\nF\nBased on the description, we would need to create a new dummy variable, which I’ll call new_dean, from the ysdeg variable. To avoid multicollinearity, it would be a good idea to not include highly correlated variables. Because we are creating one variable from another, it is likely that they are highly correlated. Let’s see if that’s the case.\n\n\nsalary$new_dean <- ifelse(salary$ysdeg <= 15, 1, 0)\ncor.test(salary$new_dean, salary$ysdeg)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  salary$new_dean and salary$ysdeg\nt = -11.101, df = 50, p-value = 4.263e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9074548 -0.7411040\nsample estimates:\n       cor \n-0.8434239 \n\nYes, they are very highly (negatively) correlated. So, we’ll exclude ysdeg and only include new_dean in its place, alongside other control variables.\n\n\nsummary(lm(salary ~ . -ysdeg, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ . - ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24425.32    1107.52  22.054  < 2e-16 ***\ndegreePhD      818.93     797.48   1.027   0.3100    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nyear           434.85      78.89   5.512 1.65e-06 ***\nnew_dean      2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\nThe model above does suggest that the new dean has been making more generous offers, since the faculty appointed under the new dean make about $2163 more, controlling for other variables. The variable is statistically significant at the 5% level.\nLet’s see what would have happened if we included both variables:\n\n\nsummary(lm(salary ~ . , data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3621.2 -1336.8  -271.6   530.1  9247.6 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  25179.14    1901.59  13.241  < 2e-16 ***\ndegreePhD     1135.00    1031.16   1.101    0.277    \nrankAsst    -11411.45    1362.02  -8.378 1.16e-10 ***\nrankAssoc    -6177.44    1043.04  -5.923 4.39e-07 ***\nsexFemale     1084.09     921.49   1.176    0.246    \nyear           460.35      95.09   4.841 1.63e-05 ***\nysdeg          -47.86      97.71  -0.490    0.627    \nnew_dean      1749.09    1372.83   1.274    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2382 on 44 degrees of freedom\nMultiple R-squared:  0.8602,    Adjusted R-squared:  0.838 \nF-statistic: 38.68 on 7 and 44 DF,  p-value: < 2.2e-16\n\nNow, neither variable is significant because of multicollinearity.\nQuestion 3\nLoad the data:\n\n\ndata(house.selling.price)\n\n\n\nA\n\n\nfit <- lm(Price ~ Size + New, data = house.selling.price)\nsummary(fit)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\nIn the model, both the Size and the New variables are positively associated with Price. They are both statistically significant at the 5% level. A 1 square foot increase in the size of the house is associated with a $116 increase in house, controlling for whether the house is new. New houses are on average $57736 more expensive than old houses, controlling for size.\nB\nThe interpretation part is a bit redundant, since it was done in part A.\nThe prediction equation is:\n\\[\\mathbf{E}[\\textrm{Price}] = -40230.867 + 116.132 * \\textrm{size} + 57736.283 * \\textrm{new}\\]\nFor new homes, the new variable takes the value 1, for old homes, it takes 0.\nSo, for new homes, the equation is:\n\\[\\mathbf{E}[\\textrm{Price}] = -40230.867 + 116.132 * \\textrm{size} + 57736.283 * 1 = 17505.42 + 116.132 * \\textrm{size} \\]\nFor old homes, the equation is:\n\\[\\mathbf{E}[\\textrm{Price}] = -40230.867 + 116.132 * \\textrm{size} + 57736.283 * 0 = -40230.867 + 116.132 * \\textrm{size}\\]\nC\nWe can create a data frame and use the predict() function to do the prediction.\n\n\ndata.frame(Size = c(3000, 3000), New = c(1, 0)) %>%\n  predict(fit, .) \n\n\n       1        2 \n365900.2 308163.9 \n\nA new home of 3000 square feet has a predicted price of 365900.2. An old home of 3000 square feet has a predicted price of 308163.9. (Note that the difference, 57736.3, is the coefficient of New)\nD\nModel with interaction term:\n\n\nfit_ia <- lm(Price ~ Size + New + Size * New, data = house.selling.price)\nsummary(fit_ia)\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\nE\n\\[\\mathbf{E}[\\textrm{Price}] = -22227.808 + 104.438 * \\textrm{size} + -78527.502 * \\textrm{New} + 61.916 * \\textrm{Size} * \\textrm{New}\\]\nAgain, we follow the same logic from B, replacing New with 1 and 0:\nFor new homes:\n\\[\\mathbf{E}[\\textrm{Price}] = -22227.808 + 104.438 * \\textrm{size} + -78527.502 * 1 + 61.916 * \\textrm{Size} * 1 = -100755.3 + 166.354 * size\\]\nFor old homes:\n\\[\\mathbf{E}[\\textrm{Price}] = -22227.808 + 104.438 * \\textrm{size} + -78527.502 * 0 + 61.916 * \\textrm{Size} * 0 = -22227.808 + 104.438 * \\textrm{size}\\]\nF\nUsing predict() again:\n\n\ndata.frame(Size = c(3000, 3000), New = c(1, 0)) %>%\n  predict(fit_ia, .) \n\n\n       1        2 \n398307.5 291087.4 \n\nA new home of 3000 square feet has a predicted price of $398307.5. An old home of 3000 square feet has a predicted price of $291087.4. The difference is $107220.1.\nG\n\n\ndata.frame(Size = c(1500, 1500), New = c(1, 0)) %>%\n  predict(fit_ia, .) \n\n\n       1        2 \n148776.1 134429.8 \n\nA new home of 1500 square feet has a predicted price of $148776.1. An old home of 1500 square feet has a predicted price of $134429.8. The difference is $14346.3.\nThe difference between new and old home prices is much more when the size of the home is larger. For 3000 sq ft homes, the difference is 107220.1 as opposed to the 14346.1 difference for homes that are 1500 sq ft. This is consistent with the positive coefficient for the interaction term.\nH\nI prefer the model with the interaction term, because (1) the interaction term is significant, (2) the Adjusted R-squared is higher in the model with interaction (i.e. despite the penalty for the additional term, we have better fit). We could look into cross-validation / PRESS (which also end up showing the interaction model to be superior), but those are not this homework’s topic.\n\n\nsummary(fit)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\nsummary(fit_ia)\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-15T17:22:05-04:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomtpaske890262/",
    "title": "Homework 4",
    "description": "DACSS 603 - Tyler Paske",
    "author": [
      {
        "name": "Tyler Paske",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\r\n                                                 Question 1. \r\n                                                 \r\nFor the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.\r\n##A. For backward elimination, which variable would be deleted first? Why?\r\nFor backward elimination, Beds would be eliminated first because it has the largest P-Value. The process of Backwards Elimination as referenced from slides is as follows; predetermine a significance level, start with including all variables in the model, at each stage delete variable with the largest p-value and stop when all variables are significant. As we see if we deleted beds, all the remaining variables hold significance.\r\n\r\nlibrary(smss)\r\n\r\n\r\ndata(“house.selling.price.2”)\r\n\r\n\r\nsummary(lm(P~ ., data = house.selling.price.2))\r\n\r\n##B. For forward selection, which variable would be added first? Why?\r\nFor forward selection we essentially do the opposite of backwards elimination where we begin with no explanatory variable, add the variable with the most significance at each step and stop when no remaining variable can make a significant partial contribution. In this case, Size would be added first because it has the most significance with a P-Value at 0 where New has a continuous variable that may have a little less significance.\r\n##C. Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\r\nBeds had such a large p value because there was week evidence against the experimental study with relation to the Price of the home. However, there was a substantial correlation with price. This shows to me that though Beds in a home was important, people could do without them as other factors took more importance as other factors go into buying a home when compared to its price (Size & Baths) for example.\r\n##D. Using software with these four predictors, find the model that would be selected using each criterion:\r\n###a\r\nA statistical calculation that measures the degree of interrelation and dependence between two variables. In other words, it is a formula that determines how much a variable’s behavior can explain the behavior of another variable.\r\nAs R(squared) measures the degree of interrelation and dependence between two variables I find that this model would NOT be selected using each criterion.\r\n###b The adjusted R-squared is a modified version of R-squared that adjusts for predictors that are not significant in a regression model. Compared to a model with additional input variables, a lower adjusted R-squared indicates that the additional input variables are not adding value to the model. Compared to a model with additional input variables, a higher adjusted R-squared indicates that the additional input variables are adding value to the model.\r\nThis model would in my opinion be SELECTED as it uses each criterion of the model itself. I find that this model would be selected as this model uses additional input variables with a lower adjusted R-Squared to indicate that the additional input variables are or are not adding value to the model.\r\n###c The idea is that RSS describes how well a linear model fits the data to which it was fitted, but PRESS tells you how well the model will predict new data. As we’re not interested in predicting new data this model would NOT be selected.\r\n###d This model would again be important for predicting the relationship between variables or in our case, predictors. In the instance that we’re not looking to predict but more so compare and contrast I lean in the direction that this model again would NOT be selected using each criterion.\r\n###e Bayesian Information Criterion (BIC) is a model selection tool. If a model is estimated on a particular data set (training set), BIC score gives an estimate of the model performance on a new, fresh data set (testing set). BIC is given by the formula:\r\n                            BIC = -2 * loglikelihood + d * log(N),\r\nwhere N is the sample size of the training set and d is the total number of parameters. The lower BIC score signals a better model.\r\n#E. Explain which model you prefer and why. As mentioned I would prefer the Adjusted R-Squared as the model uses additional input variables with a lower adjusted R-Squared to indicate that the additional input variables are or are not adding value to the model.\r\n                                             Question 2 \r\n                                             \r\n                                             \r\n                                             \r\n                                             \r\n“This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.” Tree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\r\n##A fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\r\nCODE:\r\n\r\n\r\nCall:\r\nlm(formula = log(Volume) ~ log(Girth), data = trees)\r\n\r\nResiduals:\r\n      Min        1Q    Median        3Q       Max \r\n-0.205999 -0.068702  0.001011  0.072585  0.247963 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -2.35332    0.23066  -10.20 4.18e-11 ***\r\nlog(Girth)   2.19997    0.08983   24.49  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.115 on 29 degrees of freedom\r\nMultiple R-squared:  0.9539,    Adjusted R-squared:  0.9523 \r\nF-statistic: 599.7 on 1 and 29 DF,  p-value: < 2.2e-16\r\n\r\n\r\nsummary(fm1 <- lm(log(Volume) ~ log(Girth), data = trees))\r\n\r\n##B Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\r\nAfter reviewing the following I conclude that there’s no violated regressions assumptions as a violation would include there NOT being a relationship between the residuals and the variables. As we can see from the regression diagnostic plots on the model, there is a relationship.\r\nCODE:\r\n\r\n\r\n\r\npairs(trees, panel = panel.smooth, main = “trees data”) plot(Volume ~ Girth, data = trees, log = “xy”) coplot(log(Volume) ~ log(Girth) | Height, data = trees,panel = panel.smooth)\r\n#Question 3. In the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.The data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\r\nSimple linear regression model\r\nCODE:\r\n\r\n\r\nCall:\r\nlm(formula = Buchanan ~ Bush, data = florida)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-907.50  -46.10  -29.19   12.26 2610.19 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \r\nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 353.9 on 65 degrees of freedom\r\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \r\nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\r\n\r\n\r\nsummary(lm(Buchanan ~ Bush, data = florida))\r\n\r\nRegression Diagnostic Plots\r\nCODE:\r\n\r\n\r\n\r\nBased on the plots we can see that in my opinion Palm Beach County is an outlier based on the plots in that there’s little proof to show that. The plots show a similar correlation for Buchanan and Core as they do with Buchanan and Bush. Seeing that there’s a similar regression I lead to believe that the butterfly ballot had little to do with the layout of the ballot causing some voters to cast votes for Buchanan when their intended choice was Gore.\r\n#PART 2 (Final Project)\r\n##1. What is your research question for the final project?\r\nBased on studies conducted in 1986, what percentage of people were working remotely vs those that had to travel for work? How does that compare to the data shared in the same year for the number of hours people were working. Is there any correlation?\r\n##2. What is your hypothesis (i.e. an answer to the research question) that you want to test?\r\nMy hypothesis is that I’d like to test is the difference between in time to commute to time worked. I’d like to see if people that don’t travel worked very many hours.\r\n##3. Present some exploratory analysis. In particular:\r\n###a. Numerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables).\r\n                                 Could not knit my data file for summary \r\n\r\nCHW <- DACSS_603_Tyler_Paske_Final_Project_Commuting_Hours_worked_ summary(CHW)\r\n\r\nThe outcome variable is going to be the Number of Commuted participants as we have the greatest number of people who participated in that study. The explanatory variable will be the length of time it took them to commute and the control variable will be the number of hours worked as it relates to those that commuted and or didn’t have to commute. From the summary we already start to gather evidence of what we can expect. We see that most of the people that are actually working are those that work full time.\r\n###b. Plot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\r\n                                   Could not knit my data file for summary \r\n                                   \r\n\r\nplot(x = CHW\\(`Commute hours`, y = CHW\\)Commute participants, xlab = “Time in hours”, ylab = “Participants”)\r\n\r\nWe see that among the key variables (Commute hours & Commute participants) we see that most of our participants had to travel anywhere between roughly 10 to 45 minutes for work within this year.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomtpaske890262/distill-preview.png",
    "last_modified": "2022-04-15T17:22:09-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomkpopiela890273/",
    "title": "HW3",
    "description": "DACSS-603",
    "author": [
      {
        "name": "Katie Popiela",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\r\nQuestion 1\r\n(SMSS 11.2, except part (d))\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2\r\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\n\r\n\r\npredicted_price<--10536+53.8*1240+2.84*18000 \r\npredicted_price\r\n\r\n\r\n[1] 107296\r\n\r\nGiven the variable information above (y=selling price, x1=house size, x2=lot size) as well as the data in the prediction equation (1,240 sqft house on a 18,000 sqft lot), the predicted selling price of the home is therefore $107,296. Now to calculate the residual between this and the actual selling price ($145,000).\r\n\r\n\r\nresidual<-145000-predicted_price\r\nresidual\r\n\r\n\r\n[1] 37704\r\n\r\nThe residual between the two prices indicates that the home was sold for $37,740 more than the predicted selling price.\r\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\nSince the coefficient for home size is 53.8x1, the price of the home would go up $53.80 for each sqft increase in home size.\r\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\nSince the coefficient for lot size is 2.84x2, every sqft increase would add $2.84 to the overall price of the home. However (with lot size) to achieve the same impact as the price increase of home size, we need to do some simple division.\r\n\r\n\r\n53.8/2.84\r\n\r\n\r\n[1] 18.94366\r\n\r\nAs the math shows, the lot size would need to increase by 18.9 sqft in order to have the same impact as an increase of 1 sqft of home space.\r\nQuestion 2\r\n(ALR, 5.17, slightly modified)\r\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\nA. Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\n\r\n\r\nlibrary(alr4)\r\ndata(\"salary\")\r\n?salary\r\nhead(salary)\r\n\r\n\r\n   degree rank    sex year ysdeg salary\r\n1 Masters Prof   Male   25    35  36350\r\n2 Masters Prof   Male   13    22  35350\r\n3 Masters Prof   Male   10    23  28200\r\n4 Masters Prof Female    7    27  26775\r\n5     PhD Prof   Male   19    30  33696\r\n6 Masters Prof   Male   16    21  28516\r\n\r\n\r\n\r\nsummary(salary)\r\n\r\n\r\n     degree      rank        sex          year            ysdeg      \r\n Masters:34   Asst :18   Male  :38   Min.   : 0.000   Min.   : 1.00  \r\n PhD    :18   Assoc:14   Female:14   1st Qu.: 3.000   1st Qu.: 6.75  \r\n              Prof :20               Median : 7.000   Median :15.50  \r\n                                     Mean   : 7.481   Mean   :16.12  \r\n                                     3rd Qu.:11.000   3rd Qu.:23.25  \r\n                                     Max.   :25.000   Max.   :35.00  \r\n     salary     \r\n Min.   :15000  \r\n 1st Qu.:18247  \r\n Median :23719  \r\n Mean   :23798  \r\n 3rd Qu.:27259  \r\n Max.   :38045  \r\n\r\nNow I will filter the dataset down to focus on sex and salary.\r\n\r\n\r\ndata(\"salary\") \r\nt.test(salary~sex,data=salary)\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.7744, df = 21.591, p-value = 0.09009\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -567.8539 7247.1471\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\n24696.79-21357.14\r\n\r\n\r\n[1] 3339.65\r\n\r\nWithout regard to education level and job rank, the mean salary for males is $24,696.79 whereas the mean salary for females is $21,357.14 – a $3,339.65 difference.\r\nB. Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\n\r\n\r\nfit <- lm(salary~degree+rank+sex+year+ysdeg,data=salary)\r\n\r\nhead(predict(fit), n=10)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n34412.44 30316.19 28762.69 28001.84 33566.07 31869.70 25433.42 \r\n       8        9       10 \r\n32243.42 30708.21 30583.64 \r\n\r\n\r\n\r\nset.seed(3)\r\ndf<- data.frame(degree = sample(salary$degree, size = 10, replace = T),\r\n                rank = sample(salary$rank, size = 10, replace = T),\r\n                sex = sample(salary$sex, size = 10, replace = T),\r\n                year = sample(salary$year, size = 10, replace = T),\r\n                ysdeg = sample(salary$ysdeg, size = 10, replace = T))\r\npredict(fit,df)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n15368.63 20671.56 14287.78 29407.55 20069.45 32082.22 23606.80 \r\n       8        9       10 \r\n16837.87 21524.03 28077.52 \r\n\r\nsummary(fit)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\nconfint(fit,'sexFemale')\r\n\r\n\r\n              2.5 %   97.5 %\r\nsexFemale -697.8183 3030.565\r\n\r\nAfter running a multiple linear regression and calculating it with a 95% confidence interval, the difference in salary between males and females is between -697.82 and 3030.57.\r\nC. Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\r\n\r\n\r\nset.seed(3)\r\ndf <- data.frame(degree = sample(salary$degree, size = 10, replace = T),\r\n                rank = sample(salary$rank, size = 10, replace = T),\r\n                sex = sample(salary$sex, size = 10, replace = T),\r\n                year = sample(salary$year, size = 10, replace = T),\r\n                ysdeg = sample(salary$ysdeg, size = 10, replace = T))\r\npredict(fit,df)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n15368.63 20671.56 14287.78 29407.55 20069.45 32082.22 23606.80 \r\n       8        9       10 \r\n16837.87 21524.03 28077.52 \r\n\r\n\r\n\r\nsummary(fit)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\n?salary\r\n\r\n\r\n\r\nTo summarize, salary increases by:a. $1388.61 if the individual has a PhD b. $5292.36 if the individual is an Associate Professor c. $11,118.75 if the individual is a full/tenured Professord. $1,166.37 if the individual is femalee. $476.31 every year the individual is at their current rank\r\nHowever, salary decreases by $124.57 for every year that passes since the individual attained their highest degree/rank level. All slopes are positive except for this one. Additionally, individuals’ rank and the years they spent at their current rank are statistically significant (less than 0.05).\r\nD. Change the baseline category for the rank variable. Interpret the coefficients related to rank again\r\n\r\n\r\nfit2 <- lm(salary~rank+sex+degree+year+ysdeg, data=salary)\r\nset.seed(3)\r\ndf2 <- data.frame(rank=sample(salary$rank,size=10,replace=T),\r\n                  sex=sample(salary$sex,size=10,replace=T),\r\n                  degree=sample(salary$degree,size=10,replace=T),\r\n                  year=sample(salary$year,size=10,replace=T),\r\n                  ysdeg=sample(salary$ysdeg,size=10,replace=T))\r\npredict(fit2,df2)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n25098.78 25963.92 15676.39 24969.76 22624.44 26255.82 16925.83 \r\n       8        9       10 \r\n29123.01 31254.18 28077.52 \r\n\r\n\r\n\r\nsummary(fit2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ rank + sex + degree + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nIn this calculation I reordered the categories to change the baseline (I put ‘rank’ and ‘sex’ first rather than ‘degree’ and ‘rank’). None of the figures changed, though, so I’m not sure if this is correct or if I miscalculated.\r\nE. Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\r\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\r\n\r\n\r\nfit3 <- lm(salary~degree+sex+year+ysdeg,data=salary)\r\n\r\nset.seed(3)\r\ndf3 <- data.frame(degree=sample(salary$degree,size=10,replace=T),\r\n                  sex=sample(salary$sex,size=10,replace=T),\r\n                  year=sample(salary$year,size=10,replace=T),\r\n                  ysdeg=sample(salary$ysdeg,size=10,replace=T))\r\npredict(fit3,df3)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n15631.50 25840.16 23116.76 29904.74 30290.05 27114.13 19427.73 \r\n       8        9       10 \r\n25588.74 23098.28 21451.56 \r\n\r\nsummary(fit3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\r\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \r\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \r\nyear          351.97     142.48   2.470 0.017185 *  \r\nysdeg         339.40      80.62   4.210 0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\r\n\r\nWith variable ‘rank’ removed, salary decreases by:a. $3299.35 if the individual has a PhDb. $1,286.54 if the individual is female\r\nHowever, salary increases by:a. $351.97 for each year the individual spends at their current rankb. $339.40 each year after the individual earned their highest degree\r\nThe slopes are split 50/50 in terms of how many are positive and negative. ‘degreePhD’, ‘year’, and ‘ysdeg’ are all statistically significant (less than 0.05) while ‘sexFemale’ is not.\r\nF.Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\r\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\n\r\n\r\nfit4 <- lm(salary~rank+degree+sex+year+ysdeg+year*ysdeg,data=salary)\r\nsummary(fit4)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ rank + degree + sex + year + ysdeg + year * \r\n    ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4055.4 -1007.7  -172.6   800.0  9275.7 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 16289.506    944.422  17.248  < 2e-16 ***\r\nrankAssoc    5627.056   1184.705   4.750 2.19e-05 ***\r\nrankProf    11475.286   1389.241   8.260 1.71e-10 ***\r\ndegreePhD    1557.213   1028.855   1.514   0.1373    \r\nsexFemale    1233.531    925.994   1.332   0.1897    \r\nyear          318.343    174.450   1.825   0.0748 .  \r\nysdeg        -172.406     89.161  -1.934   0.0596 .  \r\nyear:ysdeg      7.094      6.578   1.078   0.2867    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2394 on 44 degrees of freedom\r\nMultiple R-squared:  0.8588,    Adjusted R-squared:  0.8363 \r\nF-statistic: 38.22 on 7 and 44 DF,  p-value: < 2.2e-16\r\n\r\nThe slope for year*ysdeg is positive, therefore supporting the hypothesis.\r\nQuestion 3\r\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\nBefore doing any specific calculations, I’m going to first present summary info to get a sense of the dataset.\r\n\r\n\r\nlibrary(smss)\r\ndata(\"house.selling.price\")\r\nhead(house.selling.price)\r\n\r\n\r\n  case Taxes Beds Baths New  Price Size\r\n1    1  3104    4     2   0 279900 2048\r\n2    2  1173    2     1   0 146500  912\r\n3    3  3076    4     2   0 237700 1654\r\n4    4  1608    3     2   0 200000 2068\r\n5    5  1454    3     3   0 159900 1477\r\n6    6  2997    3     2   1 499900 3153\r\n\r\n\r\n\r\nsummary(house.selling.price)\r\n\r\n\r\n      case            Taxes           Beds       Baths     \r\n Min.   :  1.00   Min.   :  20   Min.   :2   Min.   :1.00  \r\n 1st Qu.: 25.75   1st Qu.:1178   1st Qu.:3   1st Qu.:2.00  \r\n Median : 50.50   Median :1614   Median :3   Median :2.00  \r\n Mean   : 50.50   Mean   :1908   Mean   :3   Mean   :1.96  \r\n 3rd Qu.: 75.25   3rd Qu.:2238   3rd Qu.:3   3rd Qu.:2.00  \r\n Max.   :100.00   Max.   :6627   Max.   :5   Max.   :4.00  \r\n      New           Price             Size     \r\n Min.   :0.00   Min.   : 21000   Min.   : 580  \r\n 1st Qu.:0.00   1st Qu.: 93225   1st Qu.:1215  \r\n Median :0.00   Median :132600   Median :1474  \r\n Mean   :0.11   Mean   :155331   Mean   :1629  \r\n 3rd Qu.:0.00   3rd Qu.:169625   3rd Qu.:1865  \r\n Max.   :1.00   Max.   :587000   Max.   :4050  \r\n\r\n\r\n\r\nstr(house.selling.price)\r\n\r\n\r\n'data.frame':   100 obs. of  7 variables:\r\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\r\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\r\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\r\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\r\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\r\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\r\n\r\n?house.selling.price\r\n\r\n\r\n\r\nNow I will conduct a regression analysis in which y = selling price(USD) in terms of house size (sqft), and whether the home is new (1 = yes, 0 = no)\r\n\r\n\r\nsummary(lm(Price~Size+New,data=house.selling.price))\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nB.Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\ny = -40230.87 + 116.13x1 + 57736.28x2 – where x1 = house size and x2 = new or not new. Since x2 is determined by ‘1’ (new) or ‘0’ (not new) the formula can be reconfigured slightly.\r\nNew homes:\r\ny = -40230.87 + 116.13x1 + 57736.28\r\nNot new homes:\r\ny = -40230.87 + 116.13x1\r\nWith these figures in mind, I will first look at the first 10 entries in the ‘house.selling.price’ dataset in order to create a new object.\r\n\r\n\r\nlibrary(smss)\r\ndata(\"house.selling.price\")\r\nstr(house.selling.price)\r\n\r\n\r\n'data.frame':   100 obs. of  7 variables:\r\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\r\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\r\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\r\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\r\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\r\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\r\n\r\nhead(house.selling.price,10)\r\n\r\n\r\n   case Taxes Beds Baths New  Price Size\r\n1     1  3104    4     2   0 279900 2048\r\n2     2  1173    2     1   0 146500  912\r\n3     3  3076    4     2   0 237700 1654\r\n4     4  1608    3     2   0 200000 2068\r\n5     5  1454    3     3   0 159900 1477\r\n6     6  2997    3     2   1 499900 3153\r\n7     7  4054    3     2   0 265500 1355\r\n8     8  3002    3     2   1 289900 2075\r\n9     9  6627    5     4   0 587000 3990\r\n10   10   320    3     2   0  70000 1160\r\n\r\nNext, I will conduct a multiple regression to show the relationship between a new home’s selling price and its size:\r\n\r\n\r\nnew_selling_price <- lm(formula=Price~New+Size,data=house.selling.price)\r\nsummary(new_selling_price)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New + Size, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nNow I will conduct a correlation test:\r\n\r\n\r\ncor.test(house.selling.price$Size,house.selling.price$New)\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  house.selling.price$Size and house.selling.price$New\r\nt = 4.1212, df = 98, p-value = 7.891e-05\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 0.2032530 0.5399831\r\nsample estimates:\r\n      cor \r\n0.3843277 \r\n\r\nWhile controlling for size, predictor variables ‘New’ and ‘Size’ have p-values of 0.00257 and 2e-16 respectively. Both p-values are statistically significant as they are less than 0.05. This indicates that the null hypothesis can be rejected (there is no relationship between ‘New’ and ‘Price’ OR between ‘Size’ and ‘Price’ of new homes). By calculating the correlation, we can see that the correlation between ‘New’ and ‘Size’ is 0.3843, which is a wear relationship.\r\nC. Find the predicted selling price for a home of 3,000 sqft that is (a) new, and (b) not new.\r\n\r\n\r\nsqftNew <-- 40230.87+116.13*3000+57736\r\nsqftNew\r\n\r\n\r\n[1] 365895.1\r\n\r\nIf the house is new, the predicted selling price is $365,895.10\r\n\r\n\r\nsqftNotNew <-- 40230.87+116.13*3000\r\nsqftNotNew\r\n\r\n\r\n[1] 308159.1\r\n\r\nIf the house isn’t new, the predicted selling price is $308,159.10\r\nD. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\r\n\r\n\r\nsize_new_interaction <- summary(lm(Price~Size+New+Size*New, data=house.selling.price))\r\nsize_new_interaction\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432  0.15536    \r\nSize           104.438      9.424  11.082  < 2e-16 ***\r\nNew         -78527.502  51007.642  -1.540  0.12697    \r\nSize:New        61.916     21.686   2.855  0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nE. Report the lines relating the predicted selling price to the size for homes that are (a) new and (b) not new.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nggplot(data=house.selling.price,aes(x=Size,y=Price, color=New))+\r\n  geom_point()+\r\n  geom_smooth(method=\"lm\",se=F)\r\n\r\n\r\n\r\n\r\nAs can be seen in the graph, the variables in the scatterplot has a linear/correlative relationship, indicating that as size increases price does as well. However, going by the colors of the dots (which correspond to the newness of the house), the relationship isn’t as black-and-white. There are new houses (light blue dots) scattered throughout the graph, for the most part along the slope line. The older homes (dots that aren’t light blue) are mostly concentrated at the lower right end of the graph, but there are also a few that surpass the price/size of brand new houses.\r\nF. Find the predicted selling price for a home of 3,000 sqft that is (a) new and (b) not new.\r\n\r\n\r\nNew_B <-- 22227.81+104.44*3000-78527.50+61.9*3000*1\r\nNew_B\r\n\r\n\r\n[1] 398264.7\r\n\r\nThe predicted selling price for a New home with the above measurements is $398,264.70.\r\n\r\n\r\nNotNew_B <--22227.81+104.4*3000\r\nNotNew_B\r\n\r\n\r\n[1] 290972.2\r\n\r\nThe predicted selling price for a not-new home with the above measurements is $290,972.20\r\nG. Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\n\r\n\r\nNewC <-- 22227.81+104.4*1500-78527.50+61.9*1500\r\nNewC\r\n\r\n\r\n[1] 148694.7\r\n\r\nThe predicted selling price of a new 1500sqft home is $148,694.70\r\n\r\n\r\nNotNew_C <-- 22227.81+104.4*1500\r\nNotNew_C\r\n\r\n\r\n[1] 134372.2\r\n\r\nThe predicted selling price of a 1500sqft home that ISN’T new is $134,372.20\r\nCompared to the figures in part F (in which the house size is double the size at 3000sqft), the predicted selling prices in this part (G) are much lower. For a new 3000sqft house, the predicted selling price is $398,264.70, while a new 1500sqft house’s predicted selling price is $148,694.70. The size decreased by half and so too did the price, indicating that these two variables are correlated and have a linear relationship. The predicted selling price of a 3000sqft house that’s NOT new is $290,972.20. The predicted selling price of a 1500sqft house that’s not new is $134,372.20. The difference in price between the two prices is $156,000. This indicates that for “older” or simply not new houses, the price is more steeply related to size than it is in new houses.\r\nH. Do you think the model with interaction or the one without it represents the relationship of ‘Size’ and ‘New’ to the outcome price? What makes you prefer one over the other?\r\nI think a model without interaction would best show the relationship of ‘Size’ and ‘New’ with outcome price; the model with interaction best represents the relationship between ‘Size’ and ‘Price’ rather than ‘Size’ and ‘New’\r\n\r\n\r\n\r\n",
    "preview": "posts/httprpubscomkpopiela890273/distill-preview.png",
    "last_modified": "2022-04-15T17:22:13-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomemersonflemi890385/",
    "title": "DACSS 603 HW 4",
    "description": "The following document contains my homework for DACSS 603 HW 4.",
    "author": [
      {
        "name": "Emerson Fleming",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\nQuestion 1\n(SMSS 14.3, 14.4, merged & modified)\n(Data file: house.selling.price.2 from smss R package)\nFor the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.\nAnswer\n\n            P         S        Be        Ba       New\nP   1.0000000 0.8988136 0.5902675 0.7136960 0.3565540\nS   0.8988136 1.0000000 0.6691137 0.6624828 0.1762879\nBe  0.5902675 0.6691137 1.0000000 0.3337966 0.2672091\nBa  0.7136960 0.6624828 0.3337966 1.0000000 0.1820651\nNew 0.3565540 0.1762879 0.2672091 0.1820651 1.0000000\n\nP has a high correlation between S and Ba and a medium correlation followed by a medium correlation with New.\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\nWith these four predictors,\nA. For backward elimination, which variable would be deleted first? Why? Variable “Be” would be deleted first because the coefficient of Be is not statistically significant in the full model (denoted by the large p-value). With backward elimination, we start with all variables and delete the variables (in order) with the highest p-value until we have a model we like.\nB. For forward selection, which variable would be added first? Why? For forward selection, we start with nothing in the model and add variables as we go. We would want to add the variable with the highest correlation (will show up as adj-R2) with the dependent variable. In this case, it would be variable “S.” It also has the highest statistical significance (as as denoted by the p-value).\nC. Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\n\n\nCall:\nlm(formula = Be ~ S + Ba + New, data = house.selling.price.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.28797 -0.26134 -0.01249  0.25848  1.21977 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2409     0.2204  10.169  < 2e-16 ***\nS             0.9032     0.1164   7.761 1.34e-11 ***\nBa           -0.3142     0.1475  -2.130   0.0360 *  \nNew           0.2218     0.1010   2.196   0.0307 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4379 on 89 degrees of freedom\nMultiple R-squared:  0.4964,    Adjusted R-squared:  0.4794 \nF-statistic: 29.24 on 3 and 89 DF,  p-value: 2.999e-13\n\nIt does not look like the p-value for BEDS is inflated due to multicollinearity. It means that the number of beds in a house does not have a statistically significant impact on house once we control for the area (S). This can be seen in a regression without (S).\n\n\nCall:\nlm(formula = P ~ S + Be + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.184  -8.815   1.550   9.943  73.261 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -15.215      9.771  -1.557    0.123    \nS             76.825      4.621  16.624  < 2e-16 ***\nBe            -5.729      4.085  -1.402    0.164    \nNew           20.749      4.059   5.112 1.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.3 on 89 degrees of freedom\nMultiple R-squared:  0.8516,    Adjusted R-squared:  0.8466 \nF-statistic: 170.3 on 3 and 89 DF,  p-value: < 2.2e-16\n\nD. Using software with these four predictors, find the model that would be selected using each criterion: R2 Adjusted R2 PRESS AIC BIC\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1     0.869         0.863  16.4      146. 5.94e-38     4  -389.  791.\n# … with 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\n\n\nCall:\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1     0.868         0.864  16.3      195. 4.96e-39     3  -390.  789.\n# … with 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\n\n\nCall:\nlm(formula = P ~ S + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,    Adjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1     0.848         0.845  17.4      252. 1.37e-37     2  -396.  800.\n# … with 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\nWhen we drop the bathroom, r-squared goes down, adjusted r-sqaured goes down and AIC and BIC go up (which is not good). Therefore, we should not drop bathroom. Therefore, our second model is the very best.\nE. Explain which model you prefer and why.\nHere we have three models. One with the bedroom variable, one with the bedroom and the bathroom variable and one with both. R-squared prefers our original “full model,” our adjusted r-squared prefers the 2nd model. AIC prefers the second model in addition to BIC. R-squared adjusted is more worthwhile because it does not increase when we add an extra variable. Therefore, we will keep the second model and not the third or the original.\nQuestion 2\n(Data file: trees from base R) From the documentation: “This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular:\nAnswer\nA. fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\nB. Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\n\nResiduals vs. Fitted\nYes, there is a violation with the residuals vs. fitted plot. There should not be any sort of shape that demonstrates a pattern like this. The residuals should form a relatively horizontal band and no residuals should stick out. It looks like the homoscedasticity assumption is violated. Not only do we have a violation with the shape of the graph but outliers as well.\nQQ Plot\nNo, the QQ plot looks correct. The points generally fall somewhat along the line but not exactly. 31 represents somewhat of a violation as it sticks out as an outlier.\nScale-Location\nYes, there appears to be a violation. The red line is nowhere near horizontal. There is a also a sizable and not random variance in the spread. Homoscedasticity is violated here as well.\nCook’s Distance\nYes, there is a violation technically in a sense that there are outliers. The Cook’s Distance looks ok. However, we do have an observation (31) that is clearly much larger than 1/4 and this could negatively influence our inferences and predictions with the data. Therefore, this must be investigated.\nResiduals vs. Leverage\nYes, there is a violation as we have an outlier. We do not want any points inside of the red-dashed lines. Again, it is the pesky “31” observation. Otherwise, it looks mostly ideal.\nCook’s Distance vs. Leverage\nThis plot seems to be ok. These plots are confusing to interpret as we discussed in class and perhaps not necessary. However, what may be concerning would be the sudden upward trajectory on the red line. I would infer that this should be much more horizontal.\nQuestion 3\n(Data file: florida in alr R package)\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\nAnswer\n\n\n\n\n\n\nResiduals vs. Fitted\nThis looks good overall. However, Palm Beach stands out and no one observation should “stand out.” Dade stands out as well.\nQQ Plot\nThis looks fine. However, both Dade and Palm beach stand out too much.\nScale-Location\nScale location violates homoskedacicity. Additionally, Dade and Palm Beach stand out too much from the rest of the entries.\nCook’s Distance\nYes, there are violations in the form of outliers. Dade and Palm Beach stand out too much from the rest of the points. They are significantly larger compared to other observations.\nResiduals vs. Leverage\nYes, there are violations, both Dade and Palm Beach are inside of the dashed lines. This is problematic.\nCook’s Distance vs. Leverage\nThis plot is difficult to read and interpret. However, Dade and Palm Beach still seem to follow the same trajectory as they do in the other plots.\nPART 2 (Final Project)\nPlease put the data in your applications folder so that the code can be read! What is your research question for the final project? What is your hypothesis (i.e. an answer to the research question) that you want to test? Present some exploratory analysis. In particular: Numerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables). Plot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\nAnswer\nMy research question is “which factors have the most influence on car price on the used car market?” My initial hypothesis was that the power, year, make, owner_type and kilometers driven would be the largest predictors of price. I was wrong. Kilometers Driven has absolutely no statistical significance to price for this particular dataset. Therefore, it was used as a control variable (as you will see!) In actuality, Engine, Power, Owner_Type, Mileage, Year, Seats & Make are the largest predictors of Price for this dataset. Details on how this conclusion was reached will be included below.\n\n\nCars_Data <- read_csv(\"/Applications/EFleming-train-data-used-cars - train-data-used-cars (3).csv\")\nCars_Data$Make <- word(Cars_Data$Name, 1)\nCars_Data$Model <- word(Cars_Data$Name, 2)\nCars_Data <- rename(Cars_Data, \"Price\" = \"Price in $\")\n#Here, we are simply creating a new Make and Model column in the dataset to compare vehicles further. Originally, there is only a \"name\" column for cars that includes the make and model. This change allows for better analysis across makes primarily.\n\n\n\n\n\nImputed_Data <- mice(Cars_Data, m=5, method = \"rf\") #Used MICE for imputed data\n\n\n\n iter imp variable\n  1   1  Seats\n  1   2  Seats\n  1   3  Seats\n  1   4  Seats\n  1   5  Seats\n  2   1  Seats\n  2   2  Seats\n  2   3  Seats\n  2   4  Seats\n  2   5  Seats\n  3   1  Seats\n  3   2  Seats\n  3   3  Seats\n  3   4  Seats\n  3   5  Seats\n  4   1  Seats\n  4   2  Seats\n  4   3  Seats\n  4   4  Seats\n  4   5  Seats\n  5   1  Seats\n  5   2  Seats\n  5   3  Seats\n  5   4  Seats\n  5   5  Seats\n\nCars_Data_Imputed <- complete(Imputed_Data) \nCars_Data_Imputed <- na.omit(Cars_Data_Imputed) #Omitted the few variables MICE did not create a variable for. This is not exactly ideal. However, MICE has helped create inputs for several hundred NA values at this point.\nCars_Data_Imputed$Engine = as.numeric(sub(\"\\\\ .*\", \"\", Cars_Data_Imputed$Engine))\nCars_Data_Imputed$Mileage = as.numeric(sub(\"\\\\ .*\", \"\", Cars_Data_Imputed$Mileage))\nCars_Data_Imputed$Power = as.numeric(sub(\"\\\\ .*\", \"\", Cars_Data_Imputed$Power))\n##Here we are taking off the the original \"bhp (for \"Power), kmpl (for \"Mileage), and cc (for \"Engine\"). This is necesary as we cannot plot character vectors. Instead, we need the values to be numerical.\n\n\n\n\n\nCars_Data_Imputed$Make[Cars_Data_Imputed$Make == \"ISUZU\"] = \"Isuzu\"\nCars_Data_Imputed$Make[Cars_Data_Imputed$Make == \"MiniCooper\"] = \"Mini\"\nunique(Cars_Data_Imputed$Make)\n\n\n [1] \"Maruti\"        \"Hyundai\"       \"Honda\"         \"Audi\"         \n [5] \"Nissan\"        \"Toyota\"        \"Volkswagen\"    \"Tata\"         \n [9] \"LandRover\"     \"Mitsubishi\"    \"Renault\"       \"Mercedes-Benz\"\n[13] \"BMW\"           \"Mahindra\"      \"Ford\"          \"Porsche\"      \n[17] \"Datsun\"        \"Jaguar\"        \"Volvo\"         \"Chevrolet\"    \n[21] \"Skoda\"         \"Mini\"          \"Fiat\"          \"Jeep\"         \n[25] \"Smart\"         \"Ambassador\"    \"Isuzu\"         \"Force\"        \n[29] \"Bentley\"       \"Lamborghini\"  \n\n##Here we fix problems in the dataset. For instance, there are two occurrences of two makes which needed changing to unnecessary prevent noise or data distortion.\n\n\n\n\n\nCars_Data_Imputed$LPrice = log(Cars_Data_Imputed$Price) #Here, we create the logs themselves.\nCars_Data_Imputed$LEngine = log(Cars_Data_Imputed$Engine)\n\n\n\n\n\nReg_w_L <- lm(LPrice~LEngine * Power + Owner_Type + Mileage + Year + Seats + Make + Kilometers_Driven, data = Cars_Data_Imputed)\nsummary(Reg_w_L)\n\n\n\nCall:\nlm(formula = LPrice ~ LEngine * Power + Owner_Type + Mileage + \n    Year + Seats + Make + Kilometers_Driven, data = Cars_Data_Imputed)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8555 -0.1484  0.0091  0.1630  1.5010 \n\nCoefficients:\n                           Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)              -2.618e+02  2.546e+00 -102.843  < 2e-16 ***\nLEngine                   9.846e-01  2.819e-02   34.930  < 2e-16 ***\nPower                     3.167e-02  1.504e-03   21.051  < 2e-16 ***\nOwner_TypeFourth & Above  3.465e-02  9.532e-02    0.363 0.716253    \nOwner_TypeSecond         -5.760e-02  9.538e-03   -6.038 1.65e-09 ***\nOwner_TypeThird          -1.017e-01  2.639e-02   -3.854 0.000117 ***\nMileage                   4.908e-03  1.096e-03    4.477 7.72e-06 ***\nYear                      1.270e-01  1.257e-03  101.039  < 2e-16 ***\nSeats                     6.605e-02  6.240e-03   10.585  < 2e-16 ***\nMakeAudi                  1.954e-01  2.535e-01    0.771 0.440889    \nMakeBentley               1.130e+00  3.648e-01    3.098 0.001960 ** \nMakeBMW                   1.185e-01  2.537e-01    0.467 0.640304    \nMakeChevrolet            -8.184e-01  2.533e-01   -3.230 0.001243 ** \nMakeDatsun               -9.108e-01  2.620e-01   -3.476 0.000513 ***\nMakeFiat                 -7.120e-01  2.578e-01   -2.762 0.005767 ** \nMakeForce                -6.341e-01  2.913e-01   -2.177 0.029538 *  \nMakeFord                 -5.492e-01  2.527e-01   -2.173 0.029795 *  \nMakeHonda                -6.007e-01  2.527e-01   -2.377 0.017463 *  \nMakeHyundai              -5.222e-01  2.526e-01   -2.067 0.038743 *  \nMakeIsuzu                -6.813e-01  2.911e-01   -2.341 0.019291 *  \nMakeJaguar                2.740e-01  2.564e-01    1.068 0.285437    \nMakeJeep                 -3.598e-01  2.612e-01   -1.377 0.168475    \nMakeLamborghini           1.302e+00  3.632e-01    3.585 0.000339 ***\nMakeLandRover             5.057e-01  2.550e-01    1.983 0.047378 *  \nMakeMahindra             -7.006e-01  2.530e-01   -2.769 0.005641 ** \nMakeMaruti               -4.728e-01  2.526e-01   -1.872 0.061273 .  \nMakeMercedes-Benz         1.898e-01  2.533e-01    0.749 0.453672    \nMakeMini                  5.249e-01  2.576e-01    2.038 0.041635 *  \nMakeMitsubishi           -2.733e-01  2.567e-01   -1.065 0.287076    \nMakeNissan               -5.654e-01  2.537e-01   -2.228 0.025913 *  \nMakePorsche               1.800e-01  2.621e-01    0.687 0.492177    \nMakeRenault              -4.868e-01  2.533e-01   -1.921 0.054739 .  \nMakeSkoda                -4.732e-01  2.532e-01   -1.869 0.061713 .  \nMakeTata                 -8.850e-01  2.531e-01   -3.497 0.000474 ***\nMakeToyota               -3.404e-01  2.527e-01   -1.347 0.178114    \nMakeVolkswagen           -5.259e-01  2.528e-01   -2.080 0.037528 *  \nMakeVolvo                 1.215e-02  2.589e-01    0.047 0.962580    \nKilometers_Driven        -1.194e-09  3.643e-08   -0.033 0.973862    \nLEngine:Power            -3.407e-03  1.833e-04  -18.585  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2507 on 5835 degrees of freedom\n  (107 observations deleted due to missingness)\nMultiple R-squared:  0.9163,    Adjusted R-squared:  0.9158 \nF-statistic:  1682 on 38 and 5835 DF,  p-value: < 2.2e-16\n\n#Here, we can plug in our new logged \"LPrice\" and \"LEngine\" variables into our original model for a better analysis. #As we can see, there is not a considerable correlation for every make of vehicle. Additionally, there is a strong correlation with mileage and the current model (denoted by its lack of stars and very high p-value).\n\n#All variables are included at first and we implement backward elimination in order to create the best model. The process in which each variable is excluded will be included below.\n\n#The interesting thing to note is the interaction between Power and Engine. Power and Engine have a correlation as typically, the larger an engine is, the more power it brings to the table. Before I added an interaction between LEngine and Power, there were only correlations between Audi, Bentley, BMW, Datsun, Jaguar, Lamborghini, Land Rover, Mercedes-Benz, Mini, Porsche, SmartCar, and TaTa. Now the model is more inclusive (from only 12 makes to 15 makes). Our model already demonstrates that there is a high correlation between Price to Engine, Mileage, Owner Amount, Seats, Transmission, Make and Year for higher end automobiles only.\n\n#We also will take out Transmission as this only seems to be less inclusive when added. In other words, we have statistical significance with 16 makes without Transmission versus 15 makes with Transmission.\n\n#Adding Fuel_Type also seems to make the data less inclusive so no point in adding this variable either.\n\n#There does appear to be some correlation between location but not a lot and adding this variable fails to create any more makes with statistical significance. Therefore, it will not be added.\n\n#Strangely, there is no correlation between LPrice and Kilometers_Driven so we will use Mileage (Gas Mileage that is) instead.\n\n#Ultimately, we end up with Statistical Significance across 16 makes.\n\n#We see the highest correlation between LPrice and Year, Lprice and Power and LPrice and Seats. Therefore, these will be the 3 we graphically represent before we build predictive models.\n\n#Kilometers Driven will serve as the control variable due to its very low (if any) statistical significance.\n\n\n\nAccording to my analysis, Engine (size in cc’s), Power, Year and Seats have the largest statistical significance and play the largest role in determining the price for the most car manufacturers. Therefore, Price (as the most important factor) represents my outcome variable while LEngine, Power, Owner_Type, Mileage, Year, Seats & Make represent explanatory variables. I used Backward Elimination as seen above. Therefore, it will be these variables that will be utilized to create and graphically represent a predictive model. Kilometers_Driven (or mileage) will be used as the control variable as for some reason, this seems to have very little effect on the price of the used car in India where this data was extracted.\n\n\ndata(\"Cars_Data_Imputed\")\npairs(Cars_Data_Imputed[,c(5, 6, 10:14)], pch=2)\n\n\n\n\n\n\ncor(na.omit(Cars_Data_Imputed[,c(5, 6, 10:14)]))\n\n\n                          Year Kilometers_Driven     Mileage\nYear               1.000000000      -0.169396153  0.28519061\nKilometers_Driven -0.169396153       1.000000000 -0.06066936\nMileage            0.285190612      -0.060669362  1.00000000\nEngine            -0.067918338       0.093011548 -0.63602818\nPower              0.014612185       0.033469705 -0.53772941\nSeats              0.007959909       0.083042604 -0.33071080\nPrice              0.299543471      -0.008298378 -0.34055408\n                       Engine       Power        Seats        Price\nYear              -0.06791834  0.01461219  0.007959909  0.299543471\nKilometers_Driven  0.09301155  0.03346970  0.083042604 -0.008298378\nMileage           -0.63602818 -0.53772941 -0.330710803 -0.340554076\nEngine             1.00000000  0.86630307  0.401130094  0.658044052\nPower              0.86630307  1.00000000  0.101487024  0.772836621\nSeats              0.40113009  0.10148702  1.000000000  0.055616037\nPrice              0.65804405  0.77283662  0.055616037  1.000000000\n\nThe correlations between each variables have intuitive positive and negative signs as expected. Price with relation to power and engine have relatively high correlations. Price to seats and mileage have weak correlations and kilometers driven has no correlation. Seats appears to have low correlation but is actually statistically significant as the model above demonstrates.\n\n\n\n",
    "preview": "posts/httprpubscomemersonflemi890385/distill-preview.png",
    "last_modified": "2022-04-15T17:22:19-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomclairebattagliahomework-4-603/",
    "title": "Homework 4",
    "description": "Model Building & Evaluation | Towards My Final Project",
    "author": [
      {
        "name": "Claire Battaglia",
        "url": "https://rpubs.com/clairebattaglia"
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\n\nContents\nPart 1\nQuestion 1\nSolution\nA\nB\nC\nD\nE\n\n\nQuestion 2\nSolution\nA\nB\n\n\nQuestion 3\nSolution\n\n\nPart 2\n\nPart 1\nQuestion 1\nFor backward elimination, which variable would be deleted first? Why?\nFor forward selection, which variable would be added first? Why?\nWhy do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\nUsing software with these four predictors, find the model that would be selected using each criterion:\nR2\nAdjusted R2\nPRESS\nAIC\nBIC\n\nExplain which model you prefer and why.\nSolution\nFirst I’ll load the data and recreate both the correlation matrix and regression model. I really like correlation plots for visualizing correlation so I’ll create one using the ggcorr() function from the GGally package.\n\n\nShow code\n\n# load data\ndata(\"house.selling.price.2\")\n\n# create object\nhSP2 <- house.selling.price.2\n\n# create cor matrix\nround(cor(hSP2), 3)\n\n\n        P     S    Be    Ba   New\nP   1.000 0.899 0.590 0.714 0.357\nS   0.899 1.000 0.669 0.662 0.176\nBe  0.590 0.669 1.000 0.334 0.267\nBa  0.714 0.662 0.334 1.000 0.182\nNew 0.357 0.176 0.267 0.182 1.000\n\nShow code\n\n# create cor plot\nggcorr(hSP2, label = TRUE, label_round = 3)\n\n\n\nShow code\n\n# fit model\nfithSP2 <- lm(P ~ . , hSP2)\n\n# get summary\nsummary(fithSP2)\n\n\n\nCall:\nlm(formula = P ~ ., data = hSP2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453             0.000855 ***\nS             64.761      5.630  11.504 < 0.0000000000000002 ***\nBe            -2.766      3.960  -0.698             0.486763    \nBa            19.203      5.650   3.399             0.001019 ** \nNew           18.984      3.873   4.902            0.0000043 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 0.00000000000000022\n\nA\nBackward elimination is one of the three primary automated variable selection methods. It works as follows: fit a model including all possible predictor variables and one-by-one remove any variable that is not statistically significant (according to a predetermined \\(\\alpha\\)-level), refitting the model after removing each variable and continuing until the model contains only statistically significant predictor variables.\nWe’ve already fit the model using all possible predictor variables (above). Let’s determine the \\(\\alpha\\)-level we’re looking for to be .05. Every variable except bed (the number of bedrooms in the house) is statistically significant at that level so backward elimination would lead us to remove bed first.\n\n\nShow code\n\n# refit model w/o be\nfitBack <- lm(P ~ . - Be, hSP2)\n\n# get summary\nsummary(fitBack)\n\n\n\nCall:\nlm(formula = P ~ . - Be, data = hSP2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847         0.0000000815 ***\nS             62.263      4.335  14.363 < 0.0000000000000002 ***\nBa            20.072      5.495   3.653             0.000438 ***\nNew           18.371      3.761   4.885         0.0000045396 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 0.00000000000000022\n\nNow every variable in the model is statistically significant. In fact, removing bed has increased the significance level of bath (the number of bathrooms).\nBackward elimination would thus lead us to select the model with three predictor variables: size, bath, and new.\nI can confirm this using the step() function.\n\n\nShow code\n\n# backward elimination\nstep(object = fithSP2, # specify full model\n     direction = \"backward\") # specify backward\n\n\nStart:  AIC=524.7\nP ~ S + Be + Ba + New\n\n       Df Sum of Sq   RSS    AIC\n- Be    1       131 23684 523.21\n<none>              23553 524.70\n- Ba    1      3092 26645 534.17\n- New   1      6432 29985 545.15\n- S     1     35419 58972 608.06\n\nStep:  AIC=523.21\nP ~ S + Ba + New\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n- Ba    1      3550 27234 534.20\n- New   1      6349 30033 543.30\n- S     1     54898 78582 632.75\n\nCall:\nlm(formula = P ~ S + Ba + New, data = hSP2)\n\nCoefficients:\n(Intercept)            S           Ba          New  \n     -47.99        62.26        20.07        18.37  \n\nB\nForward selection is another primary method of automated variable selection. It works as follows: begin with the intercept-only model and one-by-one add each variable that is statistically significant.\n\n\nShow code\n\n# fit model s\nfitS <- lm(P ~ S, data = hSP2)\n\n# fit model be\nfitBe <- lm(P ~ Be, data = hSP2)\n\n# fit model ba\nfitBa <- lm(P ~ Ba, data = hSP2)\n\n# fit model new\nfitN <- lm(P ~ New, data = hSP2)\n\n# view all to compare\nstargazer(fitS, fitBe, fitBa, fitN,\n          type = \"text\",\n          report = (\"vctp*\"))\n\n\n\n===================================================================================\n                                               Dependent variable:                 \n                              -----------------------------------------------------\n                                                        P                          \n                                   (1)          (2)          (3)           (4)     \n-----------------------------------------------------------------------------------\nS                                75.607                                            \n                               t = 19.561                                          \n                              p = 0.000***                                         \n                                                                                   \nBe                                             42.969                              \n                                             t = 6.976                             \n                                            p = 0.000***                           \n                                                                                   \nBa                                                          76.026                 \n                                                          t = 9.720                \n                                                         p = 0.000***              \n                                                                                   \nNew                                                                      34.158    \n                                                                        t = 3.641  \n                                                                      p = 0.0005***\n                                                                                   \nConstant                         -25.194      -37.229      -49.248       89.249    \n                               t = -3.767    t = -1.866   t = -3.148   t = 17.336  \n                              p = 0.0003***  p = 0.066*  p = 0.003*** p = 0.000*** \n                                                                                   \n-----------------------------------------------------------------------------------\nObservations                       93            93           93           93      \nR2                                0.808        0.348        0.509         0.127    \nAdjusted R2                       0.806        0.341        0.504         0.118    \nResidual Std. Error (df = 91)    19.473        35.861       31.119       41.506    \nF Statistic (df = 1; 91)       382.628***    48.660***    94.473***     13.254***  \n===================================================================================\nNote:                                                   *p<0.1; **p<0.05; ***p<0.01\n\nWe can see that all of the variables are statistically significant and while the stargazer() function doesn’t display the full \\(P\\)-value, we can see that size has the largest \\(t\\) test statistic so that’s the variable we would add in first. If we look at our correlation matrix/plot, we can see that size is also the most highly correlated with price.\nOnce again, I can confirm this using the step() function.\n\n\nShow code\n\nfitNull <- lm(P ~ 1, data = hSP2) # fit null model\nstep(object = fitNull, # pass null model to step function\n     direction = \"forward\", # specify forward\n     scope = P ~ S + Be + Ba + New) # specify maximum model\n\n\nStart:  AIC=705.63\nP ~ 1\n\n       Df Sum of Sq    RSS    AIC\n+ S     1    145097  34508 554.22\n+ Ba    1     91484  88121 641.41\n+ Be    1     62578 117028 667.79\n+ New   1     22833 156772 694.99\n<none>              179606 705.63\n\nStep:  AIC=554.22\nP ~ S\n\n       Df Sum of Sq   RSS    AIC\n+ New   1    7274.7 27234 534.20\n+ Ba    1    4475.6 30033 543.30\n<none>              34508 554.22\n+ Be    1      40.4 34468 556.11\n\nStep:  AIC=534.2\nP ~ S + New\n\n       Df Sum of Sq   RSS    AIC\n+ Ba    1    3550.1 23684 523.21\n+ Be    1     588.8 26645 534.17\n<none>              27234 534.20\n\nStep:  AIC=523.21\nP ~ S + New + Ba\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n+ Be    1    130.55 23553 524.70\n\nCall:\nlm(formula = P ~ S + New + Ba, data = hSP2)\n\nCoefficients:\n(Intercept)            S          New           Ba  \n     -47.99        62.26        18.37        20.07  \n\nIn this case, forward selection gives us the same model as backward elimination:\n\\[P=-47.99+62.26*size+18.37*new+20.07*bath\\]\nC\nThe correlation between bed and price is .59, indicating a fairly strong relationship between the two variables. Yet in our multiple regression model, the \\(P\\)-value of bed is .486763, indicating that we can’t be sure that there is any relationship between the two (i.e. that the coefficient/slope isn’t zero). While these two statistics appear to nudge us towards two different conclusions about the relationship between these two variables, I believe what we’re actually seeing is the effect of bed being related to other predictor variables in the model.\nBed and size are even more highly correlated (.669) than bed and price (.59) and size and price are very highly correlated with one another (.899). This means that it’s hard to increase size without increasing bed, meaning that any increase in bed is likely already captured by the increase in size in the model and thus that bed doesn’t add much explanatory power to the model. A similar, albeit weaker, effect can be seen with bed and bath.\nD\nWe have two models already: our initial model with all possible predictor variables and the model we arrived at using backward elimination & forward selection. I’ll use stepwise selection to see if perhaps that leads us to a third model.\n\n\nShow code\n\nstep(object = fitNull, # pass null model to step function\n     direction = \"both\", # specify both for stepwise selection\n     scope = P ~ S + Be + Ba + New) # specify maximum model\n\n\nStart:  AIC=705.63\nP ~ 1\n\n       Df Sum of Sq    RSS    AIC\n+ S     1    145097  34508 554.22\n+ Ba    1     91484  88121 641.41\n+ Be    1     62578 117028 667.79\n+ New   1     22833 156772 694.99\n<none>              179606 705.63\n\nStep:  AIC=554.22\nP ~ S\n\n       Df Sum of Sq    RSS    AIC\n+ New   1      7275  27234 534.20\n+ Ba    1      4476  30033 543.30\n<none>               34508 554.22\n+ Be    1        40  34468 556.11\n- S     1    145097 179606 705.63\n\nStep:  AIC=534.2\nP ~ S + New\n\n       Df Sum of Sq    RSS    AIC\n+ Ba    1      3550  23684 523.21\n+ Be    1       589  26645 534.17\n<none>               27234 534.20\n- New   1      7275  34508 554.22\n- S     1    129539 156772 694.99\n\nStep:  AIC=523.21\nP ~ S + New + Ba\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n+ Be    1       131 23553 524.70\n- Ba    1      3550 27234 534.20\n- New   1      6349 30033 543.30\n- S     1     54898 78582 632.75\n\nCall:\nlm(formula = P ~ S + New + Ba, data = hSP2)\n\nCoefficients:\n(Intercept)            S          New           Ba  \n     -47.99        62.26        18.37        20.07  \n\nStepwise selection leads us to the same model as backward elimination & forward selection so I’ll be comparing two models:\nP ~ S + Be + Ba + New\nP ~ S + New + Ba\nLet’s look at \\(R^2\\) first.\nFor model P ~ S + Be + Ba + New, \\(R^2\\)=.8689\nFor model P ~ S + New + Ba, \\(R^2\\)=.8681\nBased on the \\(R^2\\) values of these two models, model P ~ S + Be + Ba + New is the better model.\nNext we can look at the \\(R^2_{adj}\\) values for each model.\nFor model P ~ S + Be + Ba + New, \\(R^2_{adj}\\)=.8629\nFor model P ~ S + New + Ba, \\(R^2_{adj}\\)=.8637\nBased on the \\(R^2_{adj}\\) values, model P ~ S + New + Ba is the better model.\nNext we can look at the PRESS statistic for each model.\n\n\nShow code\n\n# calculate press\nPRESS(fithSP2) # for P ~ S + Be + Ba + New\n\n\n[1] 28390.22\n\nShow code\n\nPRESS(fitBack) # for P ~ S + New + Ba\n\n\n[1] 27860.05\n\nBased on the PRESS statistic for each model, model P ~ S + New + Ba is the better model.\nFinally we can look at the AIC and BIC statistics for both models.\n\n\nShow code\n\n# aic\nAIC(fithSP2, fitBack)\n\n\n        df      AIC\nfithSP2  6 790.6225\nfitBack  5 789.1366\n\nShow code\n\n# bic\nBIC(fithSP2, fitBack)\n\n\n        df      BIC\nfithSP2  6 805.8181\nfitBack  5 801.7996\n\nBased on both the AIC and BIC statistics, model P ~ S + New + Ba is the better model.\nE\nIt seems clear in this case that model P ~ S + New + Ba is the better model. Every criterion except \\(R^2\\) points us to that model. Given that \\(R^2\\) always increases with additional predictor variables, this was to be expected.\nUltimately we know that for our second model 1) all of the predictor variables are statistically significant, which is not true of the first model, 2) it scores better on every criterion but one, and 3) there is a logical reason for removing bed from the model.\nQuestion 2\n“This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\nfit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\nRun regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\nSolution\nFirst I’ll load the data. I’ll also rename the variable girth to diameter.\n\n\nShow code\n\n# load data\ndata(\"trees\")\n\n# get string\nstr(trees)\n\n\n'data.frame':   31 obs. of  3 variables:\n $ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...\n $ Height: num  70 65 63 72 81 83 66 75 80 75 ...\n $ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...\n\nShow code\n\n# rename girth to diameter\ntrees <- trees %>%\n  rename(Diameter = Girth)\n\n# preview\nhead(trees, 5)\n\n\n  Diameter Height Volume\n1      8.3     70   10.3\n2      8.6     65   10.3\n3      8.8     63   10.2\n4     10.5     72   16.4\n5     10.7     81   18.8\n\nA\nNow I’ll fit a model regressing tree volume onto diameter and height.\n\n\nShow code\n\n# fit model\nfitTrees <- lm(Volume ~ . , data = trees)\n\n# get summary\nsummary(fitTrees)\n\n\n\nCall:\nlm(formula = Volume ~ ., data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713          0.000000275 ***\nDiameter      4.7082     0.2643  17.816 < 0.0000000000000002 ***\nHeight        0.3393     0.1302   2.607               0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 0.00000000000000022\n\nThis appears to be quite a good model. This makes sense, given that we’re trying to understand the nature of a physical object which has a formula for calculating what we’re interested in.\nBoth diameter and height are statistically significant predictors of volume and both the \\(R^2\\) and \\(R^2_{adj}\\) values are quite high.\nB\nTo further evaluate this model, I’ll use the autoplot() function from the ggfortify package to produce a series of diagnostic plots to test some specific assumptions upon which the model is based.\n\n\nShow code\n\n# create diagnostic plots\nautoplot(fitTrees, which = 1:6, ncol = 3) +\n  theme_minimal()\n\n\n\n\nMoving clockwise from the top, left-hand corner we have:\nResiduals vs Fitted: This plot allows us to assess whether the assumptions of linearity and constant variance of errors have been violated. We would like to see the residuals scattered around a horizontal line in a random manner (i.e. not in any discernible pattern). We do not see that here in this plot, indicating that perhaps a nonlinear model would be more appropriate and that variance is not constant.\nNormal Q-Q: This plot helps us get a sense of whether the assumption of normality has been violated. We would like to see the points fall more or less along the diagonal line. That holds mostly true for this model, suggesting that the data are normally distributed.\nScale-Location: This plot diagnoses whether the assumption of constant variance has been violated. As with the Residuals vs Fitted plot, we would like to see the points scattered around a horizontal line in a random manner. We do not see that here.\nCook’s distance: This plot helps us identify highly influential observations in the dataset. Inferences based on datasets containing highly influential observations are not representative. For this dataset, \\(n\\)=31, meaning that we’re concerned about Cook’s distance values greater than \\(4/31\\), or 0.129. We have at least a few values greater than that, indicating that there are several observations in the dataset that are highly influential.\nResiduals vs Leverage: This plot allows us to identify \\(x\\) values that have high leverage. A value greater than \\(2(p + 1)/n\\) is considered to have high leverage so for this dataset, a leverage value greater than 0.194 would be considered high. This dataset appears to contain at least one high leverage observation.\nCook’s dist vs Leverage: This plot shows observations that have a high Cook’s distance and/or high leverage. Again, we have at least one observation that has both high Cook’s distance and high leverage and a couple that have high leverage.\nBased on the plots alone, I believe the assumptions of linearity, constant variance of errors, and influential observations have all been violated.\nBased on the Normal Q-Q plot, I do not believe the assumption of normality has been violated. That being said, the Shapiro-Wilk test allows us formally test that assumption.\n\n\nShow code\n\n# shapiro wilk test\nshapiro.test(trees$Diameter) # diameter\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  trees$Diameter\nW = 0.94117, p-value = 0.08893\n\nShow code\n\nshapiro.test(trees$Height) # height\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  trees$Height\nW = 0.96545, p-value = 0.4034\n\nFor this test, \\(H_0\\): the observations are normally distributed and \\(H_a\\): the observations are not normally distributed.\nThus the \\(P\\)-values of .08893 (diameter) and .4034 (height) indicate that the observations for each variable are normally distributed (more precisely, we cannot reject the null hypothesis of normal distribution).\nBased on the Scale-Location plot, I believe that the assumption of homoscedasticity (i.e. constant variance) has been violated but I will use the Breusch-Pagan test to formally test whether that is true.\nFor this test, \\(H_0\\): homoscedasticity and \\(H_a\\): heteroscedasticity.\n\n\nShow code\n\n# bp test\nbptest(fitTrees)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  fitTrees\nBP = 2.4681, df = 2, p-value = 0.2911\n\nSomewhat surprisingly, the \\(P\\)-value is .2911, meaning that we cannot reject the null hypothesis of homoscedasticity at this time.\nQuestion 3\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\nSolution\nFirst I’ll load and inspect the data.\n\n\nShow code\n\n# load data\ndata(\"florida\")\n\n# get string\nstr(florida)\n\n\n'data.frame':   67 obs. of  3 variables:\n $ Gore    : int  47300 2392 18850 3072 97318 386518 2155 29641 25501 14630 ...\n $ Bush    : int  34062 5610 38637 5413 115185 177279 2873 35419 29744 41745 ...\n $ Buchanan: int  262 73 248 65 570 789 90 182 270 186 ...\n\nShow code\n\n# preview\nhead(florida, 5)\n\n\n          Gore   Bush Buchanan\nALACHUA  47300  34062      262\nBAKER     2392   5610       73\nBAY      18850  38637      248\nBRADFORD  3072   5413       65\nBREVARD  97318 115185      570\n\nBefore I do anything else, I’m going to visualize the data.\n\n\nShow code\n\n# pivot longer\nfloridaLong <- florida %>%\n  rownames_to_column(\"County\") %>%\n  pivot_longer(cols = c(`Gore`, `Bush`, `Buchanan`),\n               names_to = \"Candidate\",\n               values_to = \"Votes\")\n\n# create bar plot\nggplot(floridaLong, aes(fill = Candidate, y = Votes, x = County)) +\n  geom_bar(position = \"stack\", stat = \"identity\") +\n  labs(title = \"Votes by County\") + \n  theme_minimal() +\n  coord_flip() +\n  theme(axis.text.y = element_text(hjust = .75, size = 4))\n\n\n\n\nThis plot shows the number of votes each candidate received in each county. It’s somewhat ridiculous because of the scale but we can see that Buchanan received more votes in Palm Beach County than in any other county.\nThis perhaps alerts us to the distribution of votes in Palm Beach County being atypical but can’t tell us much more than that.\nTo better answer the question of whether Palm Beach County is an outlier, I’ll fit a model regressing Buchanan’s share of the vote onto Bush’s share of the vote and then produce some diagnostic plots to help evaluate the model.\n\n\nShow code\n\n# fit model\nfitVote <- lm(Buchanan ~ Bush, data = florida)\n\n# get summary\nsummary(fitVote)\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n              Estimate Std. Error t value     Pr(>|t|)    \n(Intercept) 45.2898613 54.4794230   0.831        0.409    \nBush         0.0049168  0.0007644   6.432 0.0000000173 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 0.00000001727\n\nThe first thing to note is that Bush’s share of the vote is a statistically significant predictor of Buchanan’s share of the vote. That is, we can be confident that there is a relationship between the two variables.\nThat being said, both the \\(R^2\\) and \\(R^2_{adj}\\) values are quite low, indicating that this model explains relatively little of the variation in Buchanan’s share of the vote.\nNow we can take a look at the diagnostic plots for this model.\n\n\nShow code\n\n# create diagnostic plots\nautoplot(fitVote, which = 1:6, ncol = 3) +\n  theme_minimal()\n\n\n\n\nMoving clockwise from the top, left-hand corner we have:\nResiduals vs Fitted: The residual for Palm Beach County is extremely large, which indicates that the observed value was significantly greater than the predicted value. It falls outside the pattern of residuals, suggesting that it is an outlier.\nNormal Q-Q: The data appear to be normally distributed with the exception of Palm Beach County, which falls quite far outside the distribution.\nScale-Location: We see both an increasing trend and increasing variance, indicating that the assumption of homoscedasticity has been violated. And, again, Palm Beach County appears to be an outlier.\nCook’s distance: This plot indicates that Miami-Dade and Palm Beach counties are both influential observations. That is, removing those observations from the model would change the model in a meaningful way.\nResiduals vs Leverage: This plot indicates that Bush’s share of the vote in Palm Beach County is influential.\nCook’s dist vs Leverage: Again, we can see that Palm Beach County has a high Cook’s distance and Miami-Dade County has both high Cook’s distance and high leverage.\nBased on the above diagnostic plots, we can say that, yes, Palm Beach County does appear to be an outlier.\nWe can also pass the model to a test that will test specifically for outliers.\n\n\nShow code\n\n# bonferroni test\noutlierTest(fitVote)\n\n\n           rstudent                       unadjusted p-value\nPALM BEACH 24.08014 0.00000000000000000000000000000000086246\n                                     Bonferroni p\nPALM BEACH 0.000000000000000000000000000000057785\n\nThis confirms our conclusion based on the diagnostic plots: Palm Beach County is an outlier. None of these tests explain why it is an outlier (that is, what happened on election night) but they do tell us that it warrants further attention.\nThis question isn’t ultimately concerned with fitting a good model to predict vote share but if it were, these plots suggest that we might consider fitting a model without the two outliers/influential points.\nPart 2\nWhat is your research question for the final project?\nWhat is your hypothesis (i.e. an answer to the research question) that you want to test?\nPresent some exploratory analysis. In particular:\nNumerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables).\nPlot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\n\nI am interested in exploring predictors of happiness. I am specifically interested in this right now because of the impact of the COVID-19 pandemic. So much advice was given about how to maintain some degree of contentedness in the midst of the sea change wrought by the pandemic and I am curious whether people who spent more or less time engaged in different activities experienced different levels of happiness.\nIt seems as though people generally fell into one of two camps and either leaned into the “Netflix and chill” mode of survival or hit the trails—many for the first time—to pass the time. This leads me to the two variables I’ll be looking at.\n\nThis is not to imply that these are the only two things people did to get through the pandemic or that they are mutually exclusive. These are just two that I noticed a lot. Plus the GSS doesn’t have a question about how frequently you bake sourdough bread.\nMy research question is “How does amount of TV watched and frequency of leisure time in nature affect happiness?” I’ll be looking at data from the General Social Survey (GSS) from 2021.\nThe outcome variable is happiness. The question asked is “Taken all together, how would you say things are these days–would you say that you are very happy, pretty happy, or not too happy?”\nMy predictor variables are:\nHours per day watching TV: “On the average day, about how many hours do you personally watch television?”\nOutside leisure activity in the past 12 months: “In the last twelve months how often, if at all, have you engaged in any leisure activities outside in nature, such as hiking, bird watching, swimming, skiing, other outdoor activities, or just relaxing?”\nMy control variables are:\nCondition of health: “Would you say your own health, in general, is excellent, good, fair, or poor?”\nSatisfaction with financial situation: “So far as you and your family are concerned, would you say that you are pretty well satisfied with your present financial situation, more or less satisfied, or not satisfied at all?”\nMy null and alternative hypotheses are:\n\\(H_0:\\) number of hours of TV watched per day has no effect on level of happiness\n\\(H_a:\\) number of hours of TV watched per day has some effect on level of happiness\n\\(H_0:\\) frequency of leisure time in nature has no effect on level of happiness\n\\(H_a:\\) frequency of leisure time in nature has some effect on level of happiness\n\n\nShow code\n\n# read data\ngSS <- read_excel(\"GSS.xlsx\", sheet = \"Data\")\n\n# convert to dataframe\ngSS <- as.data.frame(unclass(gSS), stringsAsFactors = TRUE)\n\n\n\n\n\nShow code\n\n# create subset\ngSS <- gSS %>%\n  subset(select = c(\"happy\", \"health\", \"satfin\", \"tvhours\", \"activnat\"))\n\n# convert 0 hours to 0\ngSS$tvhours <- sub(\"0 hours\", \"0\", gSS$tvhours)\n\n# convert tvhours from factor to numeric\ngSS$tvhours <- as.numeric(as.character(gSS$tvhours))\n\n# get summary\nsummary(gSS)\n\n\n           happy            health                        satfin   \n Not too happy: 331   Excellent: 337   More or less satisfied:769  \n Pretty happy :1087   Fair     : 338   Not satisfied at all  :420  \n Very happy   : 347   Good     :1011   Pretty well satisfied :576  \n                      Poor     :  79                               \n                                                                   \n                                                                   \n                                                                   \n    tvhours                        activnat  \n Min.   : 0.000   Daily                :328  \n 1st Qu.: 2.000   Never                : 85  \n Median : 3.000   Several times a month:473  \n Mean   : 3.378   Several times a week :528  \n 3rd Qu.: 4.000   Several times a year :351  \n Max.   :23.000                              \n NA's   :12                                  \n\nAll of the variables are categorical variables except hours of TV watched per day, which is a discrete numerical variable. That is, the respondent could select from a specified set of values. The most basic summary then is a simply a count of the frequency of each value.\n\n\nShow code\n\n# create table health and happy\ntable(gSS$health, gSS$happy)\n\n\n           \n            Not too happy Pretty happy Very happy\n  Excellent            32          173        132\n  Fair                 98          209         31\n  Good                157          676        178\n  Poor                 44           29          6\n\nShow code\n\n# create table financial satisfaction and happy\ntable(gSS$satfin, gSS$happy)\n\n\n                        \n                         Not too happy Pretty happy Very happy\n  More or less satisfied           117          519        133\n  Not satisfied at all             152          226         42\n  Pretty well satisfied             62          342        172\n\n\n\nShow code\n\n# create bar plot\nggplot(gSS, aes(x = happy)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Self-Reported Happiness Level\", x = NULL, y = \"Count\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\nShow code\n\n# plot health and happy\nggplot(gSS, aes(x = health,\n                fill = happy)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"Assessment of Health and Level of Happiness\", x = \"Assessment of health\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\nShow code\n\n# plot financial satisfaction and happy\nggplot(gSS, aes(x = satfin,\n                fill = happy)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"Financial Satisfcation and Level of Happiness\", x = \"Satisfaction with financial situation\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\nShow code\n\n# plot tv and happy\nggplot(gSS, aes(x = tvhours,\n                fill = happy)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"TV Watched and Level of Happiness\", x = \"Hours watched per day\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\nShow code\n\n# plot nature and happy\nggplot(gSS, aes(x = activnat,\n                fill = happy)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"Leisure Time in Nature and Level of Happiness\", x = \"Frequency of leisure time in nature\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nI don’t think I’ve hit on the best way to visualize the relationship between each set of variables yet but I’ll keep working on it as I continue my analysis.\n\n\n\n",
    "preview": "posts/httpsrpubscomclairebattagliahomework-4-603/distill-preview.png",
    "last_modified": "2022-04-15T17:22:23-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomowenvespa890475/",
    "title": "Model Evaluation",
    "description": "DACSS 603 Homework 4",
    "author": [
      {
        "name": "Rhowena Vespa",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\r\n\r\n\r\nlibrary(distill)\r\nlibrary(dplyr)\r\nlibrary(readr)\r\nlibrary(tidyverse)\r\nlibrary(knitr)\r\nlibrary(\"readxl\")\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nlibrary(smss)\r\nlibrary(broom)\r\nlibrary(qpcR)\r\n\r\n\r\n\r\nQuestion 1\r\n(SMSS 14.3, 14.4, merged & modified)\r\n(Data file: house.selling.price.2 from smss R package)\r\n\r\n\r\ndata(\"house.selling.price.2\") \r\ndim(house.selling.price.2)\r\n\r\n\r\n[1] 93  5\r\n\r\nFor the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.\r\nWith these four predictors,\r\nA. For backward elimination, which variable would be deleted first? Why?\r\nPredictor Beds has the highest p-value and should be deleted first using backward elimination.\r\nB. For forward selection, which variable would be added first? Why?\r\nPredictor Size has a p-value = 0 and should be added first using forward selection since it is known that Size affects price based on high correlation coefficient of 0.899.\r\nC.Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\r\nThe large p-value for predictor BEDS proves that we failed to reject the null hypothesis and it is NOT statistically significant. The correlation coefficient of 0.590 indicates a moderate positive correlation between BEDS and PRICE.\r\nLarge p-value and substantial correlation is possible because of low sample size of 93 observations.\r\nD. Using software with these four predictors, find the model that would be selected using each criterion:R2 ,Adjusted R2, PRESS, AIC,BIC\r\n\r\n\r\nHW4Q1.lm <-lm(P ~ S +Ba +New, data = house.selling.price.2)\r\nsummary(HW4Q1.lm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-34.804  -9.496   0.917   7.931  73.338 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\r\nS             62.263      4.335  14.363  < 2e-16 ***\r\nBa            20.072      5.495   3.653 0.000438 ***\r\nNew           18.371      3.761   4.885 4.54e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 16.31 on 89 degrees of freedom\r\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \r\nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\nModel1 <-broom::glance(HW4Q1.lm)\r\nkable(Model1)\r\n\r\n\r\nr.squared\r\nadj.r.squared\r\nsigma\r\nstatistic\r\np.value\r\ndf\r\nlogLik\r\nAIC\r\nBIC\r\ndeviance\r\ndf.residual\r\nnobs\r\n0.8681361\r\n0.8636912\r\n16.31279\r\n195.3127\r\n0\r\n3\r\n-389.5683\r\n789.1366\r\n801.7996\r\n23683.53\r\n89\r\n93\r\n\r\n\r\n\r\nqpcR::PRESS(HW4Q1.lm)\r\n\r\n\r\n.........10.........20.........30.........40.........50\r\n.........60.........70.........80.........90...\r\n$stat\r\n[1] 27860.05\r\n\r\n$residuals\r\n [1]   8.56463672  -0.03939913 -14.71598185 -27.21082231  84.40060532\r\n [6]  22.36861006 -35.08530974   6.82244427  15.21170995  -0.51102480\r\n[11]   3.88826737  -9.89311754  28.17496046   6.77600635 -15.95800509\r\n[16]  19.40028822  -4.05570104  -3.66605875 -17.19127450   0.53441460\r\n[21]   1.08005016  22.59699698  -4.94082730   4.26577625  -7.19186400\r\n[26] -16.66072432   5.03817053  -9.51913168  12.68588453 -32.01221384\r\n[31] -17.80422930 -19.82438535   7.00571544   9.12956093   4.26928129\r\n[36]   4.12150932 -22.85909891 -22.85909891 -22.85909891 -10.89668500\r\n[41] -35.85354717  15.08107642  14.24947237  17.92355801  -8.99713028\r\n[46]  -0.67074479   1.36360013   0.83787249  -4.83067593 -11.34598983\r\n[51]  12.55261095  -7.37350222 -11.92917395   4.40174996 -10.00181947\r\n[56]   3.51445921   3.27168316   5.68233212 -19.23631708   5.73458246\r\n[61]  11.80266132  -1.67544951 -14.49966704   0.16479976 -30.20396045\r\n[66]   8.40470609   8.94132534  -7.88286035  -2.08006411  25.41685658\r\n[71] -14.00283697  -6.20668743   1.88916976  21.70704935  19.17475862\r\n[76] -18.58720728   8.22508917  14.45313627   0.43946312  -2.76227404\r\n[81]   1.18971200   4.43389546   2.90665652  -2.47643784   1.32976835\r\n[86]  19.68889493   0.96971063   6.78392946 -38.94641062  12.05882289\r\n[91]  23.58446546  -0.67223759  36.15367379\r\n\r\n$P.square\r\n[1] 0.8448823\r\n\r\n\r\n\r\nHW4Q1a.lm <-lm(P ~ S +New, data = house.selling.price.2)\r\nsummary(HW4Q1a.lm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = P ~ S + New, data = house.selling.price.2)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-47.207  -9.763  -0.091   9.984  76.405 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\r\nS             72.575      3.508  20.690  < 2e-16 ***\r\nNew           19.587      3.995   4.903 4.16e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 17.4 on 90 degrees of freedom\r\nMultiple R-squared:  0.8484,    Adjusted R-squared:  0.845 \r\nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\nqpcR::PRESS(HW4Q1a.lm)\r\n\r\n\r\n.........10.........20.........30.........40.........50\r\n.........60.........70.........80.........90...\r\n$stat\r\n[1] 31066\r\n\r\n$residuals\r\n [1]  -5.37843622   8.02122284 -11.32777134 -11.57833539  87.66277233\r\n [6]  15.70373259 -48.13451672  -3.26222816   4.46834650 -12.70206532\r\n[11]  -7.14694966  -2.45551793  14.57698204  14.71553576 -11.36487654\r\n[16]   3.52135636   1.11464899   1.39393760 -14.55151878   5.99060812\r\n[21]   6.21290631   5.80504572  -0.62375565  10.21933262  -3.93350494\r\n[26] -14.45062954  10.77312074  -5.61030428  19.07090388 -31.70842424\r\n[31] -16.23266330 -18.67643485  11.57410928  13.58192784   8.12093808\r\n[36]   9.54042514 -22.09514196 -22.09514196 -22.09514196 -10.27827037\r\n[41] -39.23815733  20.21365336  18.67311241  23.23662761  -8.06355620\r\n[46]   1.62301000   3.65438911   3.97178120  -3.31513157 -10.15177555\r\n[51]  15.23927592  -8.22379429 -11.59722121   7.09431789 -11.58453893\r\n[56]   5.99297057   4.24585847   8.04973684 -21.64221168   7.88708259\r\n[61]  13.04306842  -2.41357878 -15.97777778  -0.68700490 -34.43423288\r\n[66]   8.49802178   9.27265552  -9.47118722  -4.30217431  27.46946087\r\n[71] -18.40320776  -9.37331536  -0.44477178  23.96818019  20.20515712\r\n[76] -25.14986929   6.73333333  13.17359356  -2.01138295  -6.49134918\r\n[81]  -3.09569066   0.67753723  -1.16963497  14.62269998  -3.28231853\r\n[86]  15.10401849  -5.83653258  -0.09472978 -51.52790962  28.71524622\r\n[91]  16.85539361  10.62683715  34.34057937\r\n\r\n$P.square\r\n[1] 0.8270324\r\n\r\n\r\n\r\nModel2 <-broom::glance(HW4Q1a.lm)\r\nkable(Model2)\r\n\r\n\r\nr.squared\r\nadj.r.squared\r\nsigma\r\nstatistic\r\np.value\r\ndf\r\nlogLik\r\nAIC\r\nBIC\r\ndeviance\r\ndf.residual\r\nnobs\r\n0.8483699\r\n0.8450003\r\n17.39529\r\n251.7748\r\n0\r\n2\r\n-396.0631\r\n800.1262\r\n810.2566\r\n27233.66\r\n90\r\n93\r\n\r\n\r\n\r\nHW4Q1b.lm <-lm(P ~ S +Be+Ba+New, data = house.selling.price.2)\r\nsummary(HW4Q1b.lm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-36.212  -9.546   1.277   9.406  71.953 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\r\nS             64.761      5.630  11.504  < 2e-16 ***\r\nBe            -2.766      3.960  -0.698 0.486763    \r\nBa            19.203      5.650   3.399 0.001019 ** \r\nNew           18.984      3.873   4.902  4.3e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 16.36 on 88 degrees of freedom\r\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \r\nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\nModel3 <-broom::glance(HW4Q1b.lm)\r\nkable(Model3)\r\n\r\n\r\nr.squared\r\nadj.r.squared\r\nsigma\r\nstatistic\r\np.value\r\ndf\r\nlogLik\r\nAIC\r\nBIC\r\ndeviance\r\ndf.residual\r\nnobs\r\n0.868863\r\n0.8629022\r\n16.35994\r\n145.7634\r\n0\r\n4\r\n-389.3113\r\n790.6225\r\n805.8181\r\n23552.98\r\n88\r\n93\r\n\r\n\r\n\r\nqpcR::PRESS(HW4Q1b.lm)\r\n\r\n\r\n.........10.........20.........30.........40.........50\r\n.........60.........70.........80.........90...\r\n$stat\r\n[1] 28390.22\r\n\r\n$residuals\r\n [1]   8.80642475   1.35756431 -14.50175027 -29.04111653  84.23071502\r\n [6]  20.67569000 -35.35051485   8.11027181  13.67887526   0.07581592\r\n[11]   4.85328472 -11.83695825  26.87348374   8.26886073 -15.48538574\r\n[16]  19.31532622  -3.40940204  -3.04383223 -17.15036945   1.27508747\r\n[21]   1.74294861  22.35707134  -4.49743443   5.16137712  -6.99989072\r\n[26] -14.75005106   5.88131054 -12.57508690  11.10604584 -32.59560238\r\n[31] -18.02319107 -20.15370843   7.54904425   9.64952145   4.62088927\r\n[36]   1.63097572 -23.70869816 -23.70869816 -23.70869816 -11.35464729\r\n[41] -37.89758813  15.81945785  14.77909434  18.73454884  -9.37163361\r\n[46]  -0.70587588   1.32848794   0.65652512  -5.41405416 -12.03102247\r\n[51]  12.61959192  -8.21143765 -10.21858625   4.11858943  -8.37087703\r\n[56]   3.17956757   2.58154628   5.32400952 -20.64063327   5.32602736\r\n[61]  11.52097564   0.08288066 -13.16544457  -0.63681507 -29.48977860\r\n[66]  11.06805978   8.13089578  -6.44748468  -3.24599794  25.33235011\r\n[71] -16.03557507  -4.81181345   3.63613372  21.67212616  18.85145699\r\n[76] -17.99852963   9.98343549  13.63493612  -1.10933099  -1.73965309\r\n[81]   2.11051768   5.51373185   1.00558807  -3.14292693   2.17169862\r\n[86]  21.07738062   1.61706399  11.40050558 -40.52079806  11.63736061\r\n[91]  24.00171202  -0.18925139  34.97852503\r\n\r\n$P.square\r\n[1] 0.8419304\r\n\r\nE. Explain which model you prefer and why.\r\n\r\n\r\nModel_selection<- read_excel(\"HW4q1.xlsx\")\r\nkable(Model_selection)\r\n\r\n\r\nMODEL\r\nR2\r\nadjR2\r\nPRESS\r\nAIC\r\nBIC\r\nAIC/BIC\r\nModel1: Size+Bath+Bed+New\r\n0.8689\r\n0.8629\r\n28390\r\n790.6225\r\n805.8181\r\n0.9811426\r\nModel2: Size+Bath+New\r\n0.8681\r\n0.8637\r\n27860\r\n789.1366\r\n801.7996\r\n0.9842068\r\nModel3: Size+New\r\n0.8484\r\n0.8450\r\n31066\r\n800.1262\r\n810.2566\r\n0.9874973\r\n\r\nModel 2 with predictors for Size, Bath and New would be the better model with lowest PRESS, AIC and BIC\r\nQuestion 2\r\n(Data file: trees from base R) From the documentation: “This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.”\r\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\r\nfit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\r\n\r\n\r\ndata(\"trees\")  \r\ndim(trees)\r\n\r\n\r\n[1] 31  3\r\n\r\ncolnames(trees)\r\n\r\n\r\n[1] \"Girth\"  \"Height\" \"Volume\"\r\n\r\n\r\n\r\ntrees_lm <-lm(Volume ~ Girth+Height, data = trees)\r\nsummary(trees_lm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Volume ~ Girth + Height, data = trees)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\r\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\r\nHeight        0.3393     0.1302   2.607   0.0145 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.882 on 28 degrees of freedom\r\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \r\nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\nplot(trees[, 1:3], main = \"Correlation plot\")\r\n\r\n\r\n\r\n\r\n\r\n\r\npar(mfrow = c(1, 1))\r\nplot(trees_lm)\r\n\r\n\r\n\r\n\r\nVIOLATIONS OBSERVED:\r\nResiduals vs Fitted:\r\nAssumption: Constant Variance. Violation: Funnel shape\r\nQ-Q\r\nAssumption: Normality. Violation: No Violation\r\nScale-Location\r\nViolation: Heteroskedasticity\r\nResiduals vs Leverage\r\nViolation: Points outside red dashed lines\r\nQuestion 3\r\n(inspired by ALR 9.16)\r\n(Data file: florida in alr R package)\r\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\r\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\r\n\r\n\r\ndata(\"florida\")  \r\ndim(florida)\r\n\r\n\r\n[1] 67  3\r\n\r\ncolnames(florida)\r\n\r\n\r\n[1] \"Gore\"     \"Bush\"     \"Buchanan\"\r\n\r\n\r\n\r\nflorida_lm <-lm(Buchanan ~ Bush, data = florida)\r\nsummary(florida_lm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Buchanan ~ Bush, data = florida)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-907.50  -46.10  -29.19   12.26 2610.19 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \r\nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 353.9 on 65 degrees of freedom\r\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \r\nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\r\n\r\n\r\n\r\npar(mfrow = c(1, 1))\r\nplot(florida_lm)\r\n\r\n\r\n\r\n\r\nBased on the diagnostic plots, Palm Beach County is an outlier because it stands out and has a large residual from line that is predicted by regression model\r\nPART 2 (Final Project)\r\nBackground: The increasing costs of higher education and competitiveness of admission to top universities have made it difficult for young adults to pursue and obtain college degrees. This study is still evolving and research goal is to prove or disprove how completion of college degrees can affect future employment and earnings. Also looking into comparing data from 2000 and 2019. Data is from all 50 US states. Data is sourced and cleaned in excel file.\r\nData sources:\r\nhttps://www.ers.usda.gov/data-products/county-level-data-sets/download-data/\r\nhttps://www.ers.usda.gov/data-products/county-level-data-sets/\r\nWhat is your research question for the final project?\r\nFinal question not formulated yet but general concept is:\r\nDoes education pay?\r\nConsidering the obstacles of obtaining higher education, is pursuing college degrees still significant?\r\nWhat is your hypothesis (i.e. an answer to the research question) that you want to test?\r\nCompleting a higher degree education is significant to higher household income and employment status.\r\nPresent some exploratory analysis. In particular: Numerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables). Plot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\r\n\r\n\r\nPoster603 <- read_excel(\"Poster603.xlsx\")\r\ncolnames(Poster603)\r\n\r\n\r\n [1] \"FIPScode\"           \"state\"              \"areaname\"          \r\n [4] \"rt_ue_00\"           \"hs_00\"              \"assoc_col_00\"      \r\n [7] \"col_plus_00\"        \"pct_hs_00\"          \"pct_assoc_col_00\"  \r\n[10] \"pct_col_plus_00\"    \"med_hh_inc_00\"      \"rt_ave_ue_1519\"    \r\n[13] \"hs_1519\"            \"assoc_col_1519\"     \"col_plus_1519\"     \r\n[16] \"pct_hs_1519\"        \"pct_assoc_col_1519\" \"pct_col_plus_1519\" \r\n[19] \"med_hh_inc_1519\"   \r\n\r\ndim(Poster603)\r\n\r\n\r\n[1] 50 19\r\n\r\nkable(head(Poster603))\r\n\r\n\r\nFIPScode\r\nstate\r\nareaname\r\nrt_ue_00\r\nhs_00\r\nassoc_col_00\r\ncol_plus_00\r\npct_hs_00\r\npct_assoc_col_00\r\npct_col_plus_00\r\nmed_hh_inc_00\r\nrt_ave_ue_1519\r\nhs_1519\r\nassoc_col_1519\r\ncol_plus_1519\r\npct_hs_1519\r\npct_assoc_col_1519\r\npct_col_plus_1519\r\nmed_hh_inc_1519\r\n01000\r\nAL\r\nAlabama\r\n4.6\r\n877216\r\n746495\r\n549608\r\n30.4\r\n25.9\r\n19.0\r\n44667.05\r\n4.70\r\n1022839\r\n993344\r\n845772\r\n30.80027\r\n29.91210\r\n25.46833\r\n51771\r\n02000\r\nAK\r\nAlaska\r\n6.3\r\n105812\r\n135655\r\n93807\r\n27.9\r\n35.7\r\n24.7\r\n67482.77\r\n6.14\r\n134582\r\n169609\r\n142019\r\n28.00373\r\n35.29212\r\n29.55121\r\n77203\r\n05000\r\nAR\r\nArkansas\r\n4.2\r\n791904\r\n1078521\r\n766212\r\n24.3\r\n33.1\r\n16.7\r\n42111.47\r\n3.98\r\n1129129\r\n1600240\r\n1394526\r\n23.85888\r\n33.81361\r\n29.46681\r\n49020\r\n04000\r\nAZ\r\nArizona\r\n4.0\r\n590416\r\n424907\r\n288428\r\n34.1\r\n24.5\r\n23.5\r\n53071.81\r\n5.24\r\n684659\r\n593576\r\n463236\r\n34.03489\r\n29.50708\r\n23.02779\r\n62027\r\n06000\r\nCA\r\nCalifornia\r\n4.9\r\n4288452\r\n6397739\r\n5669966\r\n20.1\r\n30.0\r\n26.6\r\n62146.54\r\n5.02\r\n5423462\r\n7648680\r\n8980726\r\n20.48790\r\n28.89397\r\n33.92596\r\n80423\r\n08000\r\nCO\r\nColorado\r\n2.7\r\n644360\r\n861478\r\n907755\r\n23.2\r\n31.0\r\n32.7\r\n61767.06\r\n3.02\r\n817452\r\n1127242\r\n1565134\r\n21.36806\r\n29.46592\r\n40.91234\r\n77104\r\n\r\nVARIABLES OF INTEREST:\r\n      \"rt_ue_00\"   -rate of unemployment 2000\r\n      \r\n      \"hs_00\"      -# of adults with highschool diploma 2000\r\n      \r\n      \"assoc_col_00\"    -# of adults with some college or associates degree 2000\r\n      \r\n      \"col_plus_00\"    -# of adults with bachelors degree or higher 2000\r\n      \r\n      \"pct_hs_00\"        -% of adults with highschool diploma 2000\r\n      \r\n      \"pct_assoc_col_00\"  -% of adults with associates or some college 2000\r\n      \r\n      \"pct_col_plus_00\"  -% of adults with bachelors degree or higher 2000\r\n      \r\n      \"med_hh_inc_00\"     -median household income 2000\r\n      \r\n      \"rt_ave_ue_1519\"    -rate of average unemployment 2015-2019\r\n      \r\n      \"hs_1519\"      -# of adults with highschool diploma 2015-2019\r\n      \r\n      \"assoc_col_1519\"  -# of adults with some college or associates degree 2015-2019\r\n      \r\n      \"col_plus_1519\"     # of adults with bachelors degree or higher 2015-2019\r\n      \r\n      \"pct_hs_1519\"        -% of adults with highschool diploma 2015-2019\r\n      \r\n      \"pct_assoc_col_1519\" -% of adults with associates or some college 2015-2019\r\n      \r\n      \"pct_col_plus_1519\" -% of adults with bachelors degree or higher 2015-2019\r\n      \r\n      \"med_hh_inc_1519\"   -median household income 2015-2019\r\n      \r\nData from Year 2000\r\n\r\n\r\nNew_Poster00 <-Poster603 %>%\r\n  dplyr::select(\"rt_ue_00\",\"med_hh_inc_00\",\"pct_hs_00\",\"pct_assoc_col_00\",\"pct_col_plus_00\")\r\nkable(head(New_Poster00))\r\n\r\n\r\nrt_ue_00\r\nmed_hh_inc_00\r\npct_hs_00\r\npct_assoc_col_00\r\npct_col_plus_00\r\n4.6\r\n44667.05\r\n30.4\r\n25.9\r\n19.0\r\n6.3\r\n67482.77\r\n27.9\r\n35.7\r\n24.7\r\n4.2\r\n42111.47\r\n24.3\r\n33.1\r\n16.7\r\n4.0\r\n53071.81\r\n34.1\r\n24.5\r\n23.5\r\n4.9\r\n62146.54\r\n20.1\r\n30.0\r\n26.6\r\n2.7\r\n61767.06\r\n23.2\r\n31.0\r\n32.7\r\n\r\n\r\n\r\nsummary(New_Poster00)\r\n\r\n\r\n    rt_ue_00     med_hh_inc_00     pct_hs_00     pct_assoc_col_00\r\n Min.   :2.100   Min.   :38858   Min.   :20.10   Min.   :21.00   \r\n 1st Qu.:3.075   1st Qu.:48575   1st Qu.:27.80   1st Qu.:25.52   \r\n Median :3.800   Median :53310   Median :29.40   Median :27.60   \r\n Mean   :3.846   Mean   :54136   Mean   :29.95   Mean   :28.23   \r\n 3rd Qu.:4.400   3rd Qu.:60699   3rd Qu.:32.20   3rd Qu.:30.85   \r\n Max.   :6.300   Max.   :72161   Max.   :39.40   Max.   :37.00   \r\n pct_col_plus_00\r\n Min.   :14.80  \r\n 1st Qu.:21.27  \r\n Median :23.35  \r\n Mean   :23.78  \r\n 3rd Qu.:26.18  \r\n Max.   :33.20  \r\n\r\n\r\n\r\npairs(New_Poster00, main =\"Figure 1. Education data 2000\")\r\n\r\n\r\n\r\n\r\nData from Year 2015-2019\r\n\r\n\r\nNew_Poster1519 <-Poster603 %>%\r\n  dplyr::select(\"rt_ave_ue_1519\",\"med_hh_inc_1519\",\"pct_hs_1519\",\"pct_assoc_col_1519\",\"pct_col_plus_1519\")\r\nhead(New_Poster1519)\r\n\r\n\r\n# A tibble: 6 x 5\r\n  rt_ave_ue_1519 med_hh_inc_1519 pct_hs_1519 pct_assoc_col_1519\r\n           <dbl>           <dbl>       <dbl>              <dbl>\r\n1           4.7            51771        30.8               29.9\r\n2           6.14           77203        28.0               35.3\r\n3           3.98           49020        23.9               33.8\r\n4           5.24           62027        34.0               29.5\r\n5           5.02           80423        20.5               28.9\r\n6           3.02           77104        21.4               29.5\r\n# ... with 1 more variable: pct_col_plus_1519 <dbl>\r\n\r\n\r\n\r\nsummary(New_Poster1519)\r\n\r\n\r\n rt_ave_ue_1519  med_hh_inc_1519  pct_hs_1519    pct_assoc_col_1519\r\n Min.   :2.680   Min.   :45928   Min.   :20.49   Min.   :22.92     \r\n 1st Qu.:3.550   1st Qu.:57445   1st Qu.:25.92   1st Qu.:28.05     \r\n Median :4.230   Median :63279   Median :28.04   Median :29.82     \r\n Mean   :4.198   Mean   :65121   Mean   :28.23   Mean   :30.04     \r\n 3rd Qu.:4.730   3rd Qu.:73906   3rd Qu.:30.76   3rd Qu.:32.64     \r\n Max.   :6.140   Max.   :86644   Max.   :40.32   Max.   :36.73     \r\n pct_col_plus_1519\r\n Min.   :20.61    \r\n 1st Qu.:27.71    \r\n Median :30.69    \r\n Mean   :31.24    \r\n 3rd Qu.:34.15    \r\n Max.   :43.69    \r\n\r\n\r\n\r\npairs(New_Poster1519,  main = \"Figure 2. Education data 2015-2019\")\r\n\r\n\r\n\r\n\r\nRegression Analysis and Model Evaulations will be conducted for final paper. Based solely on Figure 1 and Figure 2, the initial observations are:\r\n\r\n\r\nInitial_observations <- read_excel(\"Initial_obs.xlsx\")\r\nkable(head(Initial_observations))\r\n\r\n\r\nInitial Cor observations:\r\n2000\r\n2015-19\r\nUnemployment & HS\r\nno cor\r\npositive cor\r\nUnemployment & some college/associates\r\nno cor\r\nno cor\r\nUnemployment & college or higher\r\nnegative cor\r\nnegative cor\r\nMedian household income & HS\r\nno cor\r\nnegative cor\r\nMedian household income & some college/assoc\r\nno cor\r\nno cor\r\nMedian household income & college or higher\r\npositive cor\r\npositive cor\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomowenvespa890475/distill-preview.png",
    "last_modified": "2022-04-15T17:22:28-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomegeeslindacss-603-hw4/",
    "title": "DACSS603_HW4",
    "description": "DACSS 603 Homework 4",
    "author": [
      {
        "name": "Eliza Geeslin",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\nQuestion 1\n(SMSS 14.3, 14.4, merged & modified)\n(Data file: house.selling.price.2 from smss R package)\nFor the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.\n\n\nx <- c(\"Price\", \"Size\", \"Beds\", \"Baths\", \"New\")\nPrice <- c(1, 0.899, 0.590, 0.714, 0.357)\nSize <- c(0.899, 1, 0.669, 0.662, 0.176)\nBeds <- c(0.590, 0.669, 1, 0.334, 0.267)\nBaths <- c(0.714, 0.662, 0.334, 1, 0.182)\nNew <- c(0.357, 0.176, 0.267, 0.182, 1)\n\ncorrelation_matrix <- data.frame(x, Price, Size, Beds, Baths, New)\n\ncorrelation_matrix\n\n\n      x Price  Size  Beds Baths   New\n1 Price 1.000 0.899 0.590 0.714 0.357\n2  Size 0.899 1.000 0.669 0.662 0.176\n3  Beds 0.590 0.669 1.000 0.334 0.267\n4 Baths 0.714 0.662 0.334 1.000 0.182\n5   New 0.357 0.176 0.267 0.182 1.000\n\no <- c(\"(Intercept)\", \"Size\", \"Beds\", \"Baths\", \"New\")\nEstimate <- c(-41.795, 64.761, 5.630, 11.504, 0)\nStd.Error <- c(12.104, 5.630, 3.960, 5.650, 3.873)\ntvalue <- c(-3.453, 11.504, -0.698, 3.399, 4.902)\nPr <- c(0.001, 0, 0.487, 0.001, 0.00000)\n\nmodel_fit <- data.frame(o, Estimate, Std.Error, tvalue, Pr)\n\nmodel_fit\n\n\n            o Estimate Std.Error tvalue    Pr\n1 (Intercept)  -41.795    12.104 -3.453 0.001\n2        Size   64.761     5.630 11.504 0.000\n3        Beds    5.630     3.960 -0.698 0.487\n4       Baths   11.504     5.650  3.399 0.001\n5         New    0.000     3.873  4.902 0.000\n\nWith these four predictors,\n(A) For backward elimination, which variable would be deleted first? Why?\nFor backwards elimination, you start with all variables and delete the variable with the largest p-value first. In this case, the first variable that would be deleted is Beds as it has the highest p-value at 0.487.\n(B) For forward selection, which variable would be added first? Why?\nFor forward selection, you add the variable that is the most significant first. So, you would add Size first as it has a p-value of 0 (I am assuming that New has a slightly larger p-value based on the decimal points).\n(C) Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\nThis might be because the Beds and the Size variable are very similar - i.e. if a house has more Bedrooms, it is bigger (and vice-versa, often the reason a house is bigger is because it has more bedrooms). So, the information captured in the Beds variable is already captured in Size.\n(D) Using software with these four predictors, find the model that would be selected using each criterion:\nR2\nAdjusted R2\nPRESS\nAIC\nBIC\n\n\ndata(\"house.selling.price.2\")\n\n# head(house.selling.price.2) looking at names of variables\n\nmodel4 <- lm(P ~ ., data = house.selling.price.2) #Size, New, Baths, Beds\n\nmodel3 <- lm(P ~ . - Be, data = house.selling.price.2) #Size, New, Baths\n\nmodel2 <- lm(P ~ . - Be - Ba, data = house.selling.price.2) # Size, New\n\nmodel1 <- lm(P ~ . - Be - Ba - New, data = house.selling.price.2) #Size\n\n\n\n\n\n# finding R^2, adjusted R^2, AIC, BIC using broom package, glace()\n\nbroom::glance(model1) %>%\n  select(r.squared, adj.r.squared, AIC, BIC)\n\n\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.808         0.806  820.  828.\n\nbroom::glance(model2) %>%\n  select(r.squared, adj.r.squared, AIC, BIC)\n\n\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.848         0.845  800.  810.\n\nbroom::glance(model3) %>%\n  select(r.squared, adj.r.squared, AIC, BIC)\n\n\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.868         0.864  789.  802.\n\nbroom::glance(model4) %>%\n  select(r.squared, adj.r.squared, AIC, BIC)\n\n\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.869         0.863  791.  806.\n\n\n\n# finding PRESS for each model\n\n# model1 \n\nr1 <- resid(model1) #residuals \n\npr1 <- resid(model1)/(1-lm.influence(model1)$hat) #predicted residuals\n\nPRESS1 <- sum(pr1^2) #PRESS\n\n\n# model2\npr2 <- resid(model2)/(1-lm.influence(model2)$hat) #predicted residuals\n\nPRESS2 <- sum(pr2^2) #PRESS\n\n# model3\npr3 <- resid(model3)/(1-lm.influence(model3)$hat) #predicted residuals\n\nPRESS3 <- sum(pr3^2) #PRESS\n\n# model4\npr4 <- resid(model4)/(1-lm.influence(model4)$hat) #predicted residuals\n\nPRESS4 <- sum(pr4^2) #PRESS\n\n\n#comparison_table\n\nModel <- c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\")\nVariables <- c(\"Size\", \"Size, New\", \"Size, New, Baths\", \"Size, New, Baths, Beds\")\nR.Squared <- c(0.807866, 0.8483699, 0.8681361, 0.868863 )\nAdj.R.Squared <- c(0.8057546, 0.8450003, 0.8636912, 0.8629022 )\nPRESS <- c(PRESS1, PRESS2, PRESS3, PRESS4)\nAIC <- c(820.1439, 800.1262, 789.1366, 790.6225)\nBIC <- c(827.7417, 810.2566, 801.7996, 805.8181)\n\nComparison_table <- data.frame(Model, Variables, R.Squared, Adj.R.Squared, PRESS, AIC, BIC)\n\nComparison_table\n\n\n    Model              Variables R.Squared Adj.R.Squared    PRESS\n1 Model 1                   Size 0.8078660     0.8057546 38203.29\n2 Model 2              Size, New 0.8483699     0.8450003 31066.00\n3 Model 3       Size, New, Baths 0.8681361     0.8636912 27860.05\n4 Model 4 Size, New, Baths, Beds 0.8688630     0.8629022 28390.22\n       AIC      BIC\n1 820.1439 827.7417\n2 800.1262 810.2566\n3 789.1366 801.7996\n4 790.6225 805.8181\n\nNow that we have all the values, we can select the model based on the listed criterion.\nIf we are trying to minimize R^2 we would choose Model 1. If we were using the criterion of trying to minimize adjusted R^2, we would select Model 2, which has just the variable Size and New.\nAccording to the criterion of minimizing the predicted residual sum of squares (PRESS), we would select Model 3, which has a PRESS = 27869.05. This was also the model selected by backward elimination and forward selection.\nAccording to the criterion of minimizing AIC, we would also select Model 3. Lastly, if we were looking to minimize BIC, we would also select Model 3.\n(E) Explain which model you prefer and why.\nI would select Model 3, which was the model selected by PRESS, AIC, and BIC. AIC and BIC can be particularly helpful when choosing a model because they both penalize the addition of new variables. Additionally they both want the lowest value possible to have a better-fit model. Also, they are simple functions in r, so it is easy to rerun the test with multiple models to see what adding a new variable would do to AIC and BIC.\nQuestion 2\n(Data file: trees from base R)\nFrom the documentation: “This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\n(A) fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\n\n\ndata(\"trees\")\n\nsummary(lm(Volume ~ Girth + Height, data = trees))\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n(B) Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\ntrees %>%\n  summarize(n = n())\n\n\n   n\n1 31\n\nfit = (lm(Volume ~ Girth + Height, data = trees))\n\npar(mfrow = c(2,2)) # change the panel layout to 2x2\nplot(fit, which = 1:6)\n\n\n\n\nYes, it does look like some of the regression assumptions are violated with this linear model. First, looking at the Residuals vs. Fitted plot, there is the assumption of Linearity, which is that the residuals should form roughly a straight line. Here we see that the residuals are bowed in shape. Next, There is the Constant variance assumption (that residuals should form roughly a horizonal band). This is also violated in the plot.\nIt alos looks like some assumptions are also violated for the Scale vs. Location plot. Again, with the constant variance assumption we assume that the residuals should form roughly a horizontal band with equally spread points. We also assume that the red line is approximately horizonal. These are both violated in the plot.\nFinally, the Cook’s Distance vs Leverage plot shows a highly influential data point of data item 31. 4/n is 4/31 = 0.129, which is less than 0.6 (the distance of the outlier), so this assumption is also violated.\nQuestion 3\n(inspired by ALR 9.16)\n(Data file: florida in alr R package)\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\ndata(\"florida\")\n\nflorida %>%\n  summarize(n = n())\n\n\n   n\n1 67\n\nsummary(lm(Buchanan ~ Bush, data = florida))\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\nfit = (lm(Buchanan ~ Bush, data = florida))\n\npar(mfrow = c(2,2)) # change the panel layout to 2x2\nplot(fit, which = 1:6)\n\n\n\n\nBased on the diagnostic plots, it looks like Palm Beach County is an outlier. We see that the Palm Beach residual is the only residuals that violates the regression assumptions for most of the plots. Additionally, in some cases the Dade county residual also appears to violate some of the assumptions. In the Residuals vs Fitted plot Palm Beach County’s residual violates the linearity assumption. In the Normal Q-Q plot Palm Beach County’s residual violates the normality assumption by not falling around the line.In the Scale-Location plot Palm Beach County’s residual violates the homoskedasticity by the spreading wider and further than the rest of the residuals (in this one Dade county may also violate this assumption). In the Residuals vs Leverage plot Palm Beach County’s residual violates the influential observation by having the residual being outside the red dashed line. This means that Palm Beach County can be influential against a regression line. Finally, in the Cook’s Distance plot, the Cook’s distance for the Palm Beach County observation is larger than 1 and also larger than 4/67 (.06). This indicates that Palm Beach County is an outlier. In this case we also see that Dade County is an outlier (and perhaps also Orange County).\nPART 2 (Final Project)\n1. What is your research question for the final project?\nMy research question is looking to answer the age-old question of what factors go into happiness? I started broadly with a bunch of factors that I thought could go into happiness.\n2. What is your hypothesis (i.e. an answer to the research question) that you want to test?\nMy hypothesis is that the factors that are most strongly correlated with happiness are marital status (with married men potentially being the happiest group), age (older people being happier than younger people), sex (men are happier than women), physical and mental health, and income (or, more importantly, income satisfaction).\n\n\n# reading in the data files\n\nhappiness_data <- read_excel(\"~/Documents/Eliza Geeslin/DACSS/DACSS semester 2 (603)/GSS_Final_Project.xlsx\")\n\n\nhappiness_data2 <- happiness_data %>%\n  filter(year == 2018) %>%\n  select(zodiac, wrkstat, marital, childs, age, educ, sex, race, earnrs, income, polviews, relig, happy, hapmar, life, satjob, satfin, relpersn, sexfreq, hlthphys, hlthmntl, satsoc, wtsscomp,sei10)\n  \nna_strings <- c(\".n: No answer\", \".d: Do not Know/Cannot Choose\", \".r: Refused\", \".i: Inapplicable\", \".x:  Not available in this release\")\n  \nhappiness_data2 <- happiness_data2 %>%\n  replace_with_na_all(condition = ~.x %in% na_strings)\n\n\nhappiness_data2\n\n\n# A tibble: 2,348 × 24\n   zodiac wrkstat marital childs age   educ  sex   race  earnrs income\n   <chr>  <chr>   <chr>   <chr>  <chr> <chr> <chr> <chr> <chr>  <chr> \n 1 Virgo  With a… Never … 0      43    14    MALE  White 1      .r:  …\n 2 Aquar… Retired Separa… 3      74    10    FEMA… White 1      $25,0…\n 3 Aries  Workin… Married 2      42    16    MALE  White 2      $25,0…\n 4 Aries  Workin… Married 2      63    16    FEMA… White 2      .r:  …\n 5 Cancer Retired Divorc… 0      71    18    MALE  Black 1      .r:  …\n 6 Scorp… Retired Widowed 2      67    16    FEMA… White 1      .d:  …\n 7 Leo    Workin… Divorc… 6      59    13    FEMA… Black 2      $15,0…\n 8 Pisces Workin… Never … 0      43    12    MALE  White 1      $25,0…\n 9 .n:  … Workin… Widowed 4      62    8     FEMA… White 1      $5,00…\n10 Scorp… Workin… Married 2      55    12    MALE  White 1      $25,0…\n# … with 2,338 more rows, and 14 more variables: polviews <chr>,\n#   relig <chr>, happy <chr>, hapmar <chr>, life <chr>, satjob <chr>,\n#   satfin <chr>, relpersn <chr>, sexfreq <chr>, hlthphys <chr>,\n#   hlthmntl <chr>, satsoc <chr>, wtsscomp <dbl>, sei10 <chr>\n\n3. Present some exploratory analysis. In particular: i) Numerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables).\n\n\n# summarize data\n# happiness_data2 <- happiness_data2 %>%\n#  replace_na(replace = list(x = c(\".i: Inapplicable\", \".r: Refused\", \".d: Do not Know/Cannot Choose\", \".n:  No answer\", \".x: # Not available in this release\")))\n\n# transform data into numbers\n\nhappiness_data2$age <- as.numeric(happiness_data2$age)\n\nhappiness_data2$educ <- as.numeric(happiness_data2$educ)\n\nhappiness_data2$earnrs <- as.numeric(happiness_data2$earnrs)\n\nhappiness_data2$sei10 <- as.numeric(happiness_data2$sei10)\n\n# re-code data into numbers\n\nhappiness_data3 <- happiness_data2 %>%\n  add_column(happy_num = if_else(.$happy == \"Not too happy\", 1, if_else(.$happy == \"Pretty happy\", 2, if_else(.$happy == \"Very happy\", 3,999))))  %>%\n  add_column(hapmar_num = if_else(.$hapmar == \"NOT TOO HAPPY\", 1, if_else(.$hapmar == \"PRETTY HAPPY\", 2, if_else(.$hapmar == \"VERY HAPPY\", 3, 999)))) %>%\n  add_column(life_num = if_else(.$life == \"Dull\", 1, if_else(.$life == \"Routine\", 2, if_else(.$life == \"Exciting\", 3, 999)))) %>%\n  add_column(satjob_num = if_else(.$satjob == \"Very dissatisfied\", 1, if_else(.$satjob == \"A little dissatisfied\", 2, if_else(.$satjob == \"Moderately satisfied\", 3, if_else(.$satjob == \"Very satisfied\", 4, 999))))) %>%\n  add_column(satfin_num = if_else(.$satfin == \"Not satisfied at all\", 1, if_else(.$satfin == \"More or less satisfied\", 2, if_else(.$satfin == \"Pretty well satisfied\", 3, 999)))) %>%\n  add_column(relpersn_num = if_else(.$relpersn == \"Not religious at all\", 1, if_else(.$relpersn == \"Slightly religious\", 2, if_else(.$relpersn == \"Moderately religious\", 3, if_else(.$relpersn == \"Very religious\", 4, 999))))) %>%\n  add_column(hlthphys_num = if_else(.$hlthphys == \"Poor\", 1, if_else(.$hlthphys == \"Fair\", 2, if_else(.$hlthphys == \"Good\", 3, if_else(.$hlthphys == \"Very good\", 4, if_else(.$hlthphys == \"Excellent\", 5, 999)))))) %>%\n  add_column(hlthmntl_num = if_else(.$hlthmntl == \"Poor\", 1, if_else(.$hlthmntl == \"Fair\", 2, if_else(.$hlthmntl == \"Good\", 3, if_else(.$hlthmntl == \"Very good\", 4, if_else(.$hlthmntl == \"Excellent\", 5, 999)))))) %>%\n  add_column(satsoc_num = if_else(.$satsoc == \"Poor\", 1, if_else(.$satsoc == \"Fair\", 2, if_else(.$satsoc == \"Good\", 3, if_else(.$satsoc == \"Very good\", 4, if_else(.$satsoc == \"Excellent\", 5, 999)))))) %>%\n  add_column(childs_num = if_else(.$childs == 0, 0, if_else(.$childs == 1, 1, if_else(.$childs == 2, 2, if_else(.$childs == 3, 3, if_else(.$childs == 4, 4, if_else(.$childs == 5, 5, if_else(.$childs == 6, 6, if_else(.$childs == 7, 7, if_else(.$childs == \"8 or more\", 8, 999)))))))))) %>%\n  replace_with_na_all(condition = ~.x == 999)\n\n\nsummary(happiness_data3)\n\n\n    zodiac            wrkstat            marital         \n Length:2348        Length:2348        Length:2348       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n    childs               age             educ      \n Length:2348        Min.   :18.00   Min.   : 1.00  \n Class :character   1st Qu.:34.00   1st Qu.:12.00  \n Mode  :character   Median :48.00   Median :14.00  \n                    Mean   :48.47   Mean   :13.76  \n                    3rd Qu.:63.00   3rd Qu.:16.00  \n                    Max.   :88.00   Max.   :20.00  \n                    NA's   :36      NA's   :7      \n     sex                race               earnrs     \n Length:2348        Length:2348        Min.   :0.000  \n Class :character   Class :character   1st Qu.:1.000  \n Mode  :character   Mode  :character   Median :1.000  \n                                       Mean   :1.408  \n                                       3rd Qu.:2.000  \n                                       Max.   :7.000  \n                                       NA's   :13     \n    income            polviews            relig          \n Length:2348        Length:2348        Length:2348       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n    happy              hapmar              life          \n Length:2348        Length:2348        Length:2348       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n    satjob             satfin            relpersn        \n Length:2348        Length:2348        Length:2348       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n   sexfreq            hlthphys           hlthmntl        \n Length:2348        Length:2348        Length:2348       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n    satsoc             wtsscomp          sei10        \n Length:2348        Min.   :0.4715   Min.   :-100.00  \n Class :character   1st Qu.:0.4715   1st Qu.:  24.20  \n Mode  :character   Median :0.9430   Median :  39.70  \n                    Mean   :1.0000   Mean   :  40.69  \n                    3rd Qu.:0.9430   3rd Qu.:  65.20  \n                    Max.   :5.8974   Max.   :  92.80  \n                                                      \n   happy_num       hapmar_num       life_num       satjob_num   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:3.000  \n Median :2.000   Median :3.000   Median :2.000   Median :3.000  \n Mean   :2.156   Mean   :2.613   Mean   :2.446   Mean   :3.311  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:4.000  \n Max.   :3.000   Max.   :3.000   Max.   :3.000   Max.   :4.000  \n NA's   :4       NA's   :1356    NA's   :793     NA's   :609    \n   satfin_num     relpersn_num    hlthphys_num    hlthmntl_num  \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:3.000   1st Qu.:3.000  \n Median :2.000   Median :3.000   Median :3.000   Median :4.000  \n Mean   :2.078   Mean   :2.476   Mean   :3.335   Mean   :3.663  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :3.000   Max.   :4.000   Max.   :5.000   Max.   :5.000  \n NA's   :10      NA's   :20      NA's   :19      NA's   :19     \n   satsoc_num      childs_num   \n Min.   :1.000   Min.   :0.000  \n 1st Qu.:3.000   1st Qu.:0.000  \n Median :4.000   Median :2.000  \n Mean   :3.452   Mean   :1.855  \n 3rd Qu.:4.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :8.000  \n NA's   :21      NA's   :4      \n\nii) Plot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\n\n\n# did not include hapmar_num bc of so many NAs\n\nhappiness_data_num_1 <- happiness_data3 %>%\n  select(happy_num, age, educ, childs_num, hlthphys_num, hlthmntl_num, relpersn_num)\n\n# happiness_data_num_2 <- happiness_data3 %>%\n#  select(happy_num, satfin_num, satjob_num, life_num, relpersn_num)\n\n# happiness_data_num_3 <- happiness_data3 %>%\n#  select(happy_num, hlthphys_num, hlthmntl_num, satsoc_num, childs_num)\n\npairs(happiness_data_num_1)\n\n\n\n#pairs(happiness_data_num_2)\n\n#pairs(happiness_data_num_3)\n\n\n\nThis is just one summary visual. I think I need a better way to visualize this data since a lot of it is categorical. I am not sure if I am going to be able to find meaningful insights from this data because of how broadly happiness is calculated (just 3 levels). As a next step I want to see how other analysis is done using GSS happiness data to see if there is something I can get inspiration from.\n\n\n\n",
    "preview": "posts/httpsrpubscomegeeslindacss-603-hw4/distill-preview.png",
    "last_modified": "2022-04-15T17:22:33-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomalexanderhong86890491/",
    "title": "DACSS 603 HW#4",
    "description": "Fourth homework for DACSS 603.",
    "author": [
      {
        "name": "Alexander Hong",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\nQuestion 1\n(SMSS 14.3, 14.4, merged & modified)\n(Data file: house.selling.price.2 from smss R package)\nFor the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price. \n\nPrice\nSize\nBeds\nBaths\nNew\nPrice\n1\n0.899\n0.590\n0.714\n0.357\nSize\n0.899\n1\n0.669\n0.662\n0.176\nBeds\n0.590\n0.669\n1\n0.334\n0.267\nBaths\n0.714\n0.662\n0.334\n1\n0.182\nNew\n0.357\n0.176\n0.267\n0.182\n1\n\nEstimate\nStd. Error\nt value\nPr(> | t| )\n(Intercept)\n-41.795\n12.104\n-3.453\n0.001\nSize\n64.761\n5.630\n11.504\n0\nBeds\n-2.766\n3.960\n-0.698\n0.487\nBaths\n19.203\n5.650\n3.399\n0.001\nNew\n18.984\n3.873\n4.902\n0.00000\n\nWith these four predictors,\nFor backward elimination, which variable would be deleted first? Why?\nOf all the four predictors listed, Beds would be deleted first because it has the highest p-value of\nFor forward selection, which variable would be added first? Why?\nSize would be the first variable to be added, because it is the most significant over New (o vs 0.0000x)\nWhy do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\n\n\n    library(smss)\n    data(\"house.selling.price.2\")\n    hw1 <- house.selling.price.2\n\n    hw1_lm_full <- lm(P ~ S + Be + Ba + New, data=hw1)\n\n    model_terms <- c('New', 'Ba', 'Be', 'S', 'Be + Ba + New', 'S + Be + New', 'S + Ba + New', 'S + Be + Ba', 'Ba + New', 'Be + New', 'Be + Ba', 'S + New', 'S + Be', 'S + Ba', 'S + Be + Ba + New')\n\n    hw1_model_stats <- data.frame(model = character(),\n                                r2 = numeric(),\n                                adj_r2 = numeric(),\n                                PRESS = numeric(),\n                                AIC = numeric(),\n                                BIC = numeric(),\n                                stringsAsFactors = FALSE)\n\n    # Derived from https://www.statology.org/press-statistic/\n    PRESS <- function(model) {\n        i <- residuals(model)/(1 - lm.influence(model)$hat)\n        sum(i^2)\n    }\n\n    attach(hw1)\n\n    for(i in 1:length(model_terms)){\n      lm.i <- lm(paste(\"P ~ \", model_terms[i], sep = \"\"))\n      sul = summary(lm.i)\n      rowsss <- c(model_terms[i],\n                  signif(sul$r.squared, 4),\n                  signif(sul$adj.r.squared, 4), \n                  signif(PRESS(lm.i), 4),\n                  signif(AIC(lm.i), 4),\n                  signif(BIC(lm.i))\n      )\n      hw1_model_stats <- rbind(hw1_model_stats, rowsss)\n    }\n\n    colnames(hw1_model_stats)<-c(\"Model\", \"R2\", \"Adj R2\", \"PRESS\", \"AIC\", \"BIC\")\n\n    kbl <- knitr::kable(hw1_model_stats)\n\n\n\nUsing software with these four predictors, find the model that would be selected using each criterion:\nR2 - In terms of R2, using all the predictors wins out by a slim margin (.8681)\nAdjusted R2 - In terms of Adjusted R2, using Size, Baths, and New wins out.\nPRESS - In terms of using PRESS, again, using Size, Baths, and New wins out.\nAIC - In terms of using AIC, again, using Size, Baths, and New wins out.\nBIC - In terms of using BIC, again, again,using Size, Baths, and New wins out.\n\nModel\nR2\nAdj R2\nPRESS\nAIC\nBIC\nNew\n0.1271\n0.1175\n164000\n960.9\n968.506\nBa\n0.5094\n0.504\n95730\n907.3\n914.93\nBe\n0.3484\n0.3413\n123000\n933.7\n941.315\nS\n0.8079\n0.8058\n38200\n820.1\n827.742\nBe + Ba + New\n0.6717\n0.6606\n67940\n874\n886.642\nS + Be + New\n0.8516\n0.8466\n30840\n800.1\n812.756\nS + Ba + New\n0.8681\n0.8637\n27860\n789.1\n801.8\nS + Be + Ba\n0.8331\n0.8274\n35100\n811.1\n823.739\nBa + New\n0.5625\n0.5528\n87680\n898.7\n908.807\nBe + New\n0.391\n0.3775\n117500\n929.4\n939.563\nBe + Ba\n0.6488\n0.641\n70800\n878.2\n888.36\nS + New\n0.8484\n0.845\n31070\n800.1\n810.257\nS + Be\n0.8081\n0.8038\n38870\n822\n832.165\nS + Ba\n0.8328\n0.8291\n34170\n809.2\n819.355\nS + Be + Ba + New\n0.8689\n0.8629\n28390\n790.6\n805.818\nExplain which model you prefer and why.\nBased off the five criterion listed in the previous question, the model that uses (size, baths, new) as predictors won out on four of the five criteria, thus I would use this model.\nQuestion 2\n(Data file: trees from base R)\nFrom the documentation:\n“This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular, \nfit a multiple regression model with  the Volume as the outcome and Girth  and Height as the explanatory variables\nRun regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\nhw2 <- trees\nhw2_lm <- lm(Volume ~ Height + Girth, data = hw2)\npar(mfrow = c(2,3)); plot(hw2_lm, which = 1:6)\n\n\n\n\nThe following regression assumptions are being violated:\nHomoskedasticity - According the the residuals vs fitted plot, the residuals seemed to be grouped at various spots within the plot. This seems to suggest some heteroskedasticity going on.\nIndependence of Errors - Judging by the Cook’s Distance and Residuals vs Leverage plots, observation 31 seems to be unusually influential. Given its distance from observations, as well as the observation falling outside of the bound in the Residuals vs Leverage plot, it be said that due to this observation, this assumption is being violated.\nQuestion 3\n(inspired by ALR 9.16)\n(Data file: florida in alr R package) \nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\n\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\nlibrary(alr4)\ndata(florida)\nhw3 <- florida\n\nhw3_lm <- lm(Buchanan ~ Bush, data = hw3)\npar(mfrow = c(2,3)); plot(hw3_lm, which = 1:6)\n\n\n\n\nJudging by the plots, it is reasonable to assume that Palm Beach is an outlier. - In the residuals vs fitted plot, while the vast majority of points are clumped in the middle, both Palm Beach and Dade County are way outside of all the other points. - In the Residuals vs Leverage plot, Palm Beach seems to exude significant leverage, judging by its position on the plot being far away from other points. - The Cook’s distance plot displays that Palm Beach’s distance is above 2, which again signifies its distance from other data points.\nPART 2 (Final Project)\nThe dataset that I intend to analyze pertains to predicting whether or not a start-up will be a success or not. The dataset can be found at this website: https://www.kaggle.com/datasets/manishkc06/startup-success-prediction\n\n\nlibrary(readr)\nlibrary(tidyverse)\noptions(scipen=1, digits=3)\n\nstartup_data_2 <- read_csv(\"/Users/nmb48ayatin_alexanderh/Documents/DACSS Classes/DACSS603/startup data 2.csv\")\n\nff_sum <- summary(startup_data_2$age_first_funding_year)\nfr_sum <- summary(startup_data_2$funding_rounds)\n\nfunding_1styr_stats <- startup_data_2 %>%\n  group_by(status) %>%\n  summarise(avg_age_1stfundingyr = mean(age_first_funding_year, na.rm=TRUE),\n            sd_age_1stfundingyr = sd(age_first_funding_year, na.rm=TRUE),\n            iqr_age_1stfundingyr = IQR(age_first_funding_year, na.rm=TRUE)\n            ) \n\nfunding_rounds_stats <- startup_data_2 %>%\n  group_by(status) %>%\n  summarise(avg_fundingrounds = mean(funding_rounds, na.rm=TRUE),\n            sd_fundingrounds = sd(funding_rounds, na.rm=TRUE),\n            iqr_fundingrounds = IQR(funding_rounds, na.rm=TRUE)\n            )\n\nboxplot <- startup_data_2 %>%\n  group_by(status) %>%\n  dplyr::select(status, age_first_funding_year, funding_rounds) %>% \n  tidyr::gather(\"Variable\", \"Value\", 2:3) %>% \n  ggplot(., aes(x = Variable, y = Value))+geom_boxplot()\n\n\n\nWhat is your research question for the final project?\nHow does funding, particularly when or how often a start-up is funded, determine its success?\nWhat is your hypothesis (i.e. an answer to the research question) that you want to test?\nStart-ups that received their first round of funding 5 years or more since beginning will not be successful.\nPresent some exploratory analysis. In particular:\nNumerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables).\nOutcome Variable\nStatus\nFrequency\nacquired\n597\nclosed\n326\nExplantory Variables\nstatus\navg_age_1stfundingyr\nsd_age_1stfundingyr\niqr_age_1stfundingyr\nacquired\n2.10\n1.94\n2.81\nclosed\n2.49\n3.30\n3.38\nstatus\navg_fundingrounds\nsd_fundingrounds\niqr_fundingrounds\nacquired\n2.52\n1.4\n2\nclosed\n1.92\n1.3\n1\nPlot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\n\n\n\nRegardless of Status:\nWe can see that some start ups received funding before being founded, as noted by the values and points below. Some other startups took as many as 21 years to receive funding, On average, most start ups get their first round of funding around the 2nd year, and receive around 2 rounds of funding.\n\n\n\n",
    "preview": "posts/httpsrpubscomalexanderhong86890491/distill-preview.png",
    "last_modified": "2022-04-15T17:22:39-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomkbechw3603/",
    "title": "DACSS 603 Homework 3",
    "description": "Multiple Regression",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": {}
      }
    ],
    "date": "2022-04-11",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is “ŷ = −10,536 + 53.8x1 + 2.84x2”.\r\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\n\r\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\n\r\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\n\r\nResponse 1\r\nA: Using the prediction equation, the predicted selling price of this home would be $107,296. Since the home sold for $145,000, the residual is $37,704. I interpret this result to mean that the home seller was able to get a much better price for their home than the market prediction.\r\n\r\n\r\nShow code\r\n\r\nx1 <- 1240 #square feet of the home\r\nx2 <- 18000 #square feet of the lot\r\ny <- 145000 #selling price of the home\r\n\r\n# using the prediction equation here:\r\n\r\nybar1 <- (-10536)+(53.8*x1)+(2.84*x2)\r\n\r\nresidual1 <- y-ybar1\r\n\r\nybar1\r\n\r\n\r\n[1] 107296\r\n\r\nShow code\r\n\r\nresidual1\r\n\r\n\r\n[1] 37704\r\n\r\nB: The explanatory variable “square feet of the home” has a slope coefficient of 53.8. This means that for every increase in that variable, the predicted price of the home will increase by $53.80. I can confirm that this is the case by changing my calculations accordingly:\r\n\r\n\r\nShow code\r\n\r\nx1b <- 1241 #square feet of the home\r\nx2b <- 18000 #square feet of the lot\r\nyb <- 145000 #selling price of the home\r\n\r\nybar1b <- (-10536)+(53.8*x1b)+(2.84*x2b)\r\n\r\nresidual1b <- yb-ybar1b\r\n\r\nybar1b-ybar1 #difference in predicted home price given increase in quare feet of the home by \"1\"\r\n\r\n\r\n[1] 53.8\r\n\r\nC: The explanatory variable “square feet of the lot” has a slope coefficient of 2.84. This means that for every increase in that variable, the predicted price of the home will increase by $2.84. I can then calculate that it would take an increase of ~18.94 in lot size to have a proportionate increase in price to an increase of one square foot in home size.\r\n\r\n\r\nShow code\r\n\r\nneed <- 53.8/2.84\r\n\r\nneed\r\n\r\n\r\n[1] 18.94366\r\n\r\nQuestion 2\r\nThe data file (alr4 R Package) concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\nResponse 2\r\nFirst I’m loading the “salary” data and inspecting it.\r\n\r\n\r\nShow code\r\n\r\ndata(\"salary\") \r\ndim(salary)\r\n\r\n\r\n[1] 52  6\r\n\r\nShow code\r\n\r\nhead(salary)\r\n\r\n\r\n   degree rank    sex year ysdeg salary\r\n1 Masters Prof   Male   25    35  36350\r\n2 Masters Prof   Male   13    22  35350\r\n3 Masters Prof   Male   10    23  28200\r\n4 Masters Prof Female    7    27  26775\r\n5     PhD Prof   Male   19    30  33696\r\n6 Masters Prof   Male   16    21  28516\r\n\r\nPart A.\r\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\nClarifying the null and alternative hypotheses:\r\nH0: The mean salary for men and women is the same, without regard to any other variable but sex.\r\nHa: The mean salary for men and women is NOT the same, without regard to any other variable but sex.\r\nWe have n > 30, so I can assume a normal distribution.\r\nThe t-test result indicates that the mean salary for men and women is not equal ($24,696.79 for men, $21,357.14 for women). However, the p-value is 0.0706. Since this p-value indicates a significance level of % (p > 0.05), I fail to reject the null hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# Conduct t.test to determine confidence level at default of 0.95\r\nt.test(salary~sex, data = salary, var.equal = TRUE)\r\n\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.8474, df = 50, p-value = 0.0706\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -291.257 6970.550\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\nPart B.\r\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\nI ran the multiple linear regression using the “lm()” function using all predictors and salary as the outcome variable. Then, the function “confint()” produced the corresponding confidence intervals from the model. This produced a confidence interval, at the default of 0.95, that the difference in salary between males and females is [(-$697.82) to ($3,030.56)]\r\n\r\n\r\nShow code\r\n\r\n#Linear regression on all predictors\r\n\r\nmlm2 <- lm(salary~., data = salary)\r\n\r\nsummary(mlm2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nShow code\r\n\r\nconfint(mlm2, \"sexFemale\")\r\n\r\n\r\n              2.5 %   97.5 %\r\nsexFemale -697.8183 3030.565\r\n\r\nPart C.\r\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variable\r\nI can look at these relationships looking at both the output from the “lm()” function in Part B and the “confint()” function. However, I also found a great solution to represent these relationships using the “broom” package. Using the function “tidy()” from the “broom” package, I can create a tibble from the results of my “lm()” call.\r\n\r\n\r\nShow code\r\n\r\n#Manipulate the output tibble and convert scientific notation of p-value to decimals\r\n\r\ntidymlm2 <- tidy(mlm2, conf.int = TRUE)\r\n\r\noptions(scipen = 999)\r\n\r\ntidymlm2\r\n\r\n\r\n# A tibble: 7 x 7\r\n  term        estimate std.error statistic  p.value conf.low conf.high\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\r\n1 (Intercept)   15746.     800.      19.7  9.76e-24   14134.   17358. \r\n2 degreePhD      1389.    1019.       1.36 1.80e- 1    -663.    3440. \r\n3 rankAssoc      5292.    1145.       4.62 3.22e- 5    2985.    7599. \r\n4 rankProf      11119.    1352.       8.23 1.62e-10    8396.   13841. \r\n5 sexFemale      1166.     926.       1.26 2.14e- 1    -698.    3031. \r\n6 year            476.      94.9      5.02 8.65e- 6     285.     667. \r\n7 ysdeg          -125.      77.5     -1.61 1.15e- 1    -281.      31.5\r\n\r\nReviewing this information I can see:\r\nFor the predictor variable “degreePhD”, the statistical significance of its’ relationship to salary is an increase in salary of ~$1,388.61 given a PhD. The p-value of 0.18 indicates that this is not statistically significant.\r\nFor the predictor variable “rankAssoc”, the statistical significance of its’ relationship to salary is an increase in salary of ~$5,292.36 given achieving the rank of Associate. The p-value of 0.0000322 indicates that this result is statistically significant to the 0.99 confidence level.\r\nFor the predictor variable “rankProf”, the statistical significance of its’ relationship to salary is an increase in salary of ~$11,118.76.36 given achieving the rank of Professor. The p-value of 0.000000000162 indicates that this result is statistically significant to the 0.99 confidence level.\r\nFor the predictor variable “sexFemale”, the statistical significance of its’ relationship to salary is an increase in salary of ~$1,166.37 given the salary being for a female. The p-value of 0.214 indicates that this result is not statistically significant.\r\nFor the predictor variable “year”, the statistical significance of its’ relationship to salary is an increase in salary of ~$476.31 given the salary for each year of experience in current rank. The p-value of 0.00000865 indicates that this result is statistically significant.\r\nFor the predictor variable “ysdeg”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$124.57 given the salary for each year since highest degree achieved. The p-value of 0.12 indicates that this result is not statistically significant.\r\nSummarizing, the predictor variables, “degreePhD”, “sexFemale”, and “ysdeg” are not statistically significant, while predictor variables “rankAssoc”, “rankProf”, and “year” are statistically significant to the 99% confidence level. In addition, all of the predictor variables have a positive linear relationship except for the variable “ysdeg” to salary, which has a negative linear relationship.\r\nPart D.\r\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\r\n\r\n\r\nShow code\r\n\r\n# change baseline rank\r\nnew2d <- relevel(salary$rank, \"Prof\")\r\n\r\n# fit model again\r\nmlm2d <- lm(salary ~ degree + sex + year + ysdeg + new2d, data = salary)\r\n\r\n# get summary\r\nsummary(mlm2d)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + sex + year + ysdeg + new2d, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept)  26864.81    1375.29  19.534 < 0.0000000000000002 ***\r\ndegreePhD     1388.61    1018.75   1.363                0.180    \r\nsexFemale     1166.37     925.57   1.260                0.214    \r\nyear           476.31      94.91   5.018       0.000008653790 ***\r\nysdeg         -124.57      77.49  -1.608                0.115    \r\nnew2dAsst   -11118.76    1351.77  -8.225       0.000000000162 ***\r\nnew2dAssoc   -5826.40    1012.93  -5.752       0.000000727809 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 0.00000000000000022\r\n\r\nChanging the baseline for “rank” and looking at the coefficients of variables, the “Asst” rank has an estimate of a lower salary of ~$11,118.76, and the “Assoc” rank has an estimate of a lower salary of ~$5,826.40. Both are statistically significant to the 99% confidence level. This is the same information from the fit test in part C. We have just changed the base reference from “Asst” to “Prof”.\r\nPart E.\r\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression without the rank\r\n\r\nmlm2e <- lm(salary ~ sex + degree + year + ysdeg, data = salary)\r\nsummary(mlm2e)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ sex + degree + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969 < 0.0000000000000002 ***\r\nsexFemale   -1286.54    1313.09  -0.980             0.332209    \r\ndegreePhD   -3299.35    1302.52  -2.533             0.014704 *  \r\nyear          351.97     142.48   2.470             0.017185 *  \r\nysdeg         339.40      80.62   4.210             0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 0.000000001048\r\n\r\nShow code\r\n\r\n#Manipulate the output tibble and convert scientific notation of p-value to decimals\r\n\r\ntidymlm2e <- tidy(mlm2e, conf.int = TRUE)\r\noptions(scipen = 999)\r\ntidymlm2e\r\n\r\n\r\n# A tibble: 5 x 7\r\n  term        estimate std.error statistic  p.value conf.low conf.high\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\r\n1 (Intercept)   17184.    1148.     15.0   1.66e-19  14874.     19493.\r\n2 sexFemale     -1287.    1313.     -0.980 3.32e- 1  -3928.      1355.\r\n3 degreePhD     -3299.    1303.     -2.53  1.47e- 2  -5920.      -679.\r\n4 year            352.     142.      2.47  1.72e- 2     65.3      639.\r\n5 ysdeg           339.      80.6     4.21  1.14e- 4    177.       502.\r\n\r\nFor the predictor variable “sexFemale”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$1,286.54 given the salary being for a female. The p-value of 0.332 indicates that this result is not statistically significant.\r\nFor the predictor variable “degreePhD”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$3,299.35 given the salary being for a female. The p-value of 0.0147 indicates that this result is statistically significant to the .95 confidence level.\r\nFor the predictor variable “year”, the statistical significance of its’ relationship to salary is an increase in salary of ~$351.97 given the salary for each year of experience in current rank. The p-value of 0.0147 indicates that this result is statistically significant to the .95 confidence level.\r\nFor the predictor variable “ysdeg”, the statistical significance of its’ relationship to salary is an increase in salary of ~$339.40 given the salary for each year since highest degree achieved. The p-value of 0.000114 indicates that this result is statistically significant to the .99 confidence level.\r\nSummarizing, eliminating the “rank” variable, the predictor variables, degreePhD”, “year”, and “ysdeg” are statistically significant to at least the 95% confidence level, while predictor variable “sexFemale” is not statistically significant. The predictor variables “year” and “ysdeg” have a positive linear relationship and the predictor variables “sexFemale” and “degreePhD” have a negative linear relationship.\r\nPractically, this tells me that eliminating “rank” before comparing salaries between males and females shows a different linear relationship than when “rank” was involved (negative vs. positive). However, it also tells me that the relationship remains statistically not significant to a reasonable level of confidence.\r\nPart F.\r\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\r\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\nI am creating a new variable for the dummy variable indicating whether it was “Dean 1” or “Dean 2” doing the hiring. “Dean 1” represents the “old” Dean and “Dean 2” represents the “new” Dean appointed 15 years ago. I will run the model with as few predictor variables as is practical to reduce the concern of multicollinearity, or the phenomemon of predictor variables being correlated with one another and contributing to unreliable inferences.\r\nClarifying the null and alternative hypotheses:\r\nH0: The mean salary for hires of Dean 2 are higher than the mean salary for hires of Dean 1\r\nHa: The mean salary for hires of Dean 2 are equal to or less than than the mean salary for hires of Dean 1\r\nWe have n > 30, so I can assume a normal distribution.\r\n\r\n\r\nShow code\r\n\r\n# create new variable\r\ndf2f <- salary %>%\r\n  mutate(dean = case_when(\r\n    ysdeg >= 1 & ysdeg <= 15 ~ \"2\",\r\n    ysdeg >= 16 ~ \"1\"\r\n  ))\r\n\r\nmlm2f <- lm(salary ~ ., data = df2f)\r\n\r\nsummary(mlm2f)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = df2f)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3621.2 -1336.8  -271.6   530.1  9247.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value       Pr(>|t|)    \r\n(Intercept) 13767.69    1744.31   7.893 0.000000000575 ***\r\ndegreePhD    1135.00    1031.16   1.101          0.277    \r\nrankAssoc    5234.01    1138.47   4.597 0.000035985932 ***\r\nrankProf    11411.45    1362.02   8.378 0.000000000116 ***\r\nsexFemale    1084.09     921.49   1.176          0.246    \r\nyear          460.35      95.09   4.841 0.000016263785 ***\r\nysdeg         -47.86      97.71  -0.490          0.627    \r\ndean2        1749.09    1372.83   1.274          0.209    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2382 on 44 degrees of freedom\r\nMultiple R-squared:  0.8602,    Adjusted R-squared:  0.838 \r\nF-statistic: 38.68 on 7 and 44 DF,  p-value: < 0.00000000000000022\r\n\r\nBased on this summary, I can see that the hires of Dean 2 are expected to make a salary of ~$1,749.09 than the hires of Dean 1. This result has a p-value of 0.209. Since this p-value indicates a significance level of % (p > 0.05), I fail to reject the null hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# Removing the variable \"ysdeg\"\r\n\r\nmlm2g <- lm(salary ~ . - ysdeg, data = df2f)\r\n\r\nsummary(mlm2g)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ . - ysdeg, data = df2f)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3403.3 -1387.0  -167.0   528.2  9233.8 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value         Pr(>|t|)    \r\n(Intercept) 13328.38    1483.38   8.985 0.00000000001330 ***\r\ndegreePhD     818.93     797.48   1.027           0.3100    \r\nrankAssoc    4972.66     997.17   4.987 0.00000961362451 ***\r\nrankProf    11096.95    1191.00   9.317 0.00000000000454 ***\r\nsexFemale     907.14     840.54   1.079           0.2862    \r\nyear          434.85      78.89   5.512 0.00000164625970 ***\r\ndean2        2163.46    1072.04   2.018           0.0496 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2362 on 45 degrees of freedom\r\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \r\nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 0.00000000000000022\r\n\r\nRunning the model with less variables does change the adjusted R-squared and goodness of fit; though some more than others, after running this model with many different variable combinations. The best fit is seemingly the one including the new “Dean” variable but without the “ysdeg” variable.\r\nQuestion 3\r\nUsing the data file in the SMSS R package “house.selling.price”:\r\nAnswer 3\r\nI’m loading the “house.selling.price” data and inspecting it.\r\n\r\n\r\nShow code\r\n\r\ndata(\"house.selling.price\") \r\nhsp <- house.selling.price\r\ndim(hsp)\r\n\r\n\r\n[1] 100   7\r\n\r\nShow code\r\n\r\nhead(hsp)\r\n\r\n\r\n  case Taxes Beds Baths New  Price Size\r\n1    1  3104    4     2   0 279900 2048\r\n2    2  1173    2     1   0 146500  912\r\n3    3  3076    4     2   0 237700 1654\r\n4    4  1608    3     2   0 200000 2068\r\n5    5  1454    3     3   0 159900 1477\r\n6    6  2997    3     2   1 499900 3153\r\n\r\nPart A.\r\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\n\r\n\r\nShow code\r\n\r\nmlm3a <- lm(Price ~ Size + New, data = house.selling.price)\r\n\r\nsummary(mlm3a)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738              0.00737 ** \r\nSize           116.132      8.795  13.204 < 0.0000000000000002 ***\r\nNew          57736.283  18653.041   3.095              0.00257 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 0.00000000000000022\r\n\r\nPart B.\r\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\nThe coefficient for homes that are “new” indicates that the “new” variable results in a price increase of ~$57,736.28.\r\nThe coefficient for home “size” indicates that for each square foot of home size, the price of a home increases by ~$116.13.\r\nThe p-values of 0.00257 for “new” and indicates a significance level of (p < 0.01), indicating the results are statistically significant to the 99% confidence level.\r\nThe equation to indicate price for new homes:\r\n(-40230.87) + (116.13)(x) + (57,736.18)\r\nThe equation to indicate price for not new homes:\r\n(-40230.87) + (116.13)(x)\r\nPart C.\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\n-40230.87+(116.13*3000)+57736.18\r\n\r\n\r\n[1] 365895.3\r\n\r\nShow code\r\n\r\n-40230.87+(116.13*3000)\r\n\r\n\r\n[1] 308159.1\r\n\r\nFor a 3000 square foot home that is new, the price can be estimated to be $365,895.30.\r\nFor a 3000 square foot home that is not new, the price can be estimated to be $308,159.10.\r\nPart D.\r\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\r\n\r\n\r\nShow code\r\n\r\nmlm3d <- lm(Price ~ Size + New + Size*New, house.selling.price)\r\n\r\nsummary(mlm3d)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432              0.15536    \r\nSize           104.438      9.424  11.082 < 0.0000000000000002 ***\r\nNew         -78527.502  51007.642  -1.540              0.12697    \r\nSize:New        61.916     21.686   2.855              0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 0.00000000000000022\r\n\r\nFitting the model where the “size” variable interacts with the “new” variable, the result is statistically significant with a p-value of 0.00527.\r\nPart E.\r\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\nqplot(x = Price, y = Size, facets = ~ New, data = mlm3d) +\r\n  geom_smooth(method = \"lm\", se=TRUE,fullrange=TRUE,color=\"goldenrod\") + \r\n     labs(title= \"Price and Size of Homes Given Not New and New\",\r\n        x= \"Price\",\r\n        y = \"Size in Square Feet\") +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nIn the course of investigation options for graphics on visualizing multiple regression models, I also found an interesting package that uses “ggPredict()” to make a really visually pleasing representation, if not as practical:\r\n\r\n\r\nShow code\r\n\r\nggPredict(mlm3d,se=TRUE,interactive=TRUE)\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<svg xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' id='svg_0ded14db-9fc6-414e-b22d-22f5b4d943ab' viewBox='0 0 432 360'>\\n <defs>\\n  <clipPath id='svg_0ded14db-9fc6-414e-b22d-22f5b4d943ab_c1'>\\n   <rect x='0' y='0' width='432' height='360'/>\\n  <\\/clipPath>\\n  <clipPath id='svg_0ded14db-9fc6-414e-b22d-22f5b4d943ab_c2'>\\n   <rect x='52.72' y='5.48' width='311.99' height='323.02'/>\\n  <\\/clipPath>\\n <\\/defs>\\n <g>\\n  <g clip-path='url(#svg_0ded14db-9fc6-414e-b22d-22f5b4d943ab_c1)'>\\n   <rect x='0' y='0' width='432' height='360' fill='#FFFFFF' stroke='#FFFFFF' stroke-width='0.75' stroke-linejoin='round' stroke-linecap='round'/>\\n   <rect x='0' y='0' width='432' height='360' fill='#FFFFFF' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='round'/>\\n  <\\/g>\\n  <g clip-path='url(#svg_0ded14db-9fc6-414e-b22d-22f5b4d943ab_c2)'>\\n   <rect x='52.72' y='5.48' width='311.99' height='323.02' fill='#EBEBEB' stroke='none'/>\\n   <polyline points='52.72,323.92 364.71,323.92' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,241.52 364.71,241.52' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,159.11 364.71,159.11' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,76.71 364.71,76.71' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='74.37,328.50 74.37,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='149.00,328.50 149.00,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='223.64,328.50 223.64,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='298.28,328.50 298.28,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,282.72 364.71,282.72' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,200.31 364.71,200.31' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,117.91 364.71,117.91' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,35.50 364.71,35.50' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='111.69,328.50 111.69,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='186.32,328.50 186.32,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='260.96,328.50 260.96,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='335.60,328.50 335.60,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polygon points='66.90,269.69 81.83,261.76 96.76,253.79 111.69,245.76 126.61,237.61 141.54,229.30 156.47,220.74 171.40,211.94 186.32,202.92 201.25,193.78 216.18,184.56 231.11,175.29 246.03,165.99 260.96,156.68 275.89,147.34 290.82,138.00 305.74,128.65 320.67,119.29 335.60,109.93 350.53,100.57 350.53,121.72 335.60,129.57 320.67,137.42 305.74,145.28 290.82,153.14 275.89,161.01 260.96,168.89 246.03,176.79 231.11,184.70 216.18,192.64 201.25,200.63 186.32,208.71 171.40,216.91 156.47,225.31 141.54,233.97 126.61,242.87 111.69,251.94 96.76,261.11 81.83,270.36 66.90,279.64' fill='#132B43' fill-opacity='0.2' stroke='none'/>\\n   <polyline points='66.90,269.69 81.83,261.76 96.76,253.79 111.69,245.76 126.61,237.61 141.54,229.30 156.47,220.74 171.40,211.94 186.32,202.92 201.25,193.78 216.18,184.56 231.11,175.29 246.03,165.99 260.96,156.68 275.89,147.34 290.82,138.00 305.74,128.65 320.67,119.29 335.60,109.93 350.53,100.57' fill='none' stroke='none'/>\\n   <polyline points='350.53,121.72 335.60,129.57 320.67,137.42 305.74,145.28 290.82,153.14 275.89,161.01 260.96,168.89 246.03,176.79 231.11,184.70 216.18,192.64 201.25,200.63 186.32,208.71 171.40,216.91 156.47,225.31 141.54,233.97 126.61,242.87 111.69,251.94 96.76,261.11 81.83,270.36 66.90,279.64' fill='none' stroke='none'/>\\n   <polygon points='66.90,279.81 81.83,267.58 96.76,255.32 111.69,243.02 126.61,230.67 141.54,218.24 156.47,205.70 171.40,193.00 186.32,180.09 201.25,166.86 216.18,153.26 231.11,139.27 246.03,124.93 260.96,110.32 275.89,95.52 290.82,80.58 305.74,65.56 320.67,50.47 335.60,35.33 350.53,20.16 350.53,52.55 335.60,64.80 320.67,77.08 305.74,89.40 290.82,101.80 275.89,114.28 260.96,126.89 246.03,139.70 231.11,152.78 216.18,166.20 201.25,180.02 186.32,194.21 171.40,208.71 156.47,223.43 141.54,238.31 126.61,253.30 111.69,268.36 96.76,283.48 81.83,298.64 66.90,313.82' fill='#56B1F7' fill-opacity='0.2' stroke='none'/>\\n   <polyline points='66.90,279.81 81.83,267.58 96.76,255.32 111.69,243.02 126.61,230.67 141.54,218.24 156.47,205.70 171.40,193.00 186.32,180.09 201.25,166.86 216.18,153.26 231.11,139.27 246.03,124.93 260.96,110.32 275.89,95.52 290.82,80.58 305.74,65.56 320.67,50.47 335.60,35.33 350.53,20.16' fill='none' stroke='none'/>\\n   <polyline points='350.53,52.55 335.60,64.80 320.67,77.08 305.74,89.40 290.82,101.80 275.89,114.28 260.96,126.89 246.03,139.70 231.11,152.78 216.18,166.20 201.25,180.02 186.32,194.21 171.40,208.71 156.47,223.43 141.54,238.31 126.61,253.30 111.69,268.36 96.76,283.48 81.83,298.64 66.90,313.82' fill='none' stroke='none'/>\\n   <circle cx='189.91' cy='167.39' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='1' title='1&amp;lt;br/&amp;gt;Size=2048&amp;lt;br/&amp;gt;Price=279900'/>\\n   <circle cx='105.12' cy='222.36' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='2' title='2&amp;lt;br/&amp;gt;Size=912&amp;lt;br/&amp;gt;Price=146500'/>\\n   <circle cx='160.5' cy='184.78' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='3' title='3&amp;lt;br/&amp;gt;Size=1654&amp;lt;br/&amp;gt;Price=237700'/>\\n   <circle cx='191.4' cy='200.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='4' title='4&amp;lt;br/&amp;gt;Size=2068&amp;lt;br/&amp;gt;Price=200000'/>\\n   <circle cx='147.29' cy='216.84' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='5' title='5&amp;lt;br/&amp;gt;Size=1477&amp;lt;br/&amp;gt;Price=159900'/>\\n   <circle cx='272.38' cy='76.75' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='6' title='6&amp;lt;br/&amp;gt;Size=3153&amp;lt;br/&amp;gt;Price=499900'/>\\n   <circle cx='138.18' cy='173.33' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='7' title='7&amp;lt;br/&amp;gt;Size=1355&amp;lt;br/&amp;gt;Price=265500'/>\\n   <circle cx='191.92' cy='163.27' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='8' title='8&amp;lt;br/&amp;gt;Size=2075&amp;lt;br/&amp;gt;Price=289900'/>\\n   <circle cx='334.85' cy='40.86' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='9' title='9&amp;lt;br/&amp;gt;Size=3990&amp;lt;br/&amp;gt;Price=587000'/>\\n   <circle cx='123.63' cy='253.88' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='10' title='10&amp;lt;br/&amp;gt;Size=1160&amp;lt;br/&amp;gt;Price=70000'/>\\n   <circle cx='128.11' cy='256.14' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='11' title='11&amp;lt;br/&amp;gt;Size=1220&amp;lt;br/&amp;gt;Price=64500'/>\\n   <circle cx='163.19' cy='213.91' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='12' title='12&amp;lt;br/&amp;gt;Size=1690&amp;lt;br/&amp;gt;Price=167000'/>\\n   <circle cx='140.05' cy='235.5' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='13' title='13&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=114600'/>\\n   <circle cx='155.72' cy='240.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='14' title='14&amp;lt;br/&amp;gt;Size=1590&amp;lt;br/&amp;gt;Price=103000'/>\\n   <circle cx='115.42' cy='241.1' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='15' title='15&amp;lt;br/&amp;gt;Size=1050&amp;lt;br/&amp;gt;Price=101000'/>\\n   <circle cx='94.52' cy='253.88' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='16' title='16&amp;lt;br/&amp;gt;Size=770&amp;lt;br/&amp;gt;Price=70000'/>\\n   <circle cx='142.29' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='17' title='17&amp;lt;br/&amp;gt;Size=1410&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='116.16' cy='273.45' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='18' title='18&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=22500'/>\\n   <circle cx='134.08' cy='245.64' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='19' title='19&amp;lt;br/&amp;gt;Size=1300&amp;lt;br/&amp;gt;Price=90000'/>\\n   <circle cx='149' cy='227.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='20' title='20&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=133000'/>\\n   <circle cx='98.25' cy='245.43' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='21' title='21&amp;lt;br/&amp;gt;Size=820&amp;lt;br/&amp;gt;Price=90500'/>\\n   <circle cx='331.79' cy='44.77' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='22' title='22&amp;lt;br/&amp;gt;Size=3949&amp;lt;br/&amp;gt;Price=577500'/>\\n   <circle cx='124.37' cy='224.01' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='23' title='23&amp;lt;br/&amp;gt;Size=1170&amp;lt;br/&amp;gt;Price=142500'/>\\n   <circle cx='149' cy='216.8' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='24' title='24&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=160000'/>\\n   <circle cx='245.29' cy='183.83' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='25' title='25&amp;lt;br/&amp;gt;Size=2790&amp;lt;br/&amp;gt;Price=240000'/>\\n   <circle cx='113.92' cy='246.87' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='26' title='26&amp;lt;br/&amp;gt;Size=1030&amp;lt;br/&amp;gt;Price=87000'/>\\n   <circle cx='130.35' cy='233.85' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='27' title='27&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=118600'/>\\n   <circle cx='168.41' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='28' title='28&amp;lt;br/&amp;gt;Size=1760&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='152.74' cy='221.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='29' title='29&amp;lt;br/&amp;gt;Size=1550&amp;lt;br/&amp;gt;Price=148000'/>\\n   <circle cx='120.64' cy='254.29' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='30' title='30&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=69000'/>\\n   <circle cx='186.32' cy='210.2' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='31' title='31&amp;lt;br/&amp;gt;Size=2000&amp;lt;br/&amp;gt;Price=176000'/>\\n   <circle cx='137.81' cy='247.08' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='32' title='32&amp;lt;br/&amp;gt;Size=1350&amp;lt;br/&amp;gt;Price=86500'/>\\n   <circle cx='174.38' cy='208.55' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='33' title='33&amp;lt;br/&amp;gt;Size=1840&amp;lt;br/&amp;gt;Price=180000'/>\\n   <circle cx='224.39' cy='208.97' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='34' title='34&amp;lt;br/&amp;gt;Size=2510&amp;lt;br/&amp;gt;Price=179000'/>\\n   <circle cx='269.17' cy='143.45' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='35' title='35&amp;lt;br/&amp;gt;Size=3110&amp;lt;br/&amp;gt;Price=338000'/>\\n   <circle cx='168.41' cy='229.16' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='36' title='36&amp;lt;br/&amp;gt;Size=1760&amp;lt;br/&amp;gt;Price=130000'/>\\n   <circle cx='164.68' cy='215.56' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='37' title='37&amp;lt;br/&amp;gt;Size=1710&amp;lt;br/&amp;gt;Price=163000'/>\\n   <circle cx='119.9' cy='231.22' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='38' title='38&amp;lt;br/&amp;gt;Size=1110&amp;lt;br/&amp;gt;Price=125000'/>\\n   <circle cx='138.56' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='39' title='39&amp;lt;br/&amp;gt;Size=1360&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='130.35' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='40' title='40&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='130.35' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='41' title='41&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='147.51' cy='222.36' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='42' title='42&amp;lt;br/&amp;gt;Size=1480&amp;lt;br/&amp;gt;Price=146500'/>\\n   <circle cx='150.5' cy='223.02' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='43' title='43&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=144900'/>\\n   <circle cx='187.82' cy='207.32' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='44' title='44&amp;lt;br/&amp;gt;Size=2020&amp;lt;br/&amp;gt;Price=183000'/>\\n   <circle cx='112.43' cy='253.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='45' title='45&amp;lt;br/&amp;gt;Size=1010&amp;lt;br/&amp;gt;Price=69900'/>\\n   <circle cx='159.45' cy='258' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='46' title='46&amp;lt;br/&amp;gt;Size=1640&amp;lt;br/&amp;gt;Price=60000'/>\\n   <circle cx='107.21' cy='230.39' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='47' title='47&amp;lt;br/&amp;gt;Size=940&amp;lt;br/&amp;gt;Price=127000'/>\\n   <circle cx='154.98' cy='247.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='48' title='48&amp;lt;br/&amp;gt;Size=1580&amp;lt;br/&amp;gt;Price=86000'/>\\n   <circle cx='101.24' cy='262.12' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='49' title='49&amp;lt;br/&amp;gt;Size=860&amp;lt;br/&amp;gt;Price=50000'/>\\n   <circle cx='143.03' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='50' title='50&amp;lt;br/&amp;gt;Size=1420&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='131.84' cy='232.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='51' title='51&amp;lt;br/&amp;gt;Size=1270&amp;lt;br/&amp;gt;Price=121300'/>\\n   <circle cx='110.19' cy='249.35' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='52' title='52&amp;lt;br/&amp;gt;Size=980&amp;lt;br/&amp;gt;Price=81000'/>\\n   <circle cx='208.71' cy='205.26' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='53' title='53&amp;lt;br/&amp;gt;Size=2300&amp;lt;br/&amp;gt;Price=188000'/>\\n   <circle cx='143.78' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='54' title='54&amp;lt;br/&amp;gt;Size=1430&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='140.05' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='55' title='55&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='129.6' cy='222.98' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='56' title='56&amp;lt;br/&amp;gt;Size=1240&amp;lt;br/&amp;gt;Price=145000'/>\\n   <circle cx='120.64' cy='254.29' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='57' title='57&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=69000'/>\\n   <circle cx='120.64' cy='237.68' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='58' title='58&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=109300'/>\\n   <circle cx='178.86' cy='228.54' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='59' title='59&amp;lt;br/&amp;gt;Size=1900&amp;lt;br/&amp;gt;Price=131500'/>\\n   <circle cx='218.42' cy='200.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='60' title='60&amp;lt;br/&amp;gt;Size=2430&amp;lt;br/&amp;gt;Price=200000'/>\\n   <circle cx='117.66' cy='248.97' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='61' title='61&amp;lt;br/&amp;gt;Size=1080&amp;lt;br/&amp;gt;Price=81900'/>\\n   <circle cx='137.81' cy='245.14' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='62' title='62&amp;lt;br/&amp;gt;Size=1350&amp;lt;br/&amp;gt;Price=91200'/>\\n   <circle cx='165.42' cy='231.42' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='63' title='63&amp;lt;br/&amp;gt;Size=1720&amp;lt;br/&amp;gt;Price=124500'/>\\n   <circle cx='339.33' cy='190.01' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='64' title='64&amp;lt;br/&amp;gt;Size=4050&amp;lt;br/&amp;gt;Price=225000'/>\\n   <circle cx='149' cy='226.48' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='65' title='65&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=136500'/>\\n   <circle cx='229.69' cy='125.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='66' title='66&amp;lt;br/&amp;gt;Size=2581&amp;lt;br/&amp;gt;Price=381000'/>\\n   <circle cx='195.28' cy='179.71' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='67' title='67&amp;lt;br/&amp;gt;Size=2120&amp;lt;br/&amp;gt;Price=250000'/>\\n   <circle cx='241.93' cy='136.49' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='68' title='68&amp;lt;br/&amp;gt;Size=2745&amp;lt;br/&amp;gt;Price=354900'/>\\n   <circle cx='150.5' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='69' title='69&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='132.58' cy='245.68' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='70' title='70&amp;lt;br/&amp;gt;Size=1280&amp;lt;br/&amp;gt;Price=89900'/>\\n   <circle cx='157.96' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='71' title='71&amp;lt;br/&amp;gt;Size=1620&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='150.5' cy='240.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='72' title='72&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=103000'/>\\n   <circle cx='188.56' cy='207.32' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='73' title='73&amp;lt;br/&amp;gt;Size=2030&amp;lt;br/&amp;gt;Price=183000'/>\\n   <circle cx='140.79' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='74' title='74&amp;lt;br/&amp;gt;Size=1390&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='177.37' cy='216.8' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='75' title='75&amp;lt;br/&amp;gt;Size=1880&amp;lt;br/&amp;gt;Price=160000'/>\\n   <circle cx='252.83' cy='103.9' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='76' title='76&amp;lt;br/&amp;gt;Size=2891&amp;lt;br/&amp;gt;Price=434000'/>\\n   <circle cx='137.06' cy='229.16' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='77' title='77&amp;lt;br/&amp;gt;Size=1340&amp;lt;br/&amp;gt;Price=130000'/>\\n   <circle cx='107.21' cy='232.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='78' title='78&amp;lt;br/&amp;gt;Size=940&amp;lt;br/&amp;gt;Price=123000'/>\\n   <circle cx='80.34' cy='274.07' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='79' title='79&amp;lt;br/&amp;gt;Size=580&amp;lt;br/&amp;gt;Price=21000'/>\\n   <circle cx='142.29' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='80' title='80&amp;lt;br/&amp;gt;Size=1410&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='122.88' cy='253.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='81' title='81&amp;lt;br/&amp;gt;Size=1150&amp;lt;br/&amp;gt;Price=69900'/>\\n   <circle cx='140.05' cy='231.22' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='82' title='82&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=125000'/>\\n   <circle cx='146.77' cy='215.72' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='83' title='83&amp;lt;br/&amp;gt;Size=1470&amp;lt;br/&amp;gt;Price=162600'/>\\n   <circle cx='155.72' cy='218.07' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='84' title='84&amp;lt;br/&amp;gt;Size=1590&amp;lt;br/&amp;gt;Price=156900'/>\\n   <circle cx='126.61' cy='239.09' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='85' title='85&amp;lt;br/&amp;gt;Size=1200&amp;lt;br/&amp;gt;Price=105900'/>\\n   <circle cx='180.35' cy='213.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='86' title='86&amp;lt;br/&amp;gt;Size=1920&amp;lt;br/&amp;gt;Price=167500'/>\\n   <circle cx='197.52' cy='220.17' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='87' title='87&amp;lt;br/&amp;gt;Size=2150&amp;lt;br/&amp;gt;Price=151800'/>\\n   <circle cx='201.25' cy='233.98' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='88' title='88&amp;lt;br/&amp;gt;Size=2200&amp;lt;br/&amp;gt;Price=118300'/>\\n   <circle cx='101.24' cy='243.87' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='89' title='89&amp;lt;br/&amp;gt;Size=860&amp;lt;br/&amp;gt;Price=94300'/>\\n   <circle cx='128.85' cy='244.03' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='90' title='90&amp;lt;br/&amp;gt;Size=1230&amp;lt;br/&amp;gt;Price=93900'/>\\n   <circle cx='122.13' cy='214.73' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='91' title='91&amp;lt;br/&amp;gt;Size=1140&amp;lt;br/&amp;gt;Price=165000'/>\\n   <circle cx='234.84' cy='165.29' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='92' title='92&amp;lt;br/&amp;gt;Size=2650&amp;lt;br/&amp;gt;Price=285000'/>\\n   <circle cx='116.16' cy='264.18' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='93' title='93&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=45000'/>\\n   <circle cx='169.16' cy='231.26' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='94' title='94&amp;lt;br/&amp;gt;Size=1770&amp;lt;br/&amp;gt;Price=124900'/>\\n   <circle cx='175.87' cy='222.15' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='95' title='95&amp;lt;br/&amp;gt;Size=1860&amp;lt;br/&amp;gt;Price=147000'/>\\n   <circle cx='116.16' cy='210.2' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='96' title='96&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=176000'/>\\n   <circle cx='166.17' cy='201.76' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='97' title='97&amp;lt;br/&amp;gt;Size=1730&amp;lt;br/&amp;gt;Price=196500'/>\\n   <circle cx='139.3' cy='228.25' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='98' title='98&amp;lt;br/&amp;gt;Size=1370&amp;lt;br/&amp;gt;Price=132200'/>\\n   <circle cx='153.48' cy='246.3' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='99' title='99&amp;lt;br/&amp;gt;Size=1560&amp;lt;br/&amp;gt;Price=88400'/>\\n   <circle cx='137.06' cy='230.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='100' title='100&amp;lt;br/&amp;gt;Size=1340&amp;lt;br/&amp;gt;Price=127200'/>\\n   <polyline points='66.90,274.67 81.83,266.06 96.76,257.45 111.69,248.85 126.61,240.24 141.54,231.63 156.47,223.03 171.40,214.42 186.32,205.82 201.25,197.21 216.18,188.60 231.11,180.00 246.03,171.39 260.96,162.78 275.89,154.18 290.82,145.57 305.74,136.97 320.67,128.36 335.60,119.75 350.53,111.15' fill='none' stroke='#132B43' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt' title='for New=0&amp;lt;br/&amp;gt;y = 104.44*x -22227.81' data-id='1'/>\\n   <polyline points='66.90,296.82 81.83,283.11 96.76,269.40 111.69,255.69 126.61,241.98 141.54,228.27 156.47,214.57 171.40,200.86 186.32,187.15 201.25,173.44 216.18,159.73 231.11,146.02 246.03,132.31 260.96,118.61 275.89,104.90 290.82,91.19 305.74,77.48 320.67,63.77 335.60,50.06 350.53,36.36' fill='none' stroke='#56B1F7' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt' title='for New=1&amp;lt;br/&amp;gt;y = 166.35*x -100755.31' data-id='21'/>\\n  <\\/g>\\n  <g clip-path='url(#svg_0ded14db-9fc6-414e-b22d-22f5b4d943ab_c1)'>\\n   <text x='42.89' y='285.87' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>0<\\/text>\\n   <text x='18.41' y='203.46' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>200000<\\/text>\\n   <text x='18.41' y='121.06' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>400000<\\/text>\\n   <text x='18.41' y='38.65' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>600000<\\/text>\\n   <polyline points='49.98,282.72 52.72,282.72' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,200.31 52.72,200.31' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,117.91 52.72,117.91' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,35.50 52.72,35.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='111.69,331.24 111.69,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='186.32,331.24 186.32,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='260.96,331.24 260.96,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='335.60,331.24 335.60,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <text x='101.89' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>1000<\\/text>\\n   <text x='176.53' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>2000<\\/text>\\n   <text x='251.17' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>3000<\\/text>\\n   <text x='325.81' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>4000<\\/text>\\n   <text x='198.02' y='352.2' font-size='8.25pt' font-family='Arial'>Size<\\/text>\\n   <text transform='translate(13.36,179.52) rotate(-90.00)' font-size='8.25pt' font-family='Arial'>Price<\\/text>\\n   <rect x='375.67' y='110.47' width='50.85' height='113.03' fill='#FFFFFF' stroke='none'/>\\n   <image x='381.15' y='131.63' width='17.28' height='86.4' preserveAspectRatio='none' xlink:href='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAEsCAYAAAACUNnVAAAAmElEQVQ4ja2UQRLDMAgDd3hk/tnX5agcknFaCoa0vnkMCEnGsL12GQhDYDpPwoDrpBFlRDWi/9Wu6AHC9FkLYLrzlhD3oJ3mDvRHRYy7IC8A7bCK8gpWKYDzeQVArDcDyD2YAnSofZvzaOA6Q7Oahp/dmRuVTQ0xU5TGP2o8Waool1obVuv1q8qP9xPvg633XlGLDtfIhNUBcBeA5ss0BXMAAAAASUVORK5CYII=' xmlns:xlink='http://www.w3.org/1999/xlink'/>\\n   <text x='403.91' y='221.03' font-size='6.6pt' font-family='Arial'>0.00<\\/text>\\n   <text x='403.91' y='199.51' font-size='6.6pt' font-family='Arial'>0.25<\\/text>\\n   <text x='403.91' y='177.98' font-size='6.6pt' font-family='Arial'>0.50<\\/text>\\n   <text x='403.91' y='156.45' font-size='6.6pt' font-family='Arial'>0.75<\\/text>\\n   <text x='403.91' y='134.92' font-size='6.6pt' font-family='Arial'>1.00<\\/text>\\n   <text x='381.15' y='124.99' font-size='8.25pt' font-family='Arial'>New<\\/text>\\n   <line x1='381.15' y1='217.88' x2='384.6' y2='217.88' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='196.36' x2='384.6' y2='196.36' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='174.83' x2='384.6' y2='174.83' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='153.3' x2='384.6' y2='153.3' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='131.77' x2='384.6' y2='131.77' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='217.88' x2='398.43' y2='217.88' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='196.36' x2='398.43' y2='196.36' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='174.83' x2='398.43' y2='174.83' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='153.3' x2='398.43' y2='153.3' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='131.77' x2='398.43' y2='131.77' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n  <\\/g>\\n <\\/g>\\n<\\/svg>\",\"js\":null,\"uid\":\"svg_0ded14db-9fc6-414e-b22d-22f5b4d943ab\",\"ratio\":1.2,\"settings\":{\"tooltip\":{\"css\":\".tooltip_SVGID_ { background-color:white;font-style:italic;padding:10px;border-radius:10px 20px 10px 20px; ; position:absolute;pointer-events:none;z-index:999;}\",\"placement\":\"doc\",\"offx\":10,\"offy\":0,\"use_cursor_pos\":true,\"opacity\":0.75,\"usefill\":false,\"usestroke\":false,\"delay\":{\"over\":200,\"out\":500}},\"hover\":{\"css\":\".hover_SVGID_ { r:4px;cursor:pointer;stroke:red;stroke-width:2px; }\",\"reactive\":false},\"hoverkey\":{\"css\":\".hover_key_SVGID_ { stroke:red; }\",\"reactive\":false},\"hovertheme\":{\"css\":\".hover_theme_SVGID_ { fill:green; }\",\"reactive\":false},\"hoverinv\":{\"css\":\"\"},\"zoom\":{\"min\":1,\"max\":10},\"capture\":{\"css\":\".selected_SVGID_ { fill:#FF3333;stroke:black; }\",\"type\":\"multiple\",\"only_shiny\":true,\"selected\":[]},\"capturekey\":{\"css\":\".selected_key_SVGID_ { stroke:gray; }\",\"type\":\"single\",\"only_shiny\":true,\"selected\":[]},\"capturetheme\":{\"css\":\".selected_theme_SVGID_ { stroke:gray; }\",\"type\":\"single\",\"only_shiny\":true,\"selected\":[]},\"toolbar\":{\"position\":\"topright\",\"saveaspng\":true,\"pngname\":\"diagram\"},\"sizing\":{\"rescale\":true,\"width\":1}}},\"evals\":[],\"jsHooks\":[]}\r\nPart F.\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*3000)+(-78527.50*0)+(61.92*3000*0) \r\n\r\n\r\n[1] 291092.2\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*3000)+(-78527.50*1)+(61.92*3000*1) \r\n\r\n\r\n[1] 398324.7\r\n\r\nUnder the new model:\r\nFor a 3000 square foot home that is new, the price can be estimated to be $398,324.70.\r\nFor a 3000 square foot home that is not new, the price can be estimated to be $291,092.20.\r\nPart G.\r\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\nFor a 1500 square foot home that is new, the price can be estimated to be $148,784.70.\r\nFor a 1500 square foot home that is not new, the price can be estimated to be $134,432.20.\r\n\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*1500)+(-78527.50*0)+(61.92*1500*0) \r\n\r\n\r\n[1] 134432.2\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*1500)+(-78527.50*1)+(61.92*1500*1) \r\n\r\n\r\n[1] 148784.7\r\n\r\nIllustrating the difference and analyzing the output\r\n\r\n\r\nShow code\r\n\r\n#3,000 Square Foot House\r\n\r\nnew1 <- 398324.70\r\nold1 <- 291092.20\r\nold1/new1*100\r\n\r\n\r\n[1] 73.07912\r\n\r\nShow code\r\n\r\n#1,500 Square Foot House\r\n\r\nnew2 <- 148784.70\r\nold2 <- 134432.2\r\nold2/new2*100\r\n\r\n\r\n[1] 90.35351\r\n\r\nThe difference in value between the 3,000 square foot house and the 1,500 square foot house under this model represents a value ration of price of old to new of 73.08% vs. 90.35%. This tells me that in this model, the price of an “old” house is a smaller percentage of the price of a “new” house as the square footage increases.\r\nPart H.\r\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\r\nThe second model has a higher adjusted R squared (0.7363 v. 0.7169) than the first model, which was still an independently valid model. This gives me a hint at the statistical reliability of the model. I also think it takes into consideration the context a stronger threshold where square footage begins to diminish as a variable.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomkbechw3603/distill-preview.png",
    "last_modified": "2022-04-11T16:50:07-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomdacssjoe885035/",
    "title": "Homework 3",
    "description": "Homework 3",
    "author": [
      {
        "name": "Joe Davis",
        "url": {}
      }
    ],
    "date": "2022-04-11",
    "categories": [],
    "contents": "\nFormatting Note\nBeing that this homework has many more sub questions than previous ones, I’m going to move the R chunks to right after the sub questions versus doing the massive wall of code outputs. I assume this will be a welcome change compared to previous efforts.\nQuestion 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\n-A) A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\n##Question A math work\n  #observed price of home sold\nhouse_price_actual <- 145000\n\n  #predicted price of home sold\nhouse_price_prediction <-  -10536 + (53.8*1240) + (2.84*18000)\n\n  #get residual\n  house_price_residual <- house_price_prediction - house_price_actual\n\n\n\nFor the predicted sale price of the house using 1240 sq ft for x1 and 18000 square ft for x2, I found 107296 and the residual when comparing to the observed sale price of $145,000 is -37704. This means that our model was under-predicted the observed value of home sales by a pretty substantial amount. I suspect this is due to the model failing to account for other variables that explain other factors in the final sale price of a home. Distance to downtown, school districts, or other variables likely play a part in describing the distance between our predicted and actual values.\n-B) For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nFor each square foot the predicted home price increase is $53.80. This is because the slope for that variable is in change for each increment of the variable on the outcome variable controlling for the other variables in the equation for this particular type of multiple regression. Since lot sized is fixed here, we wind up with the proof of that concept as the intercept and x2 terms would remain constant while only the 53.8x term would change.\n-C) According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nIf we divide the x1 slope by the x2 slope, we will get the square footage the lot size would have to increase by in order to equal the incremental change from a square foot increase of the home size. Calculated as 18.94 square feet rounding to the same digits as that slope term.\nQuestion 2\nThe data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\n##load the data for the following questions\nsalary_dat <- as_tibble(salary)\n\n\n\n-A) Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\n## Ho: men_salary = women_salary. Ha: men_salary =/= women_salary, telling it to fit intercept explicitly in the lm function of means with a factor variable\nsalary_lm_a <- lm(salary ~ 1 + sex, data = salary_dat)\n\n#use confint to find 95% for sex parameter\ntidy(salary_lm_a) #I had a plan for using the broom functions I swear\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   24697.      938.     26.3  5.76e-31\n2 sexFemale     -3340.     1808.     -1.85 7.06e- 2\n\n#I just realized I should finish the homework instead of mess around with pacakges.\nconfint(salary_lm_a)\n\n\n               2.5 %    97.5 %\n(Intercept) 22812.81 26580.773\nsexFemale   -6970.55   291.257\n\nBy fitting a model for the data and getting a fit for the intercept, which will represent the Male salary data and the fit for the Female survey data we can compare their means and get p-value for the Female salary data. With the estimated level for the Female variable at -3340 of the intercept/Male data, we can say that we estimate that men are being paid more by just over 3300 dollars. We are confident of this at the .1 level for the two sided test, and looking at the confint range it seems likely that people in the Female dataset at this school were getting paid lower looking at the sex variable alone.\n-B) Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\n## Do the linear model for all of the variables. Year is lowercase unlike Q\nsalary_lm_b <- lm(salary ~ sex + degree + rank + year + ysdeg, data = salary_dat)\n\n#look at the model summary. point estimate has flipped to positive. hmm.\nsummary(salary_lm_b)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + rank + year + ysdeg, data = salary_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\nsexFemale    1166.37     925.57   1.260    0.214    \ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n#confint for the sexFemale\nconfint(salary_lm_b, \"sexFemale\", level = .95)\n\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\nOnly the next question asks for the interpretation, but I’ll put that question for this variable examination here. The p-value for the sexFemale variable is above thresholds for significance, so the confidence in our interval of difference is pretty low. I’m suspicious of it. The positive point estimate as well as the range on the interval, however, would suggest that holding all else equal that sexFemale is associated with a higher salary.\n-C) Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nThe rank variable and its levels of Professor and Associate Professor are significant all the way up at the functional 0 level in relation to the Assistant Professor level. The coefficients show that each level of rank that a Professor climbs is associate with a little bit more than five thousand dollar increase in our predicted salary formula.\nThe year someone has been in the current rank variable level is significant at the same level code, but with a much lower slope when compared to the coefficient leap between ranks suggesting that getting across the cataegory threshold in rank is more important than time served for salary. Between 10 and 12 years of service equates to the change in a rank level.\nThe degreePhD coefficient is not significant with its p of .180, but it does have a positive value of 1388.61. I was a little surprised by this one, but perhaps the rank variable or something else picks up the effects of higher education? I would have assumed it would have been a little bit higher from the very little I know about general education union contracts. But hey, it was the 1980’s who knows what was going on.\nThe ysdeg variable has a slightly negative slope, but it is just beyond the .1 level of significance. The only thing I could think of that might explain the slope generally, though I’m not certain it’s really helping the model all that much given the other categories, would be to adjust some of the longer serving professors who only have masters degrees downwards slightly, and since we don’t have an interaction between the ysdeg and degree level it’s messy enough to not be at a statistically significant level.\n-D) Change the baseline category for the rank variable. Interpret the coefficients related to rank again. My lack of clear reading of the instructions on the variables almost got me again, just like the quizzes! However, I realized just in time that the rank variable has three levels and I was thinking about the degree variable in my head for some reason. Phew, okay I’m going to relevel the rank variable on Associate Professor and check it out.\n\n\n#relevel the rank variable, run the model again\nsalary_dat$rank <- relevel(salary_dat$rank, ref = \"Assoc\")\n\n#new model\nsalary_lm_d <- lm(salary ~ rank + sex + degree + year + ysdeg, data = salary_dat)\n\n#summarizing the model\nsummary(salary_lm_d)\n\n\n\nCall:\nlm(formula = salary ~ rank + sex + degree + year + ysdeg, data = salary_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \ndegreePhD    1388.61    1018.75   1.363    0.180    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nNow that the middle rank is the reference category, the coefficients have shifted so that the roughly five thousand dollar increase we predict as you go up the scale is now seen in the in the -5292.36 coefficient value of stepping down to the Asst level, and the same 5826.4 increase that we saw in the last model going from Assoc to Prof.\n-E) Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\n#same as salary_lm_b but removing rank.\nsalary_lm_e <- lm(salary ~ sex + degree + year + ysdeg, data = salary_dat)\n\n#summary of model\nsummary(salary_lm_e)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + year + ysdeg, data = salary_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\nconfint(salary_lm_e, \"sexFemale\", level = .95)\n\n\n              2.5 %   97.5 %\nsexFemale -3928.138 1355.049\n\nThe sexFemale coefficient is back to being a negative value relative to the Male level in our intercept. The p value is above significance thresholds, however.\nOutside of the main point of the question, I’m a little stumped on why the degreePhD coefficient is negative relative to the Masterslevel in the intercept besides perhaps small sample size messiness. Theysdeg variable looks like it picks up some of the descriptive power that the rank variable left behind as we would expect. I would have assumed some of the same for PhD. I’m sure my self-narration of something that’s potentially indicating the whole function is wrong and therefore all of my answers will be amusing after I see the actual answers.\n-F) Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\n#relevel rank\nsalary_dat$rank <- relevel(salary_dat$rank, ref = \"Asst\")\n\n#make a dummy variable for years since degree less than 15 to note as hired by new_dean.\nsalary_dat <- salary_dat %>%\n  mutate(new_dean = case_when(\n    ysdeg <= 15 ~ 1,\n    ysdeg > 15 ~ 0\n  ))\n\n\n\n#the model\nsalary_lm_f <- lm(salary ~ year + sex + degree + rank + new_dean  + new_dean*year, data = salary_dat)\n\n#summary of model\nsummary(salary_lm_f)\n\n\n\nCall:\nlm(formula = salary ~ year + sex + degree + rank + new_dean + \n    new_dean * year, data = salary_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3309.8 -1102.5  -265.2   539.2  9339.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   12011.05    1931.93   6.217 1.62e-07 ***\nyear            495.72      97.42   5.088 7.20e-06 ***\nsexFemale      1115.96     862.06   1.295   0.2022    \ndegreePhD      1161.63     859.23   1.352   0.1833    \nrankAssoc      5416.07    1079.72   5.016 9.14e-06 ***\nrankProf      11612.80    1284.64   9.040 1.37e-11 ***\nnew_dean       3789.08    1867.72   2.029   0.0486 *  \nyear:new_dean  -195.43     183.99  -1.062   0.2940    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2358 on 44 degrees of freedom\nMultiple R-squared:  0.8629,    Adjusted R-squared:  0.8411 \nF-statistic: 39.58 on 7 and 44 DF,  p-value: < 2.2e-16\n\nconfint(salary_lm_f, \"new_dean\", level = .95)\n\n\n            2.5 %  97.5 %\nnew_dean 24.93623 7553.22\n\n#check variance inflation factor for model\ncar::vif(salary_lm_f)\n\n\n                  GVIF Df GVIF^(1/(2*Df))\nyear          2.639661  1        1.624703\nsex           1.366920  1        1.169154\ndegree        1.562142  1        1.249857\nrank          3.611003  2        1.378501\nnew_dean      8.153135  1        2.855369\nyear:new_dean 3.665354  1        1.914512\n\nMulticollinearity is a concern in this question because the years since highest degree and the new variable new_dean would be measuring similar things and therefore not clearing the additivity consideration. The variable year in rank would also potentially be overlapping with the new_dean dummy variable, as the same pre and post 15 year threshold would potentially be seen there. To deal with this, I added an interaction effect with the year in rank variable and the new_dean variable, and after looking at the adjusted R-squared for the model with and without ysdeg and a ysdeg*new_dean and even the monstrosity of ysdeg*years*new_dean, dropping ysdeg made for a better fit. This makes sense as the variable itself lost significance when the rank level was included previously, so dropping that term and adding the interaction effect should deal with the multicollinearity. I also checked the variable inflation factor, and using the second GVIF since I’m using more than one coefficient, and none of the values is near or over 5 to investigate further. The new_dean and ysdeg did have the highest values, however.\nI do find support at the .05 significance level that being hired by the new Dean are making a higher wage than those that were not. The estimated dollar amount of the effect is 3789.08, and I printed the 95% confidence interval above as well.\nQuestion 3\n-A) Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\n\n\ndata(\"house.selling.price\")\n#I like tibbling and assigning to dats, \n#even if smss wants only data() to be the way\nhouse_dat <- as_tibble(house.selling.price)\n\n#make the model for Old set as reference\nhouse_lm_a_old <- lm(Price ~ New + Size, data = house_dat)\n\n\n#summary of new model\nsummary(house_lm_a_old)\n\n\n\nCall:\nlm(formula = Price ~ New + Size, data = house_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nNew          57736.283  18653.041   3.095  0.00257 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n-B) Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient. The combined equation above would be y = -40230.87 + 116.13x1 + 57736.28x2 where x1 = sq ft and x2 = if the house is New (dummy variable = 1). Each of the terms is significant at the .001 level, and size is significant all the way to the 0 code. These terms are all strongly associated with the estimated Price for a home.\nA New home is worth 57,736.28 in price compared to an old home, or 17505.41 if we were just looking at a model for new homes. The intercept is the starting point price for an old home, which at -40,230.88 functions as a sort of tax on being pre-owned that the size of the house must compensate for. An older home would have to be larger than a newer home to have the same predicted price.\nWe can consolidate the starting price point estimates down for an equation for new homes as Price = 17505.41 + 116.13*x1 and for old homes Price = 116.13*x1 -40230.87.\n-C) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n#for the new house with x1 = 3000 sq ft\nprice_new_3k <- (116.13*3000) + 17505.41\n  \n#for the old house with x1 3000 sq ft\nprice_old_3k <- (116.13*3000) - 40230\n\n\n\nFor the 3000 square foot new home, 365895.4. For the old 3000 square foot home the prediction is 308160. I wish I would have been shopping for a home in Gainesville, FL in 2006. Well, except for the whole housing bubble thing that is about to happen back then. Living through that once was enough.\n-D) Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\n#make the new house price model with new and size interaction term\nhouse_lm_d <- lm(Price ~ Size + New + New*Size, data = house_dat)\n\n#summary of the model\nsummary(house_lm_d)\n\n\n\nCall:\nlm(formula = Price ~ Size + New + New * Size, data = house_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n-E) Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\n\n#start with the overall interaction model to simplify\n  #smss uses z for categorical, switch to that vs x2 from earlier.\n  #z = new, x = sq ft\n\n#house_lm_e <- -22227.81 -78527.50x + 104.44x + 61.92(x*z)\n\n#old model line\n#house_lm_e_old <- -22227.81 + (104.44*x)\n\n#new model line\n#house_lm_e_new <- -100755.31 + (104.44*x) + (61.92*x) \n#house_lm_e_new <- -100755.31 + (166.36*x)\n\n\n\nI commented out the line functions as to not have to get the actual assignment of the Size variables and all that, but the line for old homes is Predicted Price = -22227.81 + (104.44*size) and for new homes it is Predicted Price = -100755.31 + (166.36*size).\n-F) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n#(i) x = 3000 sq ft for new home\nlm_int_predict_30k_new <- (166.36*3000) -100755.31\n\n#(ii) x = 3000 sq ft for old home\nlm_int_predict_30k_old <- (104.44*3000) -22227.81\n\n\n\nFor (i) the predicted selling price is 398324.7. For (ii) the predicted selling price is 291092.2.\n-G) Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\n#(i) x = 1500 sq ft for new home\nlm_int_predict_15k_new <- (166.36*1500) - 100755.31\n\n#(ii) x = 1500 sq ft for old home\nlm_int_predict_15k_old <- (104.44*1500) -22227.81\n\n\n\nFor (i) the predicted selling price is 148784.7 and for (ii) it is predicted to be 134432.2. The gap is smaller when compared to the answers for (F), since the slope is higher for new homes. We would expect this as the house size got bigger that new homes would be worth an increasing level more than old homes, roughly 62 dollars per square foot more for each incremental increase.\n-H) Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\n\n\n#do anova on the two models instead of summary!\n\nanova(house_lm_a_old, house_lm_d)\n\n\nAnalysis of Variance Table\n\nModel 1: Price ~ New + Size\nModel 2: Price ~ Size + New + New * Size\n  Res.Df        RSS Df  Sum of Sq      F   Pr(>F)   \n1     97 2.8161e+11                                 \n2     96 2.5957e+11  1 2.2041e+10 8.1519 0.005272 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nInstead of just comparing the R^2 and such from the summaries of the models already printed, I decided to look at the anova results comparing the house_lm_a_old model with no interaction to the house_lm_d model which included the interaction effects. At the .001 significance level, the model including the interaction term does improve the model at capturing the data. I would use the house_lm_d model for predictions as a result if choosing between the two.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-11T16:50:13-04:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomchester887584/",
    "title": "Homework Three",
    "description": "DACSS 603",
    "author": [
      {
        "name": "Cynthia Hester",
        "url": {}
      }
    ],
    "date": "2022-04-11",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nQuestion 1\r\nSolutions\r\n\r\nQuestion 2\r\nSolutions\r\n\r\nQuestion 3\r\nSolutions\r\n\r\n\r\nQuestion 1\r\n(SMSS 11.2, except part (d))\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\r\nSolutions\r\nPart A\r\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\nHere’s what we have:\r\nThe selling price of the home depends on the size of the home and the lot size. Therefore, the selling price of home is the dependent or the response variable. The size of home is the first explanatory variable, and lot size is the second explanatory variable. Therefore,the multiple linear regression of the problem is obtained using the formula:\r\n\\(Y_{}\\) = \\(B_{0}\\) +\\(B_{1}\\) \\(x_{i1}\\) + \\(B_{2}\\)\\(x_{i2}\\) + €\r\nwhere:\r\n\\(Y_{i}\\) = dependent variable that is the selling price of home\r\n\\(B_{0}\\) = the y-intercept\r\n\\(B_{1}\\) and \\(B_{2}\\) are the slopes of the explanatory variables.\r\n\\(x_{i1}\\) is the first explanatory variable that is the size of home\r\n\\(x_{i2}\\) is the second explanatory variable that is the lot size.\r\n€ is the error or residual\r\nwhere:\r\n\\(Y_{}\\) is the selling price of home.\r\n\\(x_{1}\\) is the size of home\r\n\\(x_{2}\\) is the lot size.\r\nFor a home of 1240 square feet on a lot of 18,000 square feet,we predict the selling price with estimated multiple linear regression using the predictive equation:\r\n\\(\\hat{y}\\) = -10,536+53.8\\(x_{1}\\)+2.84\\(x_{2}\\) = $107,206\r\nCalculating selling price in R\r\n\r\n\r\nhide\r\n\r\n house_selling_price<- -(10536)+53.8*1240+2.84*18000      # selling_price in R\r\n print(house_selling_price)                               # review of output\r\n\r\n\r\n[1] 107296\r\n\r\nHouse selling price = $107296\r\nNow we calculate the residual which is the difference between the observed value and the mean value the model predicts for that observation. It is the vertical distance between a data point and the regression line. It is a measure of how well a line fits an individual data point.\r\n\r\n\r\nhide\r\n\r\nresidual_house_price<-145000-house_selling_price    #residual price of house\r\nprint(residual_house_price)\r\n\r\n\r\n[1] 37704\r\n\r\nInterpretation:\r\nSince we have a positive residual value, it means the actual selling price is MORE than the predicted selling price. The house therefore sold for more than was predicted.\r\nPart B\r\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\nInterpretation:\r\nThe slope coefficient for the first explanatory variable size of the home is obtained as 53.8 and is positive. For every additional unit increase in the size of the home, the selling price of the home increases by 53.8 sq.ft., keeping the other explanatory variable that is lot size fixed.\r\nThus, for every additional square-foot increase in the size of the home, the selling price of the home increases by $53.80, keeping the other explanatory variable that is lot size fixed.\r\nPart C\r\nAccording to this prediction equation, for fixed home size, how much would the lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\nInterpretation:\r\nThe slope coefficient for the second explanatory variable lot size is 2.84 and is positive. For every additional unit increase in the lot size, the selling price of the house increases by 2.84 sq.ft., keeping the other explanatory variable home size which is 53.8 fixed. Therefore, taking the equation :\r\n\\(\\hat{y}\\) = -10,536+53.8\\(x_{1}\\)+2.84\\(x_{2}\\)\r\nWe multiply lot size 53.8*(1)/2.84 where 1 is the one-square-foot increase in home size and divide by lot size 2.84.\r\n\r\n\r\nhide\r\n\r\nlot_increase<-53.8*1/2.84\r\nprint(lot_increase)\r\n\r\n\r\n[1] 18.94366\r\n\r\n19 sq.ft. is the amount the lot size would need to increase to have the same impact as a one-square-foot increase in home size.\r\nQuestion 2\r\n(ALR, 5.17, slightly modified)\r\n(Data file: salary in alr4 R package).\r\nThe data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\nFirst, I inspect the data set for an understanding what it is about.\r\n\r\n\r\nhide\r\n\r\nglimpse(salary)         \r\n\r\n\r\nRows: 52\r\nColumns: 6\r\n$ degree <fct> Masters, Masters, Masters, Masters, PhD, Masters, PhD~\r\n$ rank   <fct> Prof, Prof, Prof, Prof, Prof, Prof, Prof, Prof, Prof,~\r\n$ sex    <fct> Male, Male, Male, Female, Male, Male, Female, Male, M~\r\n$ year   <int> 25, 13, 10, 7, 19, 16, 0, 16, 13, 13, 12, 15, 9, 9, 9~\r\n$ ysdeg  <int> 35, 22, 23, 27, 30, 21, 32, 18, 30, 31, 22, 19, 17, 2~\r\n$ salary <int> 36350, 35350, 28200, 26775, 33696, 28516, 24900, 3190~\r\n\r\nhide\r\n\r\nsummary(salary)       \r\n\r\n\r\n     degree      rank        sex          year            ysdeg      \r\n Masters:34   Asst :18   Male  :38   Min.   : 0.000   Min.   : 1.00  \r\n PhD    :18   Assoc:14   Female:14   1st Qu.: 3.000   1st Qu.: 6.75  \r\n              Prof :20               Median : 7.000   Median :15.50  \r\n                                     Mean   : 7.481   Mean   :16.12  \r\n                                     3rd Qu.:11.000   3rd Qu.:23.25  \r\n                                     Max.   :25.000   Max.   :35.00  \r\n     salary     \r\n Min.   :15000  \r\n 1st Qu.:18247  \r\n Median :23719  \r\n Mean   :23798  \r\n 3rd Qu.:27259  \r\n Max.   :38045  \r\n\r\nhide\r\n\r\nhead(salary,15)       #First 15 rows of the data set\r\n\r\n\r\n    degree  rank    sex year ysdeg salary\r\n1  Masters  Prof   Male   25    35  36350\r\n2  Masters  Prof   Male   13    22  35350\r\n3  Masters  Prof   Male   10    23  28200\r\n4  Masters  Prof Female    7    27  26775\r\n5      PhD  Prof   Male   19    30  33696\r\n6  Masters  Prof   Male   16    21  28516\r\n7      PhD  Prof Female    0    32  24900\r\n8  Masters  Prof   Male   16    18  31909\r\n9      PhD  Prof   Male   13    30  31850\r\n10     PhD  Prof   Male   13    31  32850\r\n11 Masters  Prof   Male   12    22  27025\r\n12 Masters Assoc   Male   15    19  24750\r\n13 Masters  Prof   Male    9    17  28200\r\n14     PhD Assoc   Male    9    27  23712\r\n15 Masters  Prof   Male    9    24  25748\r\n\r\nSolutions\r\nPart A\r\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\nFirst, to gain an understanding of the data, I import the Salary data from the alr4textbook library.\r\n\r\n\r\nhide\r\n\r\nlibrary(alr4)\r\ndata(\"salary\")\r\ndim(salary)\r\n\r\n\r\n[1] 52  6\r\n\r\nhide\r\n\r\nlibrary(skimr)\r\nsummary(salary)\r\n\r\n\r\n     degree      rank        sex          year            ysdeg      \r\n Masters:34   Asst :18   Male  :38   Min.   : 0.000   Min.   : 1.00  \r\n PhD    :18   Assoc:14   Female:14   1st Qu.: 3.000   1st Qu.: 6.75  \r\n              Prof :20               Median : 7.000   Median :15.50  \r\n                                     Mean   : 7.481   Mean   :16.12  \r\n                                     3rd Qu.:11.000   3rd Qu.:23.25  \r\n                                     Max.   :25.000   Max.   :35.00  \r\n     salary     \r\n Min.   :15000  \r\n 1st Qu.:18247  \r\n Median :23719  \r\n Mean   :23798  \r\n 3rd Qu.:27259  \r\n Max.   :38045  \r\n\r\nWe see that mean salary for men is $24,696.79 and mean salary for women is $21,357.14 a difference of $3339.65.\r\nNow we test the hypothesis\r\nHypothesis Testing:\r\n\\(H_{0}\\): \\(=\\) mean salaries between men and women are the same\r\n\\(H_{a}\\): \\(\\neq\\) there is a difference in mean salaries of men and women\r\nSignificance Level = 0.05\r\nTest Statistic = t-test\r\nNow that I have a better understanding of the data, I check whether the variance between men and women is the same.\r\n\r\n\r\nhide\r\n\r\n## Calculating mean salary by sex using pipes and the group_by function\r\n\r\nsalary %>% group_by(sex) %>% \r\n  summarise(mean = mean(salary), \r\n              sd = sd(salary))\r\n\r\n\r\n# A tibble: 2 x 3\r\n  sex      mean    sd\r\n  <fct>   <dbl> <dbl>\r\n1 Male   24697. 5646.\r\n2 Female 21357. 6152.\r\n\r\nWe now use the Two Sample t-test to determine the confidence interval .\r\n\r\n\r\nhide\r\n\r\nt.test(salary~sex, data = salary, var.equal = T,    #t-test to calculate the 95% CI\r\n       conf.level = 0.95, alternative = \"two.sided\")\r\n\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.8474, df = 50, p-value = 0.0706\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -291.257 6970.550\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\nInterpretation:\r\nThe p-value of the t-test = 0.0706, therefore at a 5% significance level,there is not enough evidence to reject the null hypothesis \\(H_{0}\\) since the probability of the null hypothesis \\(H_{0}\\) being true is 7.06% higher than the benchmark rejection of 5%. We therefore fail to reject the null hypothesis \\(H_{0}\\) that mean salary for men and women are the same.\r\nPart B\r\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\nFirst, we run a multiple regression model using salary.\r\n\r\n\r\nhide\r\n\r\nlm_model <- lm(salary~.,data = salary)    #multiple linear regression\r\nsummary(lm_model)                         #review of output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nPart 2 of B\r\nAssuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\nI now calculate the 95% confidence interval for the difference between male and female salaries, using a t-test.\r\n\r\n\r\nhide\r\n\r\nt.test(salary~sex, data = salary, var.equal = T,    #t-test to calculate the 95% CI\r\n       conf.level = 0.95, alternative = \"two.sided\")\r\n\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.8474, df = 50, p-value = 0.0706\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -291.257 6970.550\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\n95% confidence interval for the difference in salaries between male and female is:\r\n[-291.257 6970.550] where -291.257 is the lower bound and 6970.550 is the upper bound\r\nUsing the summary function and the linear model (lm) we once again see the significance level or p-value is 0.0706 or 7.06%. Furthermore, we see the point estimate of the Sex variable is $3340 in favor of males.\r\n\r\n\r\nhide\r\n\r\nsummary(lm_model <- lm(salary ~ sex, salary))$coef\r\n\r\n\r\n             Estimate Std. Error  t value     Pr(>|t|)\r\n(Intercept) 24696.789   937.9776 26.32983 5.761530e-31\r\nsexFemale   -3339.647  1807.7156 -1.84744 7.060394e-02\r\n\r\nPart C\r\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables.\r\nHere, I calculate the multiple linear regression to determine the statistical significance of the predictor variables.\r\n\r\n\r\nhide\r\n\r\nlm_model <- lm(salary~.,data = salary)    #multiple linear regression\r\nsummary(lm_model)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nInterpretation:\r\nIntercept: Average salary of a person is estimated to be $15746.05, with no impact from any other predictors.\r\ndegreePHD: For a person holding a PHD degree, there is a positive slope and an average increase in the salary of the person by $1388.61 keeping all other variables as constant. The p-value is 0.18 > 0.05 therefore not statistically significant.\r\nrankAssoc: For a person who holds an associate rank, there is a positive slope and an average increase in the salary of the person by $5292.36, keeping all other variables as constant. The p-value is 3.22e-05 < 0.05 and is statistically significant.\r\nrankProf: For a person who holds an professor rank, there is a positive slope and an average increase in the salary of the person by $11118.76, keeping all other variables as constant. The p-value is 1.62e-10 < 0.05 and is statistically significant.\r\nsexFemale: For females, there is a positive slope and an average increase in the salary of $1166.37 as compared to males, keeping all other variables as constant. The p-value is 0.214 > 0.05 and is statistically significant.\r\nyear: For an extra year of person holding same rank, there is a positive slope and an average increase in the salary of $476.31, keeping all other variables as constant. The p-value is 8.65e-06 < 0.05 and is statistically significant.\r\nysdeg: For an additional year of holding the highest degree, there is a negative slope and an average decrease in the salary of $124.57, keeping all other variables as constant. The p-value is 0.115 > 0.05 and is not statistically significant.\r\nPart D\r\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\r\nI first create new dummy variables for the rank variable(s).\r\n\r\n\r\nhide\r\n\r\nsalary$D1 <- ifelse(salary$rank==\"Asst\",1,0)\r\nsalary$D2 <- ifelse(salary$rank==\"Prof\",1,0)\r\n\r\n\r\n\r\nRevised linear models after changing the base category of rank\r\n\r\n\r\nhide\r\n\r\n# Linear Model after changing the base category of rank\r\nlm_model <- lm(salary~degree+D1+D2+sex+year+ysdeg,data = salary)\r\nsummary(lm_model)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + D1 + D2 + sex + year + ysdeg, \r\n    data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nD1          -5292.36    1145.40  -4.621 3.22e-05 ***\r\nD2           5826.40    1012.93   5.752 7.28e-07 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nAnalysis:\r\nRevised Rank variables:\r\nD1 = Assistant Rank\r\nD1 = -5292.36 implies that for a person holding an Assistant Rank, there is an average decrease in salary by $5292.36 compared to a person holding a Associate Rank, keeping all other variables as constant.\r\nD2 = Professor Rank\r\nD2 = 5826.40, implies that for a person holding a Professor Rank, there is an average increase in the salary of $5826.40 compared to a person holding an Associate Rank, keeping all other variables as constant.\r\nPart E\r\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\r\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\r\nExcluding the variable rank in the linear regression model\r\n\r\n\r\nhide\r\n\r\n#linear regression model using the lm function\r\n\r\nlm_model_no_rank<-lm(formula=salary~sex+degree+year+ysdeg,data=salary)\r\nsummary(lm_model_no_rank)                       #review of linear model output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ sex + degree + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\r\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \r\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \r\nyear          351.97     142.48   2.470 0.017185 *  \r\nysdeg         339.40      80.62   4.210 0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\r\n\r\nInterpretation:\r\nExcluding the rank variable, the coefficient for SexFemale has a negative slope, indicating a salary advantage for males,however, the 𝑝-value is 0.33 which is greater than > 0.05, indicating the difference is not statistically significant.\r\nExcluding the rank variable, the coefficient for degreePhd has a negative slope, indicating a decrease in salary for females of approximately $3299.35. With a 𝑝-value of 0.0147 which is < less than benchmark, this indicates statistical significance.\r\nExcluding the rank variable, the coefficient for year has a positive slope, indicating a increase a positive relationship to salary of approximately $351.97. With a 𝑝-value of 0.017185 which is < less than 0.05 benchmark, this indicates statistical significance.\r\nExcluding the rank variable, the coefficient for ysdeg has a positive slope, indicating a increase a positive relationship to salary of approximately $339.40. With a 𝑝-value of 0.000114 which is < less than 0.05 benchmark, this indicates statistical significance.\r\nAnalysis:\r\nBy excluding the rank variable we see that coefficients for the predictor variables, degreePhd,year,ysdeg all have p-values of less than the standard benchmark of 0.05. This indicates statistical significance. Whereas, the relationship between salary and the predictor variable sexFemale has a p-value greater than the benchmark of 0.05,which indicates statistical insignificance. Of note, it is observed when the variable rank is removed, degreePhd is statistically significant, whereas before it was statistically insignificant, sexFemale is still statistically insignificant when rank is removed.This may indicate there is no gender salary discrimination. Predictor variable year is still statistically insignificant when rank is removed whereas, ysdeg was statistically insignificant before the removal of rank but is now statistically significant after the removal.\r\nPart F\r\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\r\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\nLet’s start with our hypotheses\r\nHypotheses:\r\n\\(H_{0}\\): \\(>\\) Mean salary of hires for new Dean are greater than mean salary of hires of former Dean\r\n\\(H_{a}\\): \\(\\leq\\) Mean salary of hires for new Dean are less than or equal to mean salary of former Dean\r\nThe year they earned their highest degree 16 or more were hired by the old dean and 15 or less were hired by the new dean. I will create dummy variables representing each dean and their hires. I choose this method because new dean and old dean are binary, and by definition dummy variables are dichotomous. With old dean greater than 16 represented by 1 and new dean less than 15,represented by 0.\r\nI now check if there is any multicollinearity in our model\r\n\r\n\r\nhide\r\n\r\nlm_model_ysdeg <- lm(salary~.,data = salary)    #multiple linear regression\r\nsummary(lm_model_ysdeg)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients: (2 not defined because of singularities)\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\nD1                NA         NA      NA       NA    \r\nD2                NA         NA      NA       NA    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nModel would not knit with the vif function. However, Vif chunk will run when separated from rest of model.\r\n\r\n\r\nhide\r\n\r\n#vif(lm_model_ysdeg)#variance inflation factor (VIF)\r\n\r\n\r\n\r\nInterpretation:\r\nWhen the linear regression model is run including all predictors we see that ysdeg has a p-value of 0.627 which is larger than the benchmark of 0.05 and is therefore not statistically significant. Furthermore, when running the model using VIF the Variance Inflation Factor, the variable ysdeg is 8.967 which is significantly larger than the other variables, with a value significantly greater than the accepted value of 5. This may indicate potentially severe correlation between the predictor variable year and other predictor variables in the model.\r\nThe year they earned their highest degree 16 or more were hired by the old dean and 15 or less were hired by the new dean. I will create dummy variables representing each dean and their hires. I choose this method because new dean and old dean are binary, and by definition dummy variables are dichotomous. With old dean greater than 16 represented by 1 and new dean less than 15,represented by 0.\r\n\r\n\r\nhide\r\n\r\nsalary$hires<-ifelse(salary$ysdeg<=15,1,0)  #dummy variable \r\n\r\nnew_dean<-lm(salary~rank+degree+sex+year+hires,data=salary)\r\nsummary(new_dean)                         # review model output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ rank + degree + sex + year + hires, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3403.3 -1387.0  -167.0   528.2  9233.8 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 13328.38    1483.38   8.985 1.33e-11 ***\r\nrankAssoc    4972.66     997.17   4.987 9.61e-06 ***\r\nrankProf    11096.95    1191.00   9.317 4.54e-12 ***\r\ndegreePhD     818.93     797.48   1.027   0.3100    \r\nsexFemale     907.14     840.54   1.079   0.2862    \r\nyear          434.85      78.89   5.512 1.65e-06 ***\r\nhires        2163.46    1072.04   2.018   0.0496 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2362 on 45 degrees of freedom\r\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \r\nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nAnalysis:\r\nTo avoid any multicollinearity I removed the variable that has high correlation and/or similarity to the original model. In this instance I removed the predictor variable ysdeg because of its similarity to years it also had a p-value of 0.627 greater than the bench mark of 0.05 as well as a Variance Inflation Factor VIF score of 8.9, which well exceeded the benchmark of 5.\r\nWhen the linear model is run omitting the ysdeg variable, year appears to have a statistically significant p-value of 1.65e-06. Moreover, an adjusted R-squared of 0.8407 indicates a strong correlation. It is important to avoid any multicollinearity in the model because we want a model of independent variables. If the variables are too close when fitting the model we can have skewed results, in other words the variables may not provide unique or independent information.\r\nQuestion 3\r\n(SMSS 13.7 & 13.8 combined, modified)\r\n(Data file: house.selling.price in smss R package)\r\nSolutions\r\nPart A\r\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\nI first import the house selling price data and inspect it.\r\n\r\n\r\nhide\r\n\r\nlibrary(smss)\r\ndata(house.selling.price)\r\nstr(house.selling.price)\r\n\r\n\r\n'data.frame':   100 obs. of  7 variables:\r\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\r\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\r\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\r\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\r\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\r\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\r\n\r\nI then look at the first 10 rows of the data for ease of review and create a new object for the dataset.\r\n\r\n\r\nhide\r\n\r\nhead(house.selling.price,10)          #first 10 rows of data set\r\n\r\n\r\n   case Taxes Beds Baths New  Price Size\r\n1     1  3104    4     2   0 279900 2048\r\n2     2  1173    2     1   0 146500  912\r\n3     3  3076    4     2   0 237700 1654\r\n4     4  1608    3     2   0 200000 2068\r\n5     5  1454    3     3   0 159900 1477\r\n6     6  2997    3     2   1 499900 3153\r\n7     7  4054    3     2   0 265500 1355\r\n8     8  3002    3     2   1 289900 2075\r\n9     9  6627    5     4   0 587000 3990\r\n10   10   320    3     2   0  70000 1160\r\n\r\nhide\r\n\r\nselling_price<-house.selling.price     #house selling price\r\n\r\n\r\n\r\nI fit a multiple linear regression to show the relationship of selling price of the new house in terms of its size.\r\n\r\n\r\nhide\r\n\r\nreg_selling_price<-lm(Price~New+Size,data=house.selling.price)  \r\nsummary(reg_selling_price)        #review of output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New + Size, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nCoefficients of house selling price data set\r\n\r\n\r\nhide\r\n\r\nreg_selling_price<-lm(Price~New+Size,data=selling_price)$coefficients  \r\nsummary(reg_selling_price)           #view regression model output\r\n\r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \r\n-40230.9 -20057.4    116.1   5873.9  28926.2  57736.3 \r\n\r\nPerforming a correlation test to determine relationship between New house and house Size\r\n\r\n\r\nhide\r\n\r\ncor.test(selling_price$Size, selling_price$New) #correlation test\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  selling_price$Size and selling_price$New\r\nt = 4.1212, df = 98, p-value = 7.891e-05\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 0.2032530 0.5399831\r\nsample estimates:\r\n      cor \r\n0.3843277 \r\n\r\nAnalysis:\r\nControlling for Size we see that predictor variables New and Size have p-values of 0.00257 and 2e-16 respectively which are statistically significant since they are less than the benchmark of 0.05. This indicates we can reject the \\(H_{0}\\) null hypotheses since there is no correlation between New and Price of new homes. Further, we can also reject \\(H_{0}\\) the null hypothesis for the relationship between Size and Price since there is no correlation between them as well. By calculating the correlation we see the correlation is 0.3843277 which is indicates a weak correlation.\r\nPart B\r\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\nWe first start with a modification of the previously seen linear regression model.\r\n\\(\\ E\\)\\((y)_{}\\) = \\(\\alpha\\) +\\(B_{1}\\) \\(x_{i1}\\) + \\(B_{2}\\)\\(x_{i2}\\) + \\(B_{p}\\)\\(x_{p}\\) + €\r\nwhere:\r\n\\(\\ E\\) is the estimated new/not new house selling price\r\n\\(y_{}\\) is the dependent variable/outcome variable\r\n\\(\\alpha\\) is the intercept\r\n\\(B_{1}\\) and \\(B_{2}\\) are the slopes of the explanatory variables.\r\n\\(B_{p}\\)\\(x_{p}\\) the coefficient \\(B\\) is the expected increase in the dependent variable \\(y_{}\\) for a one unit increase in the independent predictor variables \\(\\rho\\)\r\nI will now run multi-linear regression\r\n\r\n\r\nhide\r\n\r\nreg_selling_price<-lm(Price~New+Size,data = selling_price)\r\nsummary(reg_selling_price)     #view regression model output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New + Size, data = selling_price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nWe then separate coefficients of the model for easier undesirability\r\n\r\n\r\nhide\r\n\r\nreg_selling_price<-lm(Price~New+Size,data=selling_price)$coefficients \r\nsummary(reg_selling_price)       #view regression model output\r\n\r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \r\n-40230.9 -20057.4    116.1   5873.9  28926.2  57736.3 \r\n\r\nWe now plug the coefficients into our predictor equation where the dummy variables (1 = new; 0 = not_new) are.\r\n\\(\\ E\\) = -40231+116(Size)+57736(New) where 116 sq.ft.is the size which remains constant\r\n\\(\\ E\\)\\((price)_{} = -40230.9+116\\)\r\n\\(\\ E = -40230.9+116.1+57736.283\\)\r\n\\(\\ E\\)\\((price)_{}\\) = -40231+57736(New)+116(Size) = 17,505(New) +116 (Size) again lot size at 116 sq.ft. remains constant\r\nAnalysis:\r\nWe see that for New and Used houses there is an increase in square footage associated with a price increase of $116. Also,for each house size there is an expected price increase of $57736 for a new house. Interestingly, the impact of each variable is separate.\r\nPart C\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n(i)\r\nWe can use the results from Part B to answer the following questions:\r\n\\(\\ E\\)\\((y)_{}\\) = \\(\\alpha\\) +\\(B_{1}\\) \\(x_{i1}\\) + \\(B_{2}\\)\\(x_{i2}\\) + \\(B_{p}\\)\\(x_{p}\\) + €\r\nwhere the estimated equation is:\r\n\\(\\ E\\)\\((y)_{}= 17505 + 116.10(3000)\\)\r\n\r\n\r\nhide\r\n\r\nnew = 17505              #variable created for \"new\" retrieved from previous problem\r\nsq_ft = 116.10           #variable created for \"square feet\"\r\n\r\nnew_house_price<-(new+sq_ft*3000)         #object created for \"new_house_price\"\r\nprint(new_house_price)                    #view output\r\n\r\n\r\n[1] 365805\r\n\r\nNew_House = $365,805\r\n(ii)\r\nEstimated equation \\(\\ E\\)\\((y)_{}= -40230.9 + 116.10(3000)\\)\r\n\r\n\r\nhide\r\n\r\nnot_new_price = -40230.9        #new object created  \"not_new_price\" for used houses\r\nsq_ft   = 116.10\r\n\r\nnot_new_price<-(not_new_price+sq_ft*3000)\r\n                    \r\nprint(not_new_price)   #view output\r\n\r\n\r\n[1] 308069.1\r\n\r\nNot_New_House = $308,069\r\nPart D\r\nFit another model, this time with an interaction term allowing interaction between\r\nsize and new, and report the regression results\r\nHere we fit a linear model showing the interaction between New and Size.\r\n\r\n\r\nhide\r\n\r\n                              #interaction between size and new\r\nnew_size_reg<-(lm(Price~New*Size,data = house.selling.price)) \r\nsummary(new_size_reg)         #view regression model output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New * Size, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432  0.15536    \r\nNew         -78527.502  51007.642  -1.540  0.12697    \r\nSize           104.438      9.424  11.082  < 2e-16 ***\r\nNew:Size        61.916     21.686   2.855  0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nLinear model with coefficients for better readability\r\n\r\n\r\nhide\r\n\r\n#view regression model output\r\nsummary(lm(Price~New*Size,data = house.selling.price))$coefficients\r\n\r\n\r\n                Estimate   Std. Error   t value     Pr(>|t|)\r\n(Intercept) -22227.80793 15521.109973 -1.432102 1.553627e-01\r\nNew         -78527.50235 51007.641896 -1.539524 1.269661e-01\r\nSize           104.43839     9.424079 11.082080 7.198590e-19\r\nNew:Size        61.91588    21.685692  2.855149 5.271610e-03\r\n\r\nAnalysis:\r\nNew and Size have a p-value 0.00527 which is less than the standard benchmark of\r\n0.05 and thus appears to be statistically significant.\r\nPart E\r\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\r\n(i) and (ii)\r\nWe plot a linear model using ggplot to determine the “Predicted Selling Price vs Size for Houses that are New and Used”. Where the dummy variable 0 denotes not_new and 1** denotes new\r\n\r\n\r\nhide\r\n\r\nggplot(new_size_reg,aes(y=Size,x=Price))+\r\n  geom_point()+\r\n  geom_smooth(method=\"lm\",se= T,full_range=T)+\r\n  facet_wrap(.~New)+\r\n  labs(x = \"Price in Dollars\",y = \"Size in square feet\",title = \"Price vs Size for Houses that are Not_New and New\" )\r\n\r\n\r\n\r\n\r\nPart F\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\nPredicted value of new house\r\n(i)\r\nI fit a linear model to determine the predicted selling price of a 3000 sq. ft. new home\r\n\r\n\r\nhide\r\n\r\nnew_house_predict<-lm(Price~New+Size+Size*New,data = house.selling.price)\r\nsummary(new_house_predict)     #review of output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New + Size + Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432  0.15536    \r\nNew         -78527.502  51007.642  -1.540  0.12697    \r\nSize           104.438      9.424  11.082  < 2e-16 ***\r\nNew:Size        61.916     21.686   2.855  0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nUsing our trusty estimated equation we plug in the predictor variables from the\r\nmultilinear regression model we calculated earlier. Note for the new house we will\r\nuse the dummy variable “1” and for not_new house we use the dummy variable “0”\r\nEstimated equation:\r\n\\(\\ E\\)\\((y)_{}= -22227.81 -78527.50+104.44(3000)+61.92(3000)\\)\r\n\r\n\r\nhide\r\n\r\n#Predicting new house price for home with 3000 sq ft.\r\n\r\nnew_house_predict_est<-(-22227.81-78527.50+104.44*3000+61.92*3000)\r\nprint(new_house_predict_est)                  \r\n\r\n\r\n[1] 398324.7\r\n\r\nNew house predicted selling price(3000 sq.ft.) = $398324.70\r\n(ii)\r\nWe now calculate predicted price for the used aka not_new house house\r\nEstimated equation:\r\n\\(\\ E\\)\\((y)_{}= -22227.81 +104.44(3000)+61.92(0)\\)\r\n\r\n\r\nhide\r\n\r\nnot_new_predict_est<-(-22227.81+104.44*3000+61.92*0)\r\nprint(not_new_predict_est)\r\n\r\n\r\n[1] 291092.2\r\n\r\nNot_new house predicted selling price(3000 sq.ft.) = $291092.20\r\nPart G\r\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\n(i)\r\nPredicting selling price of a new house of 1500 square feet\r\nEstimated equation \\(\\ E\\)\\((y)_{}= -22227.81 -78527.50+104.44(1500)+61.92(1500)\\)\r\n\r\n\r\nhide\r\n\r\nnew_house_predict_est<-(-22227.81-78527.50+104.44*1500+61.92*1500)\r\nprint(new_house_predict_est)   \r\n\r\n\r\n[1] 148784.7\r\n\r\nNew house predicted selling price(1500 sq.ft.) = $148784.70\r\n(ii)\r\nEstimated equation \\(\\ E\\)\\((y)_{}= -22227.81 +104.44(1500)+61.92(0)\\)\r\n\r\n\r\nhide\r\n\r\n#Predicted value of used house with 1500 square feet\r\n\r\nnot_new_predict_est<-(-22227.81+104.44*1500+61.92*0)\r\nprint(not_new_predict_est)\r\n\r\n\r\n[1] 134432.2\r\n\r\nNot_new house predicted selling price(1500 sq.ft.) = $134432.20\r\nInterpretation:\r\nThe predicted selling price difference between a new 3000 sq ft and 1500 sq ft new house is:\r\n$398324.70-$148784.70 = $249540\r\nPredicted selling price difference between new 3000 sq. ft and 1500 sq ft house\r\nexpressed as a percentage\r\n\r\n\r\nhide\r\n\r\npredict_selling_new_diff<-(398324.70 - 148784.70)/((398324.70 + 148784.70)/2)*100\r\nprint(predict_selling_new_diff)   #review output\r\n\r\n\r\n[1] 91.22124\r\n\r\nPercentage difference between new 3000 sq.ft. and new 1500 sq.ft. is 91.22%\r\nThe predicted selling price difference between a new 3000 sq ft and and a not_new 3000 sq ft house is:\r\n$398324.70-$291092.20 = $107232.50\r\nThe predicted selling price difference between a not_new 3000 sq ft and a not_new 1500 sq ft house is:\r\n$291092.20-$134432.20 = $156660\r\nPredicted selling price difference between not_new 3000 sq. ft and not_new 1500 sq ft house as a percentage\r\n\r\n\r\nhide\r\n\r\npredict_selling_price_used_diff<-(291092.20 - 134432.20)/((291092.20 + 134432.20)/2)*100\r\nprint(predict_selling_price_used_diff)   #review output\r\n\r\n\r\n[1] 73.6315\r\n\r\nPercentage difference between a not_new 3000 sq ft and not_new 1500 sq ft house is 73.63%\r\nThe predicted selling price difference between a new 1500 sq ft and and a not_new 1500 sq ft house is:\r\n$148784.70-$134432.20 = $14352.50\r\nOverall, we see that as the size of a new house increases it has more value than comparably sized used aka not_new houses.\r\nPart H\r\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\r\nModel 1 with interaction term\r\n\r\n\r\nhide\r\n\r\nnew_house_predict<-lm(Price~New+Size+Size*New,data = house.selling.price)\r\nsummary(new_house_predict)     #review of output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New + Size + Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432  0.15536    \r\nNew         -78527.502  51007.642  -1.540  0.12697    \r\nSize           104.438      9.424  11.082  < 2e-16 ***\r\nNew:Size        61.916     21.686   2.855  0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nModel 2 without interaction term\r\n\r\n\r\nhide\r\n\r\nreg_selling_price<-lm(Price~New+Size,data=house.selling.price)  \r\nsummary(reg_selling_price)        #review of output\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New + Size, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nAnalysis :\r\nThe model with the interaction term has an adjusted \\(R^{2}\\) of 0.7363 compared to an adjusted \\(R^{2}\\) of 0.7169 of the model without one. This suggests that the additional input variable is contributing to the model. Furthermore, comparing the \\(R^{2}\\) between Model 1 and Model 2, the \\(R^{2}\\) predicts that Model 1 is a better model as it carries greater explanatory power (0.7443 in Model 1 vs. 0.7226 in Model 2). Therefore,Model 1 would be my preference.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomchester887584/distill-preview.png",
    "last_modified": "2022-04-11T16:50:17-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomrhyslong96888198/",
    "title": "Homework 3",
    "description": "Here is Rhys Long's submission for homework 3.",
    "author": [
      {
        "name": "Rhys Long",
        "url": {}
      }
    ],
    "date": "2022-04-11",
    "categories": [],
    "contents": "\n\n\n\n##Question 1\n#A\nTo find the predicted selling price, I must first replicate the regression equation. I will also set x1, the home size variable to 1240 square feet and x2, the lot size variable, to 18000 square feet.\n\n[1] 107296\n\nThe predicted selling price generated by the above equation is 107,296 dollars. Now I will subtract the expected selling price, 145,000 dollars, from the actual selling price to find the residual\n\n[1] 37704\n\nThe residual is 37,704 dollars. This means that the house is being sold for a price that is 37,704 dollars higher than the predicted price.\n#B\nIn order to figure out how much house price is expected to increase per square foot of house size while the lot size stays the same, I will portray a hypothetical scenario. In this hypothetical scenario, the X variable without the number next to it represents the fixed a fixed lot size and the X variables with numbers next to them represent six hypothetical house sizes, between 1240 and 1245. I will now run the regression equation six times with a different house size each time.\n\n\n\nNow that I have my Y1-Y5 values, I will subtract them by each other to figure out the price increase per square foot\n\n[1] 53.8\n[1] 53.8\n[1] 53.8\n[1] 53.8\n[1] 53.8\n\nBased on my results, there is a 53 dollar and 80 cent increase in price per square foot of house size added when lot size is fixed.\n#C\nIn order to figure out how much house price is expected to increase per square foot of lot size while the house size stays the same, I will start by re-enacting the same hypothetical scenario from part B. However, this time, the fixed x value will correspond to house size and the increasing x values will correspond to lot size.\n\n[1] 2.84\n[1] 2.84\n[1] 2.84\n[1] 2.84\n[1] 2.84\n\nI can conclude that house price goes up by 2 dollars and 84 cents per square foot increase in lot size when house size stays the same. Now, in order to figure out how much of a lot size increase would be necessary for a price increase of 53 dollars and 80 cents, I will divide 53.8 by 2.84.\n\n[1] 18.94366\n\nAccording to my calculations above, Lot size would need to increase by 18.94366 square feet in order to have the same increase as a one square foot increase in lot size. Now, I will use the code below to check my work. If I get 53.8, I’ll know I did it right.\n\n[1] 53.8\n\n##Question 2\n#A\nIn order to see whether there is a statistically significant difference between males and females, I first need to create a dummy variable for males.\n\n\n\nNow I will use the lm() function to create a regression\n\n\nCall:\nlm(formula = salary ~ Male, data = salary)\n\nCoefficients:\n(Intercept)         Male  \n      21357         3340  \n\nFinally, I will use summary to find the statistical significance\n\n\nCall:\nlm(formula = salary ~ Male, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    21357       1545  13.820   <2e-16 ***\nMale            3340       1808   1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\nAccording to the data shown above, there is a statistically significant difference between male and female salary to the extent of p<=0.1, but not to the extent of p<=0.05.\n#B\nIn order to find a regression equation, I must first usee the string function to figure out how many dummy variables are necessary for each categorical variable. The categorical variables that need dummy variables are degree, rank, and sex.\n\n'data.frame':   52 obs. of  7 variables:\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\n $ Male  : num  1 1 1 0 1 1 0 1 1 1 ...\n\nNow that I know the amount of categorical variables per category, I will create dummy variables for people with PhDs, associates, and professors.\n\n\n\nIn order to find the confidence interval for male salaries with regard for other variables, I must first create a regression model for every variable using the lm() function.\n\n\n\nNow, I will use the summary function to find the confidence interval for male summary.\n\n\nCall:\nlm(formula = salary ~ PhD + Assoc + Prof + Male + year + ysdeg, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 16912.42     816.44  20.715  < 2e-16 ***\nPhD          1388.61    1018.75   1.363    0.180    \nAssoc        5292.36    1145.40   4.621 3.22e-05 ***\nProf        11118.76    1351.77   8.225 1.62e-10 ***\nMale        -1166.37     925.57  -1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nNow that I know that the male confidence interval is 925.57, I now need the mean salary for males\n\n  mean(salary)\n1     24696.79\n\nNow that I have the male mean, I can determine the confidence interval.\n\n[1] 22882.67 26510.91\n\nAssuming there are no interactions, the confidence interval for male salary is 22882.67-26510.91.\n#C\n\n\nCall:\nlm(formula = salary ~ PhD + Assoc + Prof + Male + year + ysdeg, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 16912.42     816.44  20.715  < 2e-16 ***\nPhD          1388.61    1018.75   1.363    0.180    \nAssoc        5292.36    1145.40   4.621 3.22e-05 ***\nProf        11118.76    1351.77   8.225 1.62e-10 ***\nMale        -1166.37     925.57  -1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nThe t-values for Intercept, Assoc, Prof, and Year are the only t-values that are statistically significant. Based on the above t-scores, it is safe to assume that employment ranking and years of work experience play the most major roles in dictating salary.\nThe outcome variable, salary has an intercept of 16912.42. The intercept has a higher value than the coefficients.\nThe only coefficient that comes close to the intercept is prof, with a coefficient of 11118.76 and the second highest coefficient is the Assoc coefficient of 5292. These coefficients are consistent with employment rank being a major predictor of salary.\nThe third highest coefficient is the PhD coefficient of 1388.61 and the third lowest coefficient is year, with a coefficient of 476.31. This is not consistent with the fact that year is a statistically significant predictor, but having a PhD is not.\nThe only negative coefficients that appeared are those for Ysdeg and Males. The Ysdeg slope is -124.57, and the slope for males is -1166.37. This means that males who aren’t associates or professors, don’t have PhDs or a lot of work experience, actually make less money than many of the women. THis is especially true for men who didn’t get their most recent degree that long ago.\n#D\n\n\nCall:\nlm(formula = salary ~ PhD + Asst + Assoc + Male + year + ysdeg, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  28031.18    1677.06  16.715  < 2e-16 ***\nPhD           1388.61    1018.75   1.363    0.180    \nAsst        -11118.76    1351.77  -8.225 1.62e-10 ***\nAssoc        -5826.40    1012.93  -5.752 7.28e-07 ***\nMale         -1166.37     925.57  -1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nWith rank changed to show dummy variables for Asst and Assoc instead of Assoc and Prof, ranks still play a statistically signigicant role in dictating salary. However, the coefficient for Assoc changed from 5292.36 to -5826.40 . This means that when professor isn’t a rank option anymore, but assistant is, being an associate decreases the odds of having a high salary. However, associates still make way more than assistants who have a dreadfully low coefficient of -11118.76. Interestingly, the coefficient for assistants is the negative version of the coefficient for professors.\n#E\n\n\nCall:\nlm(formula = salary ~ PhD + Male + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15897.03    1259.87  12.618  < 2e-16 ***\nPhD         -3299.35    1302.52  -2.533 0.014704 *  \nMale         1286.54    1313.09   0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\nWithout rank factored in, the coefficient for PhD becomes negative, the coefficient for Male becomes positive, and the coefficients are almost equal in magnitude (both are positive), and PhD and ysdeg both become statistically significant. Based on these findings, it is safe to assume that rank plays a ginormous role in dictating salary.\n#F\nThe new variable would be a dummy variable labeled New_Dean. Those who arrived before the new dean will have zeros listed and those who arrived after the dean arrives will have ones listed.\n\n\n\nTo avoid multi-colinearity I’m going to replace ysdeg with New_Dean. The reason why I can’t have both simultaneously is because they’re both redundant and having two redundant variables increases the odds of multi-colinearity drastically.\n\n\nCall:\nlm(formula = salary ~ PhD + Assoc + Prof + Male + year + New_Dean, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14235.51    1289.13  11.043 2.12e-14 ***\nPhD           818.93     797.48   1.027   0.3100    \nAssoc        4972.66     997.17   4.987 9.61e-06 ***\nProf        11096.95    1191.00   9.317 4.54e-12 ***\nMale         -907.14     840.54  -1.079   0.2862    \nyear          434.85      78.89   5.512 1.65e-06 ***\nNew_Dean     2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\nGiven that the coefficient for New_Dean is 2163.46 and the T-Score for New_Dean suggests statistical significance, it is safe to assume that the hypothesis that the people hired by the new Dean are making higher than those that were not is true.\nQuestion 3\n#A\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n#B\nBased on the code above, the regression equation is Y=-40230.867+116.132X1+57736.283X2. In order to detect the separate regression equation for new and older homes, I am going to assume that X1 is zero and X2 is the dummy variable number that coincides with whether the house is new or not. This means that bigger homes are more expensive than smaller homes and newer homes are more expensive than older homes. The differences shown in the size and newness variables are also statistically significant. Now, I am going to use the regression equation to create separate regression equations for newer and older homes.\n\n[1] -40230.9\n\n\n[1] 17505.4\n\nFor older homes, the regression is older home is Y=-40230.9+116.1X and for newer homes, the regression equation is Y=17505.4+116.1X. In order to find out whether there is a significant difference between older homes and newer homes, I will now use the summary function on the equation from part A.\n#C\nNow, I will use the new equations to figure out the price for a 3000 square foot home. I will also use the multiple regression equation to check my work.\n\n[1] 308069.1\n[1] 308069.1\n\n\n[1] 365805.4\n[1] 365805.4\n\nWhen setting the house size variable, X, to 3000 square feet, older homes have a predicted selling price of 308,069 dollars and 10 cents while newer homes have a predicted selling price of 365,805 dollars and 40 cents.\n#D\nNow, I will create a regression equation with an interaction term for size and newness.\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\nWhen adding an interaction variable for size and newness, the coefficient for size doesn’t change much, but the coefficient for newness becomes negative instead of positive. The size variable remains statistically significant, but the newness variable does not. Instead, the interaction variable is statistically significant. The new regression equation is Y=104.44X1-78527.50X2+61.916X1X2.\n#E\nIn order to figure out the new regression equations, I will assume that the X variable for house size is zero and I will use whether the house is new or not as a dummy variable, like I did when looking for the separate non-interaction equations.\n\n[1] 0\n\n\n[1] -78527.5\n\nThe regression equation for older homes is Y=104.44X1 and the regression equation for newer homes is Y=-78527.5+104.44X1+61.916X1X2.\n#F\nNow, I will use the new equations to figure out the price for a 3000 square foot home. Like I did in earler questions, I will also be using the multiple regression equation to check my work.\n\n[1] 313320\n[1] 313320\n\n\n[1] 420540.5\n[1] 420540.5\n\nWhen setting the house size variable, X, to 3000 square feet, older homes have a predicted selling price of 313,320 dollars while newer homes have a predicted selling price of 420,540 dollars and 50 cents.\n#G\nNow, I will be looking for the price of 1500 square foot homes\n\n[1] 156660\n[1] 156660\n\n\n[1] 171006.5\n[1] 171006.5\n\nWhen setting the house size variable, X, to 1500 square feet, older homes have a predicted selling price of 156,660 dollars while newer homes have a predicted selling price of 171,006 dollars and 50 cents. The prices of older vs newer homes is a lot more similar for small homes than they are for larger homes.\n#H\nThere are pros and cons to each model. The upside of the non-interaction model is the fact that the difference between the prices of older vs newer homes doesn’t change no matter how high or low the house size variable is, as shown in the code below. Howeverm it doesn’t take into consideration the interaction between house age and size. Meanwhile, the in the model with the interaction, the interaction is taken into consideration, but the gap in price of older vs newer homes increases as the house size variable increases.\n\n[1] 133919.1\n[1] 191655.4\n[1] 57736.3\n\n\n[1] 308069.1\n[1] 365805.4\n[1] 57736.3\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-11T16:50:21-04:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomkirstenrjohnsondacss603hw1v2/",
    "title": "DACSS 603 Homework 1",
    "description": "Homework 1",
    "author": [
      {
        "name": "Kirsten Johnson",
        "url": {}
      }
    ],
    "date": "2022-04-11",
    "categories": [],
    "contents": "\nQuestion 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nFirst I’ll create variables for each procedure’s sample size, mean, and standard deviation.\n\n\n### Create variables for sample sizes, means, and standard deviation\nbypass_sample = 539\nbypass_mean = 19\nbypass_sd = 10\n\nangiography_sample = 847\nangiography_mean = 18\nangiography_sd = 9\n\n\n\nThe 90% confidence interval upper and lower bounds will be the sample mean of each procedure plus and minus their margin of error, respectively. The margin of error can be obtained by multiplying the t-score of each procedure by their standard errors. I will use the qt function to obtain the t-score of each procedure. Since we’re looking for a 90% confidence interval, the area of each tail will be 5%.\n\n\n## Calculate T-Score\nbypass_tscore<- qt(p=0.95, df = bypass_sample-1)\nangiography_tscore <- qt(p=0.95, df = angiography_sample-1)\n\n## print t-scores\nbypass_tscore\n\n\n[1] 1.647691\n\nangiography_tscore\n\n\n[1] 1.646657\n\nNext, I need the standard error. The standard error for each procedure is their standard deviation divided by the square root of the sample size.\n\n\n## Calculate Standard Error\nbypass_se = bypass_sd/sqrt(bypass_sample)\nangiography_se= angiography_sd/sqrt(angiography_sample)\n\n## print standard errors\nbypass_se\n\n\n[1] 0.4307305\n\nangiography_se\n\n\n[1] 0.3092437\n\nNext, I will calculate the margin of error, which as stated above is the t-score multiplied by the standard error.\n\n\n## Calculate Margin of Error\nbypass_me= bypass_tscore*bypass_se\nangiography_me = angiography_tscore*angiography_se\n\n## print Margins of Error\nbypass_me\n\n\n[1] 0.7097107\n\nangiography_me\n\n\n[1] 0.5092182\n\nWith the margins of error, we can now calculate the uppper and lower bounds of the confidence intervals.\n\n\n## Calculate upper and lower bounds for each procedure\nbypass_upper= bypass_mean + bypass_me\nbypass_lower=bypass_mean - bypass_me\nbypass_CI = c(bypass_lower, bypass_upper)\n\nangiography_upper= angiography_mean + angiography_me\nangiography_lower=angiography_mean - angiography_me\nangiography_CI = c(angiography_lower, angiography_upper)\n\n## Print Confidence Intervals\nbypass_CI\n\n\n[1] 18.29029 19.70971\n\nangiography_CI\n\n\n[1] 17.49078 18.50922\n\nThe 90% confidence interval for the bypass procedure is 18.29029-19.70971, making the range 1.41942. For angiography, the 90% confidence interval is 17.49078- 18.50922, making the range 1.01844. Due to the angiography confidence interval range being smaller than the bypass procedure range, the confidence interval is narrower for angiography.\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p. \nI can calculate the point estimate by simply dividing the number of people who believe college education is essential for success by the sample size.\n\n\n## Get the point estimate\npoint_estimate = 567/1031\npoint_estimate\n\n\n[1] 0.5499515\n\nWith the point estimate, I could calculate the standard error, then use that and the z-score for the 95% confidence interval to calculate the confidence interval for the point estimate. However, for ease I will use the prop.test function to get the confidence interval.\n\n\n## Use prop.test to obtain 95% confidence interval\nprop.test(567, 1031, 0.5499515)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5499515\nX-squared = 0, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\nThe function returned a confidence interval of 0.5194543-0.5800778. This means that we can say with 95% certainty that the proportion of adult Americans who believe college is essential for success is between 51.95% and 58.00%.\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nWe can determine the sample size for estimating the population mean by using the formula:\nsample size = standard deviation^2 * (z-score/margin of error)^2\nWe are given a margin of error of 5. We are given the textbook price range and can estimate the standard deviation.\n\n\n## Get sample standard deviation\nsample_sd = (200-30)/4\nsample_sd\n\n\n[1] 42.5\n\nNext, we know that the confidence interval will be 95% since we are given a significance level of 5%. We can use the qnorm function to obtain the z-score for the 95% confidence interval.\n\n\n## Get z-score for 95% confidence, each tail is 0.025.\nzscore = qnorm(0.975)\nzscore\n\n\n[1] 1.959964\n\nNow, we can calculate our sample size.\n\n\n## Calculate sample size\nsample_size= sample_sd^2 * (zscore/5)^2\nsample_size\n\n\n[1] 277.5454\n\nThe financial aid office needs a sample size of about 278.\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nWe are assuming the sample size of the female employees is large enough for use to use to estimate the population mean. We are also assuming that this is a normal distribution.\nThe null hypothesis is that the income of female employees does not differ from $500 per week. The alternative hypothesis is that the income of female employees is different from 500 per week.\nWe can use the values for the sample size (9 females), the sample mean ($410), the population mean for our null hypothesis (500), and the sample standard deviation (90) to calculate the test statistic.\n\n\n### Calculate t-statistic\ntstat = (410-500)/(90/(sqrt(9)))\ntstat\n\n\n[1] -3\n\nThe test statistic is -3 or absolute value of 3. Now that we have the test statistic, I can get the P-Value.\n\n\n### Get two-tailed test P value\npvalue= 2 * pt(tstat, df=8)\npvalue\n\n\n[1] 0.01707168\n\nAssuming a significance level of 0.05, we can reject the null hypothesis, since 0.05 >0.01707168. The P-value returned is saying that there is a 1.7% likelihood that we would observe this sample mean if the null hypothesis were true.\nReport the P-value for Ha : μ < 500. Interpret.\n\n\n### Get P-value for left tail\npvalueleft= pt(tstat, df= 8)\npvalueleft\n\n\n[1] 0.008535841\n\nThere is a .853% probability that we would observe what we did if the population mean is greater than 500. We would reject the null hypothesis.\nReport and interpret the P-value for H a: μ > 500.\n\n\n###Get P-value for right tail\npvalueright= 1-pt(tstat, df= 8)\npvalueright\n\n\n[1] 0.9914642\n\nThere is a 99.14% probability that we would observe what we did if population mean is less than 500. We would fail to reject the null hypothesis.\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7,with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nWe will calculate the T-Statistics for Jones and Smith’s different sample means.\n\n\n ##Get Jones and Smith T-statistic\ntstatJones = (519.5-500)/(10)\ntstatSmith= (519.7-500)/(10)\ntstatJones\n\n\n[1] 1.95\n\ntstatSmith\n\n\n[1] 1.97\n\nUsing those t-statistics, we will calculate the P-values.\n\n\npvalueJones= 2*pt(tstatJones, df= 999, lower.tail=FALSE)\npvalueSmith= 2*pt(tstatSmith, df= 999, lower.tail = FALSE)\npvalueJones\n\n\n[1] 0.05145555\n\npvalueSmith\n\n\n[1] 0.04911426\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nJones’ study result has a p-value of 0.051,which is greater than the 0.05 significance level. As a result, he is unable to reject the null hypothesis, as his results statistically insignificant. Smith’s study has a p-value of 0.049, which is greater than 0.05 significance level. As a result, he can reject the null hypothesis since his result is statistically significant.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nSince both Jones and Smith had test results with p values within 1/100th of the significance level, it is important to report the actual p-values in order to determine the strength of the evidence for rejecting or accepting the null hypothesis. If someone reading the report saw how close the p-values were to the significance level, they may want to look deeper into the data before deciding to trust the author of the study.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nFirst, I will calculate the mean of the gas taxes in the 18 cities.\n\n\n## Create vector for gas tax sample from 18 cities\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nmean_gas_taxes= round(sum(gas_taxes)/18)\nmean_gas_taxes\n\n\n[1] 41\n\nThe sample mean is 41 cents. We want to know if there is evidence to support the average gas tax per gallon being less than 45 cents. Since our alternative hypothesis is that the true average is less than 45 cents, we only need to do a one sided t-test.\n\n\n### Conduct a one-sided t-test to see whether true mean gas tax per gallon is less than 45 cents\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\nOur 95% confidence interval gives us a significance level of 5% or 0.05. Since our p-value is 0.038, the results are statistically significant and we can reject the null hypothesis that gas taxes are not less than 45 cents. We also can see that the 95% confidence interval is negative infinity to 44.8 cents, which our sample mean of 41 is within.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-11T16:50:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomkirstenrjohnsondacss603hwv2/",
    "title": "DACSS 603 Homework 2",
    "description": "Homework 2",
    "author": [
      {
        "name": "Kirsten Johnson",
        "url": {}
      }
    ],
    "date": "2022-04-11",
    "categories": [],
    "contents": "\nQuestions 1\n(Problem 1.1 in ALR)\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\nLet’s load and preview the UN11 data set first. Since we’re only interested in ppgdp and fertility, I’ll filter out the variables that I don’t need.\n\n\n### Load United Nations data\ndata(\"UN11\")\n### Filter and preview data for ppgdp and fertility\nUN11<- UN11 %>%\n  select(c(ppgdp, fertility))\nhead(UN11)\n\n\n              ppgdp fertility\nAfghanistan   499.0     5.968\nAlbania      3677.2     1.525\nAlgeria      4473.0     2.142\nAngola       4321.9     5.135\nAnguilla    13750.1     2.000\nArgentina    9162.1     2.172\n\n1.1.1. Identify the predictor and the response.\nSince we are wondering about the dependence of fertility on ppgdp, the predictor or the independent variable is ppgdp and the response or the independent variable is fertility.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\nI’ll use the plot function to create a scatterplot of ppgdp vs fertility.\n\n\n### Draw scatterplot of ppgdp and fertility\nggplot(UN11, aes(x=ppgdp, y=fertility)) + \n  geom_point(shape=10, color=\"darkblue\")+\n  geom_smooth(method=lm, se=FALSE, color=\"darkorange2\")+\n  labs(title = \"Fertility v. Gross National Product Per Person\", x = \"Gross National Product Per Person (USD)\", y = \"Fertility (Births per 1000 women)\")\n\n\n\n\nA scatter plot doesn’t seem like a plausible summary of this graph. As ppgdp decreases, fertility has more values but doesn’t appear have a consistent response or pattern. The straight line mean function doesn’t summarize the data in a way to determine if/what the relationship is between the variables or if there is a clear dependence of the response on the predictor.\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\n### Draw scatterplot of log(ppgdp) and log(fertility)\nggplot(UN11, aes(x=log(ppgdp), y=log(fertility))) + \n  geom_point(shape=10, color=\"darkblue\")+\n  geom_smooth(method=lm, se=FALSE, color=\"darkorange2\")+\n  labs(title = \"Log Fertility vs. Log Gross National Product Per Person\", x = \"Log(Gross National Product Per Person)\", y = \"Log(Fertility)\")\n\n\n\n\nA simple linear regression model using the natural log of the variables looks to be a much better fit to show the dependencey fertility on ppgdp. We can clearly see that as ppgdp increases, fertility decreases.\nQuestion 2\n(Problem 9.47 in SMSS)\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n(a) How, if at all, does the slope of the prediction equation change?\n(b) How, if at all, does the correlation change?\nTo illustrate, I will create an imaginary scenario where annual income predicts the size of someone’s residence.I will create columns for income in USD, income in pounds, and residence size in square feet.\n\n\nusd=c(279900, 146500, 237700, 200000, 289900)\npounds=round(usd/1.33)\nsize= c(2048, 912, 1654, 2068,2075)\n\n\n\n\n\n### Generate data of income in USD, income in pounds and residence size in square feet\nid= c(1,2,3,4,5)\nusd=c(27990, 14650, 23770, 20000, 28990)\npounds=round(usd/1.33)\nsize= c(2048, 912, 1654, 2068,2075)\ndf<-data.frame(id,usd,pounds,size)\n### View data\ndf\n\n\n  id   usd pounds size\n1  1 27990  21045 2048\n2  2 14650  11015  912\n3  3 23770  17872 1654\n4  4 20000  15038 2068\n5  5 28990  21797 2075\n\nNow that we have some data, I will use the lm function to fit the data to a linear model where income is the explanatory variable and size is the response variable. I will create separate models for income in pounds and USD.\n\n\n### fit data to linear model for residence size vs income in USD\nusdsizefit<-lm(size~usd, data = df)\nsummary(usdsizefit)\n\n\n\nCall:\nlm(formula = size ~ usd, data = df)\n\nResiduals:\n      1       2       3       4       5 \n -33.01 -273.49 -143.72  523.36  -73.14 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) 202.01974  708.14080   0.285    0.794\nusd           0.06713    0.02991   2.245    0.110\n\nResidual standard error: 353.9 on 3 degrees of freedom\nMultiple R-squared:  0.6268,    Adjusted R-squared:  0.5024 \nF-statistic: 5.039 on 1 and 3 DF,  p-value: 0.1105\n\n\n\n### fit data to linear model for residence size vs income in pounds\npoundssizefit<-lm(size~pounds, data = df)\nsummary(poundssizefit)\n\n\n\nCall:\nlm(formula = size ~ pounds, data = df)\n\nResiduals:\n      1       2       3       4       5 \n -33.02 -273.45 -143.71  523.34  -73.16 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) 201.94000  708.10907   0.285    0.794\npounds        0.08929    0.03977   2.245    0.110\n\nResidual standard error: 353.9 on 3 degrees of freedom\nMultiple R-squared:  0.6269,    Adjusted R-squared:  0.5025 \nF-statistic:  5.04 on 1 and 3 DF,  p-value: 0.1105\n\nIn the USD example, the slope estimate is 0.067. In the pounds example, the slope is 0.089. Since the units of pounds are smaller, as the units get smaller of the explanatory variable, the slope increases. This makes sense since slope can be calculated as y/x. If the units of x get smaller, the slope will increase since the denominator is decreasing. This proves that slope can change if the units of the variables change.\nThe r^2 for both examples are extremely close, only different by 0.0001, this is likely because I rounded the conversion from USD to pounds. For our purposes, the r^2 are the same. This makes since because r^2 is a standardized measure of fit and cannot be impacted by units. If the units of a variable change, their relationship with other variables will not change.\nQuestion 3\n(Problem 1.5 in ALR)\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\nFirst, I will load and preview the data.\n\n\n### Load water dataset\ndata(\"water\")\n### Preview runoff data\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\nFor this data, the precipitation at each site are the predictors and the runoff (BSAAM) is the response. Next I’ll generate the scatter plot matrix for the data.\n\n\n### Generate scatterplot matrix\npairs(water)\n\n\n\n\nSome of the key findings from these scatter plots are:\nThere looks to be positive correlation between APMAM, APSAB and APSLAKE. Any combination of those sites seems to have a positive correlation between their precipitation measurements.\nSimilarly to #1, OPBPC, OPRC, OPSLAKE all look to be positively correlated. My guess is that these sites are nearby each other as are the sites from #1.\nStream runoff volume, BSAAM, looks to be positively correlated with OPBPC, OPRC, and OPSLAKE.\nYear does not look to have a strong correlation with any of the variables.\nQuestion 4\n(Problem 1.6 in ALR - slightly modified)\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\nFirst, I will import the data, filter the variables to only include quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest, and preview the data.\n\n\ndata(\"Rateprof\")\nrateprof<- Rateprof\nrateprofselect<-rateprof%>%\n  select(quality, helpfulness, clarity, easiness, \"raterInterest\")\nhead(rateprofselect)\n\n\n   quality helpfulness  clarity easiness raterInterest\n1 4.636364    4.636364 4.636364 4.818182      3.545455\n2 4.318182    4.545455 4.090909 4.363636      4.000000\n3 4.790698    4.720930 4.860465 4.604651      3.432432\n4 4.250000    4.458333 4.041667 2.791667      3.181818\n5 4.684211    4.684211 4.684211 4.473684      4.214286\n6 4.233333    4.266667 4.200000 4.533333      3.916667\n\nNow that I have the variables I need, I’ll create the scatterplot matrix.\n\n\npairs(rateprofselect, pch = 19,  cex = 0.5,\n      col = \"black\")\n\n\n\n\nHere are some thoughts about the relationships between the ratings:\nQuality vs. helpfulness, quality vs. clarity, and clarity vs. helpfulness all have very strong, positive correlations. Given that these ratings are all very closely related in terms of what makes a good course, it isn’t surprising that these variables show a clear dependency. These ratings have much weaker relationships with easiness and raterInterest.\nInterestingly, raterInterest has a very gentle positive slope against the other ratings, and to me it looks like the slope would be closer to zero across all of the other ratings. This shows that the rater’s interest in the subject matter doesn’t seem to be very dependent on the quality, helpfulness, clarity, or easiness of the course.\nEasiness has a positive relationship with clarity, helpfulness, and quality, but the relationship isn’t as strong as the ratings from #1. There is much more variability in the ratings. There doesn’t seem to be strong dependency of course easiness and raterInterst.\nQuestion 5\n(Problem 9.34 in SMSS)\nFor the student.survey data file in the smss package, conduct regression analyses relating\n(i) y = political ideology and x = religiosity,\n(ii) y = high school GPA and x = hours of TV watching.\n(a) Use graphical ways to portray the individual variables and their relationship.\nI will load the student survey data, select my variables of interest and get a summary of the data.\n\n\n### Load student survey data, select and preview variables for political ideology, religiosity, high school gpa and hours of TV watching\ndata(\"student.survey\")\nstudentsurvey<- student.survey%>%\n  select(pi, re, hi, tv)\nhead(studentsurvey)\n\n\n            pi           re  hi tv\n1 conservative   most weeks 2.2  3\n2      liberal occasionally 2.1 15\n3      liberal   most weeks 3.3  0\n4     moderate occasionally 3.5  5\n5 very liberal        never 3.1  6\n6      liberal occasionally 3.5  4\n\nSince political ideology and religiosity are catagorical variables, I will create different visualizations from high schoolGPA and hours of TV watching, as those are numerical variables.\nFor political ideology vs. religiosity I will create a stacked bar chart.\n\n\nggplot(studentsurvey, \n       aes(x = factor(re,\n                      levels = c(\"never\", \"occasionally\", \n                                 \"most weeks\", \"every week\")),\n           fill = factor(pi, \n                         levels = c(\"very liberal\", \"liberal\", \"slightly liberal\",\n                                    \"moderate\",\"slightly conservative\", \n                                    \"conservative\",\"very conservative\"),\n                         labels = c(\"very liberal\", \"liberal\", \"slightly liberal\",\n                                    \"moderate\",\"slightly conservative\", \n                                    \"conservative\",\"very conservative\")))) + \n  \n  geom_bar(position = \"stack\") +\n  scale_fill_brewer(palette = \"RdBu\", direction= -1) +\n  labs(y = \"Count\", \n       fill = \"Political Ideology\",\n       x = \"Religiousity\",\n       title = \"Political Ideology by Religiosity\") +\n  theme_minimal()\n\n\n\n\nThis stacked bar chart shows that as graduate student responses leaning more liberal increase, frequency attending religious services decreases. As graduate students who identify as conservative responses become more conservative, there are more response to attending religious services more frequently.\nI will create a scatterplot for high school GPA vs hours of TV watching.\n\n\n# scatterplot with linear fit line\nggplot(studentsurvey,\n       aes(x = tv, \n           y = hi)) +\n  geom_point(shape=10, color=\"darkblue\")+\n  geom_smooth(method=lm, se=FALSE, color=\"darkorange2\")+\n  labs(title = \"High School GPA vs. Hours Watching TV\", x= \"Hours Watching TV\", y = \"High School GPA\")\n\n\n\n\nHigh school GPA has a negative relationship with hours watching TV.\n(b) Interpret descriptive statistics for summarizing the individual variables and their relationship.\nI will use the summary function to get descriptive statistics.\n\n\nsummary(studentsurvey)\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nPolitical ideology and religiosity summaries show the number of responses for each category.There is variability in the number of response for each category. For political ideology, there are significantly more responses for the liberal categories versus the conservative. This could mean the graduate school where the survey was taken is in a more liberal location. As a result, there are also more responses for less religiosity. There may be some bias in the sample.\nFor the high school GPA and hours watching TV, we are given numerical summaries. For high school GPA, there is a specific scale, so the quartile values seem reasonable. The hours of TV summary has much more variability, as the max has a huge value that differs significantly from the mean. The presence of these outliners may mean the tv hours data is skewed.\n(c) Summarize and interpret results of inferential analyses.\nI will conduct a correlation test for each of the variable pairs. The political ideology and religiosity variables are categorical, we will need to assume that they are correlated and treat them as numeric.\n\n\n### Conduct regression test for pi vs re\ncor.test(as.numeric(studentsurvey$pi), as.numeric(studentsurvey$re))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  as.numeric(studentsurvey$pi) and as.numeric(studentsurvey$re)\nt = 5.4163, df = 58, p-value = 1.221e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3818345 0.7265650\nsample estimates:\n      cor \n0.5795661 \n\nOur p value is less than 0.05, making the correlation statistically significant between political ideology and religiosity. We can reject the null hypothesis in favor of the alternative hypothesis that true correlation is not equal to zero. The test also returned a positive correlation estimate, meaning they are positively correlated.\nLet’s do the same for GPA and TV hours.\n\n\n### Conduct regression test for hi vs tv\ncor.test(studentsurvey$hi, studentsurvey$tv)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  studentsurvey$hi and studentsurvey$tv\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nWe have another p-value less than 0.05, making the correlation statistically significant between high school GPA and hours watching TV. We can reject the null hypothesis in favor of the alternative hypothesis that true correlation is not equal to zero. This time, the test returned a negative correlation estimate, meaning the variables are negatively correlated, which we saw in our previous graphics.\nQuestion 6\n(Problem 9.50 in SMSS)\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\nThere is an element of chance to take into consideration when observing these scores. Some of the initial test takers may have scored a certain way due to skill, but some of the more extreme scores may have scored a certain way just due to chance. There is inherent variability in the sample outside of someone’s knowledge of the test material. When taking the mean of specifically the tutored students, the odds subset of students doing better on their second try are higher since these students were outliers during the first try. It makes sense that their second try is closer to the mean, since its like taking a new sample. The improvement in the tutored students could be due to their scores regressing towards the mean and not just based on the tutoring program.\n\n\n\n",
    "preview": "posts/httpsrpubscomkirstenrjohnsondacss603hwv2/distill-preview.png",
    "last_modified": "2022-04-11T16:50:30-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomowenvespa883531/",
    "title": "Multiple Regression",
    "description": "DACSS 603 Homework 3",
    "author": [
      {
        "name": "Rhowena Vespa",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\r\n\r\n\r\nlibrary(distill)\r\nlibrary(dplyr)\r\nlibrary(tidyverse)\r\nlibrary(knitr)\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nlibrary(smss)\r\n\r\n\r\n\r\nQuestion 1\r\n(SMSS 11.2, except part (d))\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\r\nA. A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\n\r\n\r\n# predictive equation: ŷ = −10,536 + 53.8x1 + 2.84x2\r\nyhat = -10536 + 53.8*1240 + 2.84*18000 \r\nprint(paste(\"The predicted selling price is\", yhat))\r\n\r\n\r\n[1] \"The predicted selling price is 107296\"\r\n\r\n\r\n\r\nResidual = 145000-yhat\r\nprint(paste(\"The residual error is\", Residual))\r\n\r\n\r\n[1] \"The residual error is 37704\"\r\n\r\nInterpretation: The property sold 37,704 higher than what the model predicted.\r\nB. For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\nFor fixed lot size, the positive regression coefficient is 53.8/ sq ft. This means a house selling price increases $53.8 for each sq ft increase in size, given that X2(independent variable) is constant.\r\nC. According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\n\r\n\r\n#Z=lot size increase to have the same impact as a 1 sq ft increase in home size\r\n#(2.84)*(Z)=(53.8)*(1 sq ft)\r\nx1_coef= 53.8 #house size\r\nx2_coef= 2.84 #lot size\r\nZ=x1_coef/x2_coef\r\nZ \r\n\r\n\r\n[1] 18.94366\r\n\r\nZ1 <- as.integer(round(Z)) #round up answer\r\nprint(paste(\"A lot size increase of\", Z1, \"sq ft have same impact per 1 sq ft increase in house size\"))\r\n\r\n\r\n[1] \"A lot size increase of 19 sq ft have same impact per 1 sq ft increase in house size\"\r\n\r\nQuestion 2\r\n(ALR, 5.17, slightly modified)\r\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\n\r\n\r\ndata(\"salary\") # load the UN11 data\r\ndim(salary)\r\n\r\n\r\n[1] 52  6\r\n\r\nkable(head(salary), format = \"markdown\", digits = 10,caption = \"**1980 Salaries of faculty at Midwestern college**\")\r\n\r\n\r\nTable 1: 1980 Salaries of faculty at Midwestern college\r\ndegree\r\nrank\r\nsex\r\nyear\r\nysdeg\r\nsalary\r\nMasters\r\nProf\r\nMale\r\n25\r\n35\r\n36350\r\nMasters\r\nProf\r\nMale\r\n13\r\n22\r\n35350\r\nMasters\r\nProf\r\nMale\r\n10\r\n23\r\n28200\r\nMasters\r\nProf\r\nFemale\r\n7\r\n27\r\n26775\r\nPhD\r\nProf\r\nMale\r\n19\r\n30\r\n33696\r\nMasters\r\nProf\r\nMale\r\n16\r\n21\r\n28516\r\n\r\nA. Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\n\r\n\r\nt.test(salary~sex, data=salary)\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.7744, df = 21.591, p-value = 0.09009\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -567.8539 7247.1471\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\nEXPLANATION: With a p-value of 0.09, we fail to reject the null hypothesis. We accept the alternative hypothesis: true difference in means between group Male and group Female is not equal to zero. The mean of Male group (24,696) is higher than mean of Female group (21,357)\r\nB.1. Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex.\r\n\r\n\r\nMLM <-lm(salary ~ degree + rank + sex + year + ysdeg, data = salary)\r\nsummary(MLM)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nB.2. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\n\r\n\r\nt.test(salary~sex, data=salary)\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.7744, df = 21.591, p-value = 0.09009\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -567.8539 7247.1471\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\nThe 95% confidence interval for the difference in salary between males and females is [-567.8539 , 7247.1471]\r\nC. Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\r\n\r\n\r\ntab <- matrix(c(\"degree\",  \"NOT significant\",  \"PhD salary change =1388.61\", \"rank\",\"YES- significant\",\"Assoc salary change=5292.36\", \"sex\",\"NOT significant\",\"Female salary change= 1166.37\",\"year\",\"  YES- significant\",\" Salary change per year=476.31\", \"ysdegree\",\"  NOT significant\",\"  Salary change per ysdegree=-124.57 \" ), ncol=3, byrow=TRUE)\r\ncolnames(tab) <- c('Variable','Statistical Significance','Slope:salary increase per change in variable')\r\ntab <- as.table(tab)\r\nkable(tab)\r\n\r\n\r\n\r\nVariable\r\nStatistical Significance\r\nSlope:salary increase per change in variable\r\nA\r\ndegree\r\nNOT significant\r\nPhD salary change =1388.61\r\nB\r\nrank\r\nYES- significant\r\nAssoc salary change=5292.36\r\nC\r\nsex\r\nNOT significant\r\nFemale salary change= 1166.37\r\nD\r\nyear\r\nYES- significant\r\nSalary change per year=476.31\r\nE\r\nysdegree\r\nNOT significant\r\nSalary change per ysdegree=-124.57\r\n\r\nD. Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\r\n\r\n\r\nMLM_Rank <-lm(rank ~ degree + salary + sex + year + ysdeg, data = salary)\r\nprint(MLM_Rank)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = rank ~ degree + salary + sex + year + ysdeg, data = salary)\r\n\r\nCoefficients:\r\n(Intercept)    degreePhD       salary    sexFemale         year  \r\n -0.5925499   -0.4710229    0.0001085   -0.3130130   -0.0616205  \r\n      ysdeg  \r\n  0.0469866  \r\n\r\nINTERPRETATION: Based on the coefficients, degree, sex and year variables have NEGATIVE linear relationship. The variables salary and ysdeg have POSITIVE linear relationship.\r\nE. Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. \r\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\r\n\r\n\r\nMLM <-lm(salary ~ degree  + sex + year + ysdeg, data = salary)\r\nsummary(MLM)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\r\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \r\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \r\nyear          351.97     142.48   2.470 0.017185 *  \r\nysdeg         339.40      80.62   4.210 0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\r\n\r\nINTERPRETATION:\r\nExcluding rank, the model is NOT a better fit with Multiple R-squared: 0.6312,Adjusted R-squared: 0.5998. The variables : degree, year and ysdeg are statistically significant. The results showed that Female sex had negative coefficient -1286.54.\r\nF. Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\r\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\n\r\n\r\nlm(salary ~ rank+ sex + year*ysdeg, data = salary)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ rank + sex + year * ysdeg, data = salary)\r\n\r\nCoefficients:\r\n(Intercept)    rankAssoc     rankProf    sexFemale         year  \r\n  16352.169     5254.351    10351.178      867.705      312.454  \r\n      ysdeg   year:ysdeg  \r\n    -86.317        5.581  \r\n\r\nBased on the new variable coefficient ysdeg:year, there is support for the hypothesis that the people hired by the new Dean are making higher than those that were not.\r\nQuestion 3\r\n(SMSS 13.7 & 13.8 combined, modified)\r\n(Data file: house.selling.price in smss R package)\r\n\r\n\r\ndata(\"house.selling.price\") \r\ndim(house.selling.price)\r\n\r\n\r\n[1] 100   7\r\n\r\nkable(head(house.selling.price))\r\n\r\n\r\ncase\r\nTaxes\r\nBeds\r\nBaths\r\nNew\r\nPrice\r\nSize\r\n1\r\n3104\r\n4\r\n2\r\n0\r\n279900\r\n2048\r\n2\r\n1173\r\n2\r\n1\r\n0\r\n146500\r\n912\r\n3\r\n3076\r\n4\r\n2\r\n0\r\n237700\r\n1654\r\n4\r\n1608\r\n3\r\n2\r\n0\r\n200000\r\n2068\r\n5\r\n1454\r\n3\r\n3\r\n0\r\n159900\r\n1477\r\n6\r\n2997\r\n3\r\n2\r\n1\r\n499900\r\n3153\r\n\r\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\n\r\n\r\nReg <-lm(Price ~ Size + New, data = house.selling.price)\r\nsummary(Reg)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nB. Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\n\r\n\r\nPrice_New_aov <-aov(Price ~ New, data = house.selling.price)\r\nsummary.aov(Price_New_aov)\r\n\r\n\r\n            Df    Sum Sq   Mean Sq F value   Pr(>F)    \r\nNew          1 2.274e+11 2.274e+11   28.29 6.61e-07 ***\r\nResiduals   98 7.878e+11 8.039e+09                     \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nlm(Price ~ New, data = house.selling.price)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ New, data = house.selling.price)\r\n\r\nCoefficients:\r\n(Intercept)          New  \r\n     138567       152396  \r\n\r\nINTERPRETATION: The coefficient 152396 means the selling price increase for variable “NEW” = 1.\r\nC. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\nDummy variable: 1=NEW, 0=NOT NEW\r\nEQUATION: Price =138567 + 57736.283New +116.132 size\r\n\r\n\r\nPrice_NEW = 138567  + 57736.283*1 +116.132 *3000\r\nprint(paste(\"Predicted Selling Price for NEW home is\", Price_NEW))\r\n\r\n\r\n[1] \"Predicted Selling Price for NEW home is 544699.283\"\r\n\r\n\r\n\r\nPrice_OLD = 138567  + 57736.283*0 +116.132 *3000\r\nprint(paste(\"Predicted Selling Price for NOT NEW home is\", Price_OLD))\r\n\r\n\r\n[1] \"Predicted Selling Price for NOT NEW home is 486963\"\r\n\r\nD. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\r\n\r\n\r\nSize_New <-lm(Price ~ Size*New, data = house.selling.price)\r\nsummary(Size_New)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432  0.15536    \r\nSize           104.438      9.424  11.082  < 2e-16 ***\r\nNew         -78527.502  51007.642  -1.540  0.12697    \r\nSize:New        61.916     21.686   2.855  0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nThis model is only slightly better fitting than previous model based on adjusted R-squared of 73.63% compared to 71.69%. The variables size and the new variable size:new are statistically significant with p-values < 0.05\r\nE. Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\r\n\r\n\r\nNew_HSP <- subset(house.selling.price, New %in% c(\"1\"))\r\nOld_HSP <- subset(house.selling.price, New==\"0\")\r\n\r\n\r\n\r\n\r\n\r\nggplot(house.selling.price, aes(x = Size, y = Price, color=New)) +\r\n     geom_point() + \r\n    geom_smooth(method = \"lm\",colour = 7) +\r\n    labs(x=\"Size\", y=\"Price\", title = \"Predicted selling price vs size for NEW and NOT NEW homes\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(New_HSP, aes(x = Size, y = Price, color=New)) +\r\n     geom_point() + \r\n    geom_smooth(method = \"lm\",colour = 7)  +\r\n    labs(x=\"Size\", y=\"Price\", title = \"Predicted selling price vs size for NEW homes\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(Old_HSP, aes(x = Size, y = Price)) +\r\n     geom_point() + \r\n    geom_smooth(method = \"lm\",colour = 7) +\r\n    labs(x=\"Size\", y=\"Price\", title = \"Predicted selling price vs size for NOT NEW homes\")\r\n\r\n\r\n\r\n\r\nF. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\nDummy variable: 1=NEW, 0=NOT NEW\r\nEQUATION: Price =138567 + 57736.283New +116.132 size\r\n\r\n\r\nPrice_NEW = 138567  + 57736.283*1 +116.132 *3000\r\nprint(paste(\"Predicted Selling Price for NEW home is\", Price_NEW))\r\n\r\n\r\n[1] \"Predicted Selling Price for NEW home is 544699.283\"\r\n\r\n\r\n\r\nPrice_OLD = 138567  + 57736.283*0 +116.132 *3000\r\nprint(paste(\"Predicted Selling Price for NOT NEW home is\", Price_OLD))\r\n\r\n\r\n[1] \"Predicted Selling Price for NOT NEW home is 486963\"\r\n\r\nG. Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\nDummy variable: 1=NEW, 0=NOT NEW\r\nEQUATION: Price =138567 + 57736.283New +116.132 size\r\n\r\n\r\nPrice_NEW_1500 = 138567  + 57736.283*1 +116.132 *1500\r\nprint(paste(\"Predicted Selling Price for 1500 sqft NEW home is\", Price_NEW_1500))\r\n\r\n\r\n[1] \"Predicted Selling Price for 1500 sqft NEW home is 370501.283\"\r\n\r\n\r\n\r\nPrice_OLD_1500 = 138567  + 57736.283*0 +116.132 *1500\r\nprint(paste(\"Predicted Selling Price for 1500 sqft NOT NEW home is\", Price_OLD_1500))\r\n\r\n\r\n[1] \"Predicted Selling Price for 1500 sqft NOT NEW home is 312765\"\r\n\r\n\r\n\r\nPrice_diff_3000 = Price_NEW - Price_OLD\r\nPrice_diff_1500 = Price_NEW_1500 - Price_OLD_1500\r\nprint(paste(\"There is no price difference in prices NEW vs NOT NEW 3000 sqft\", Price_diff_3000,\" vs NEW vs NOT NEW 1500 sqft house\",Price_diff_1500 ))\r\n\r\n\r\n[1] \"There is no price difference in prices NEW vs NOT NEW 3000 sqft 57736.2830000001  vs NEW vs NOT NEW 1500 sqft house 57736.283\"\r\n\r\nH. Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\r\nSee Figure 1 and 2 below. The model with the interaction Size:New is a better fitting than model without interaction based on adjusted R-squared of 73.63% compared to 71.69%. The variables are statistically significant with p-value: < 2.2e-16\r\n\r\n\r\nggplot(house.selling.price, aes(x = Size*New, y = Price)) +\r\n   geom_point() +\r\n geom_smooth(method = \"lm\",colour = 7) +\r\n     labs(x=\"Size\", y=\"Price\", title = \"Figure 1:Predicted selling price With Interaction\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(house.selling.price, aes(x = Size, y = Price)) +\r\n   geom_point() +\r\n geom_smooth(method = \"lm\",colour = 7) +\r\n     labs(x=\"Size\", y=\"Price\", title = \"Figure 2:Predicted selling price WITHOUT INTERACTION\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomowenvespa883531/distill-preview.png",
    "last_modified": "2022-04-03T21:42:26-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomektracy884427/",
    "title": "HOMEWORK 3",
    "description": "DACSS 603, Spring 2022",
    "author": [
      {
        "name": "Erin Tracy",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2\r\nA. A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\n\r\n\r\nprediction<- -10536+53.8*1240+2.84*18000 \r\nprediction\r\n\r\n\r\n[1] 107296\r\n\r\nresidual<- 145000 -prediction\r\nresidual\r\n\r\n\r\n[1] 37704\r\n\r\nThe prediction was $107,296. This was calculated by inserting the data (1240 sq ft size of home and 18,000 sq ft lot size) into the equation.\r\nSince the actual selling price was $145,000, the residual is 37,704. That is the difference between the actual price and the predicted price.\r\nFor some reason, this home sold for 37,704 more than it was predicted to sell for.\r\nB. For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\n\r\n\r\nlot_size<- 30000\r\n\r\n#House sizes\r\na<- 2000\r\nb<- 2001\r\nc<- 2002\r\nd<- 2003\r\ne<- 2004\r\n\r\nprediction_a<- -10536+53.8*a+2.84*lot_size\r\nprediction_b<- -10536+53.8*b+2.84*lot_size\r\nprediction_c<- -10536+53.8*c+2.84*lot_size\r\nprediction_d<- -10536+53.8*d+2.84*lot_size\r\nprediction_e<- -10536+53.8*e+2.84*lot_size\r\n\r\n\r\nprediction_a\r\n\r\n\r\n[1] 182264\r\n\r\nprediction_b\r\n\r\n\r\n[1] 182317.8\r\n\r\nprediction_c\r\n\r\n\r\n[1] 182371.6\r\n\r\nprediction_d\r\n\r\n\r\n[1] 182425.4\r\n\r\nprediction_e\r\n\r\n\r\n[1] 182479.2\r\n\r\nprediction_a - prediction_b\r\n\r\n\r\n[1] -53.8\r\n\r\nprediction_b - prediction_c\r\n\r\n\r\n[1] -53.8\r\n\r\nprediction_c - prediction_d\r\n\r\n\r\n[1] -53.8\r\n\r\nprediction_d - prediction_e\r\n\r\n\r\n[1] -53.8\r\n\r\nI used sample data of a 10,000 sq lot size and a 1,000 sq ft house size, and increased the house size by 1. The difference is consistently $53.8. I then altered the lot size to 30,000 sq ft and house size to 2,000 sq ft and increased the size by 1 sq ft. The difference was still consistently 53.8\r\nC. According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\n\r\n\r\n53.8/2.84\r\n\r\n\r\n[1] 18.94366\r\n\r\nhouse_size<- 1000\r\n\r\n#Lot Sizes:\r\nx<- 10000\r\ny<- 10001\r\nz<- 10002\r\n\r\n\r\nprediction_x<- -10536+53.8*house_size+2.84*x*18.94\r\nprediction_y<- -10536+53.8*house_size+2.84*y*18.94\r\nprediction_z<- -10536+53.8*house_size+2.84*z*18.94\r\n\r\n\r\nprediction_x\r\n\r\n\r\n[1] 581160\r\n\r\nprediction_y\r\n\r\n\r\n[1] 581213.8\r\n\r\nprediction_z\r\n\r\n\r\n[1] 581267.6\r\n\r\nprediction_x - prediction_y\r\n\r\n\r\n[1] -53.7896\r\n\r\nprediction_y - prediction_z\r\n\r\n\r\n[1] -53.7896\r\n\r\nFor each square foot of house size, the home price increases by $53.8. For each square foot of lot size, the price increases by $2.84. To increase the price of the home by changing lot size, the same amount as changing 1 sq foot of house size, I would need to divide $53.8 by 2.84, the answer is 18.94\r\nI did a test using sample data again, and this proved correct.\r\nQuestion 2\r\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\n\r\n\r\nlibrary(alr4)\r\ndata(\"salary\")\r\n?salary\r\nhead(salary)\r\n\r\n\r\n   degree rank    sex year ysdeg salary\r\n1 Masters Prof   Male   25    35  36350\r\n2 Masters Prof   Male   13    22  35350\r\n3 Masters Prof   Male   10    23  28200\r\n4 Masters Prof Female    7    27  26775\r\n5     PhD Prof   Male   19    30  33696\r\n6 Masters Prof   Male   16    21  28516\r\n\r\nstr(salary)\r\n\r\n\r\n'data.frame':   52 obs. of  6 variables:\r\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\r\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\r\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\r\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\r\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\r\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\r\n\r\nsummary(salary)\r\n\r\n\r\n     degree      rank        sex          year            ysdeg      \r\n Masters:34   Asst :18   Male  :38   Min.   : 0.000   Min.   : 1.00  \r\n PhD    :18   Assoc:14   Female:14   1st Qu.: 3.000   1st Qu.: 6.75  \r\n              Prof :20               Median : 7.000   Median :15.50  \r\n                                     Mean   : 7.481   Mean   :16.12  \r\n                                     3rd Qu.:11.000   3rd Qu.:23.25  \r\n                                     Max.   :25.000   Max.   :35.00  \r\n     salary     \r\n Min.   :15000  \r\n 1st Qu.:18247  \r\n Median :23719  \r\n Mean   :23798  \r\n 3rd Qu.:27259  \r\n Max.   :38045  \r\n\r\nstr(salary)\r\n\r\n\r\n'data.frame':   52 obs. of  6 variables:\r\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\r\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\r\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\r\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\r\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\r\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\r\n\r\nA. Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\n\r\n\r\nt.test(salary~sex, data = salary)\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.7744, df = 21.591, p-value = 0.09009\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -567.8539 7247.1471\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\nMean Salary is not the same. The mean salary of females (21357) is lower than the mean salary of males (24691).\r\nB. Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\n\r\n\r\nfit <- lm(salary~ degree + rank + sex + year + ysdeg, data = salary)\r\n\r\n\r\nhead(predict(fit), n = 10)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n34412.44 30316.19 28762.69 28001.84 33566.07 31869.70 25433.42 \r\n       8        9       10 \r\n32243.42 30708.21 30583.64 \r\n\r\nset.seed(3)\r\ndf<- data.frame(degree = sample(salary$degree, size = 10, replace = T),\r\n                rank = sample(salary$rank, size = 10, replace = T),\r\n                sex = sample(salary$sex, size = 10, replace = T),\r\n                year = sample(salary$year, size = 10, replace = T),\r\n                ysdeg = sample(salary$ysdeg, size = 10, replace = T))\r\n                \r\n\r\npredict(fit,df)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n15368.63 20671.56 14287.78 29407.55 20069.45 32082.22 23606.80 \r\n       8        9       10 \r\n16837.87 21524.03 28077.52 \r\n\r\nsummary(fit)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nconfint(fit, 'sexFemale')\r\n\r\n\r\n              2.5 %   97.5 %\r\nsexFemale -697.8183 3030.565\r\n\r\nI found a 95% confidence interval that the difference in salary between males and females is between -697 and 3030\r\n\r\n\r\nlibrary(stargazer)\r\n\r\nm1<-lm(salary~sex,data=salary)\r\nm2<-lm(salary~sex+degree+rank, data=salary)\r\nm3<-lm(salary~sex+degree+rank+degree*rank, data=salary)\r\nstargazer(m1,m2,m3, type='text',\r\n          covariate.labels = c('sex', 'degree', 'rank', 'degree*rank'),\r\n          dep.var.labels = 'salary',\r\n          star.cutoffs = NA)\r\n\r\n\r\n\r\n===============================================================================\r\n                                        Dependent variable:                    \r\n                    -----------------------------------------------------------\r\n                                              salary                           \r\n                            (1)                 (2)                 (3)        \r\n-------------------------------------------------------------------------------\r\nsex                     -3,339.647           -862.412            -578.041      \r\n                        (1,807.716)          (978.365)           (985.957)     \r\n                                                                               \r\ndegree                                       1,038.021           3,524.061     \r\n                                             (942.928)          (1,695.485)    \r\n                                                                               \r\nrank                                         4,710.542           6,003.751     \r\n                                            (1,174.879)         (1,618.400)    \r\n                                                                               \r\ndegree*rank                                 11,650.640          12,568.690     \r\n                                            (1,001.670)         (1,138.609)    \r\n                                                                               \r\ndegreePhD:rankAssoc                                             -3,504.919     \r\n                                                                (2,398.807)    \r\n                                                                               \r\ndegreePhD:rankProf                                              -3,670.395     \r\n                                                                (2,282.361)    \r\n                                                                               \r\nConstant                24,696.790          17,921.290          17,242.450     \r\n                         (937.978)           (855.470)           (931.848)     \r\n                                                                               \r\n-------------------------------------------------------------------------------\r\nObservations                52                  52                  52         \r\nR2                         0.064               0.764               0.779       \r\nAdjusted R2                0.045               0.744               0.750       \r\nResidual Std. Error 5,782.082 (df = 50) 2,992.955 (df = 47) 2,958.782 (df = 45)\r\nF Statistic         3.413 (df = 1; 50)  38.087 (df = 4; 47) 26.497 (df = 6; 45)\r\n===============================================================================\r\nNote:                                                                        NA\r\n\r\n(Just experimenting with Stargazer)\r\nC. Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\r\n\r\n\r\nset.seed(3)\r\ndf<- data.frame(degree = sample(salary$degree, size = 10, replace = T),\r\n                rank = sample(salary$rank, size = 10, replace = T),\r\n                sex = sample(salary$sex, size = 10, replace = T),\r\n                year = sample(salary$year, size = 10, replace = T),\r\n                ysdeg = sample(salary$ysdeg, size = 10, replace = T))\r\npredict(fit,df)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n15368.63 20671.56 14287.78 29407.55 20069.45 32082.22 23606.80 \r\n       8        9       10 \r\n16837.87 21524.03 28077.52 \r\n\r\nsummary(fit)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\n?salary\r\n\r\n\r\n\r\nSalary increases by:\r\n1388 if the degree is PHD\r\n5292 if the rank is Assoc\r\n11118 if the rank is Prof\r\n1166 if Female\r\n476 for each year in current rank\r\nSalary decreases by:\r\n124 for years since highest degree\r\nAll slopes are positive, except for Years since highest degree earned.\r\nRank and years in current rank are statistically significant (less than 0.05),other factors are not.\r\nD. Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\r\n\r\n\r\nfit3 <- lm(salary~ degree + year + rank + sex + ysdeg, data = salary)\r\n\r\n\r\nset.seed(3)\r\ndf3<- data.frame(degree = sample(salary$degree, size = 10, replace = T),\r\n                year = sample(salary$year, size = 10, replace = T),\r\n                rank = sample(salary$rank, size = 10, replace = T),\r\n                sex = sample(salary$sex, size = 10, replace = T),\r\n                ysdeg = sample(salary$ysdeg, size = 10, replace = T))\r\n\r\npredict(fit3,df3)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n17963.93 12836.86 25406.54 32741.72 22980.27 32558.53 23606.80 \r\n       8        9       10 \r\n20225.00 17713.56 28656.89 \r\n\r\nsummary(fit3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + year + rank + sex + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nI reordered the variables to change the baseline, but nothing changed, so I think I misunderstood the task.\r\nE. Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\r\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\r\n\r\n\r\nfit2 <- lm(salary~ degree + sex + year + ysdeg, data = salary)\r\n\r\n\r\nset.seed(3)\r\ndf2<- data.frame(degree = sample(salary$degree, size = 10, replace = T),\r\n                sex = sample(salary$sex, size = 10, replace = T),\r\n                year = sample(salary$year, size = 10, replace = T),\r\n                ysdeg = sample(salary$ysdeg, size = 10, replace = T))\r\n\r\npredict(fit2,df2)\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n15631.50 25840.16 23116.76 29904.74 30290.05 27114.13 19427.73 \r\n       8        9       10 \r\n25588.74 23098.28 21451.56 \r\n\r\nsummary(fit2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\r\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \r\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \r\nyear          351.97     142.48   2.470 0.017185 *  \r\nysdeg         339.40      80.62   4.210 0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\r\n\r\nI removed rank.\r\nSalary decreases by:\r\n3299 if the degree is PHD\r\n1286 if Female\r\nSalary increases by:\r\n352 for each year in current rank\r\n339 for years since highest degree\r\nAll slopes are positive, except for Years since highest degree earned.\r\nDegree PHD, Years in Current Rank and Years since highest degree earned are all statistically significant (less than 0.05), sex is not statistially significant.\r\nF. Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\r\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\n\r\n\r\nfit5 <- lm(salary~ rank + degree + sex + year + ysdeg+ year*ysdeg, data = salary)\r\nsummary(fit5)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ rank + degree + sex + year + ysdeg + year * \r\n    ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4055.4 -1007.7  -172.6   800.0  9275.7 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 16289.506    944.422  17.248  < 2e-16 ***\r\nrankAssoc    5627.056   1184.705   4.750 2.19e-05 ***\r\nrankProf    11475.286   1389.241   8.260 1.71e-10 ***\r\ndegreePhD    1557.213   1028.855   1.514   0.1373    \r\nsexFemale    1233.531    925.994   1.332   0.1897    \r\nyear          318.343    174.450   1.825   0.0748 .  \r\nysdeg        -172.406     89.161  -1.934   0.0596 .  \r\nyear:ysdeg      7.094      6.578   1.078   0.2867    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2394 on 44 degrees of freedom\r\nMultiple R-squared:  0.8588,    Adjusted R-squared:  0.8363 \r\nF-statistic: 38.22 on 7 and 44 DF,  p-value: < 2.2e-16\r\n\r\nThe slope for year* ysdeg is positive, supporting the hypothesis.\r\nQuestion 3\r\n(Data file: house.selling.price in smss R package)\r\n\r\n\r\nlibrary(smss)\r\ndata(house.selling.price)\r\nhead(house.selling.price)\r\n\r\n\r\n  case Taxes Beds Baths New  Price Size\r\n1    1  3104    4     2   0 279900 2048\r\n2    2  1173    2     1   0 146500  912\r\n3    3  3076    4     2   0 237700 1654\r\n4    4  1608    3     2   0 200000 2068\r\n5    5  1454    3     3   0 159900 1477\r\n6    6  2997    3     2   1 499900 3153\r\n\r\nsummary(house.selling.price)\r\n\r\n\r\n      case            Taxes           Beds       Baths     \r\n Min.   :  1.00   Min.   :  20   Min.   :2   Min.   :1.00  \r\n 1st Qu.: 25.75   1st Qu.:1178   1st Qu.:3   1st Qu.:2.00  \r\n Median : 50.50   Median :1614   Median :3   Median :2.00  \r\n Mean   : 50.50   Mean   :1908   Mean   :3   Mean   :1.96  \r\n 3rd Qu.: 75.25   3rd Qu.:2238   3rd Qu.:3   3rd Qu.:2.00  \r\n Max.   :100.00   Max.   :6627   Max.   :5   Max.   :4.00  \r\n      New           Price             Size     \r\n Min.   :0.00   Min.   : 21000   Min.   : 580  \r\n 1st Qu.:0.00   1st Qu.: 93225   1st Qu.:1215  \r\n Median :0.00   Median :132600   Median :1474  \r\n Mean   :0.11   Mean   :155331   Mean   :1629  \r\n 3rd Qu.:0.00   3rd Qu.:169625   3rd Qu.:1865  \r\n Max.   :1.00   Max.   :587000   Max.   :4050  \r\n\r\nstr(house.selling.price)\r\n\r\n\r\n'data.frame':   100 obs. of  7 variables:\r\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\r\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\r\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\r\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\r\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\r\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\r\n\r\n?house.selling.price\r\n\r\n\r\n\r\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\n\r\n\r\nsummary(lm(Price~Size+New, data= house.selling.price))\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nB. Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\ny= -40230.87 + 116.13x1 + 57736x2 where x1 is house size and x2 is new/not new\r\nSince X2 is 1 if new and 0 if not new, these formulas can be rewritten:\r\nNEW HOMES\r\ny= -40230.87 + 116.13x1 + 57736\r\nNOT NEW HOMES\r\ny= -40230.87 + 116.13x1\r\nC. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nNew3000<--40231+116*3000+57736\r\nNew3000\r\n\r\n\r\n[1] 365505\r\n\r\nNotNew3000<--40231+116*3000\r\nNotNew3000\r\n\r\n\r\n[1] 307769\r\n\r\nIf this home is new, the predicted selling price is 365,896\r\nIf this home is not new, the predicted selling price is 307,769\r\nD. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\r\n\r\n\r\nhousedata_interaction<-summary(lm(Price~Size+New+Size*New, data= house.selling.price))\r\nhousedata_interaction\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432  0.15536    \r\nSize           104.438      9.424  11.082  < 2e-16 ***\r\nNew         -78527.502  51007.642  -1.540  0.12697    \r\nSize:New        61.916     21.686   2.855  0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nE. Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nggplot(data = house.selling.price, aes(x = Size, y = Price, color = New)) +\r\n  geom_point()+\r\n    geom_smooth(method=\"lm\", se = F)\r\n\r\n\r\n\r\n\r\nF. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nBNew3000<--22228+104*3000-78528+62*3000*1\r\nBNew3000\r\n\r\n\r\n[1] 397244\r\n\r\nBNotNew3000<--22228+104*3000\r\nBNotNew3000\r\n\r\n\r\n[1] 289772\r\n\r\nIf this home is new, the predicted selling price is 397,244\r\nIf this home is not new, the predicted selling price is 289,772\r\nG. Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\n\r\n\r\nBNew1500<--22228+104*1500-78528+62*1500\r\nBNew1500\r\n\r\n\r\n[1] 148244\r\n\r\nBNotNew1500<--22228+104*1500\r\nBNotNew1500\r\n\r\n\r\n[1] 133772\r\n\r\nIf this home is new, the predicted selling price is 148,244\r\nIf this home is not new, the predicted selling price is 133,772\r\nIt makes sense that a house half the size of a 3000 sq ft house would be significantly less expensive. It is interesting that the difference in price between a new vs not new house is much greater for larger houses than smaller houses.\r\nH. Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\r\nUnlike the example that we reviewed in class with classic cars, for house prices the model without interaction better represents the relationship of size to price. Both new and not new houses are more expensive if they are larger. However larger houses are much more expensive if new than if not new.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomektracy884427/distill-preview.png",
    "last_modified": "2022-04-03T21:42:31-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomegeeslindacss-603-hw3/",
    "title": "HW3_DACSS603",
    "description": "DACSS 603 Homework 3",
    "author": [
      {
        "name": "Eliza Geeslin",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nQuestion 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 +2.84x2\n(A) A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\n# the predicted selling price\n\n -10536 + (53.8*1240) + (2.84*18000)\n\n\n[1] 107296\n\nBased on this model, we would predict the selling price of the home to be $107,296. However, the actual selling price was $145,000. So, the residual is 145000 - 107296 = 37704. This means that the actual price is $37,704 greater than the predicted price. One reason this could happen is because there may be other variables that affect the selling price that are unexplained by the model. Perhaps whether a house is new is another factor that could be incorporated in, or perhaps there is an interaction between these two variables.\n(B) For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nWe can figure this out by holding the lot size constant. Looking at the formula (ŷ = −10,536 + 53.8x1 + 2.84x) we can see that the for a fixed lot size we see that the house selling price is predicted to increase by $53.8 for each sq. foot increase in home size.\n(C) According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nAgain, looking at the formula, for a fixed lot size we see that the house selling price is predicted to increase by ~$3 for each sq. food increase in lot size. So you would have to increase by ~19 sq feet (53.8/2.84 = 18.94) in order to have the same impact as a one-square-foot increase in home size.\nQuestion 2\n(ALR, 5.17, slightly modified)\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n(A) Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\ndata(\"salary\")\n\n#box plot\nboxplot(salary~sex, salary)\n\n\n\nsummary_salary <- salary %>%\n  group_by(sex) %>%\n  summarise(average_salary = mean(salary), min_salary = min(salary), max_salary = max(salary))\n\n#summary plot\nggplot(summary_salary) +   \n  geom_point(aes(x = sex, y = average_salary), color = \"#FF5C35\", size = 4) +\n  geom_errorbar(aes(x = sex, ymin = min_salary, ymax = max_salary), color = \"#FF5C35\", width = 0.5) +\n  labs(x = \"Sex\",  y = \"Salary\") +\n  geom_text(aes(x = sex, y = max_salary, label = max_salary), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -3) +\n  geom_text(aes(x = sex, y = min_salary, label = min_salary), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -3) +\n  geom_text(aes(x = sex, y = average_salary, label = round(average_salary, digits = 2)), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -0.5) +\n  theme(axis.text.x = element_text(family = \"Avenir\", color = \"#33475b\", size=10),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#33475b\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#33475b\", size=13),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#33475b\", size=13))\n\n\n\n\nWithout regard to any other variable, we see that the mean salary for men and women are not the same. From the visuals alone, we don’t know if this is significant. The mean salary for men is higher. However, it also seems like there is an Woman who makes the most (so there is a bigger range for women). From this visual alone, we don’t know if this is significant.\n\n\n# t.test to text the null hypothesis that men and women make the same amount. \n\nt.test(salary~sex, data = salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\nEven though men have a mean salary that is higher than women, the p-value (.09) is higher than a significance level of 5%. Therefore, we cannot reject the null hypothesis that the mean salary for men and women are the same.\n(B) Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\nI used the lm() function to run a linear regression.\n\n\nsummary(lm(salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nNext, I used ’confint()` to find the 95% confidence interval of the means for all variables.\n\n\nconfint(lm(salary ~ ., data = salary))\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\nFrom this we see that at a 95% confidence interval of the mean for the difference in salary between males and females, the lower bound -$697.82 and the upperbound is $3,030.56. So, 95% of the time males will make between $697.82 more and $3,030.56 less than women.\n(C) Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nSalary and degreePhD - from the regression we see that if an employee has a PhD with all other variables holding constant, we would predict that their salary would be $1,388.61 higher, or a positive slope. However, the p-value of this effect is 0.18, so it is not significant at the 0.05 level (which means it is a weak predictor).\nSalary and rankAssoc - we can see that if an employee has a rank of Associate we’d predict their salary would be $5292.36 higher (baseline is rankAsst), all other variables holdign constant. Again, this indicates a positive slope. The p value for rankAssoc is 3.22e-05 and statistically significant at the 0.00l level, so there is a strong effect.\nSalary and rankProf - we can see that if an employee has a rank of Professor, holding all other variables constant, we’d predict salary to be $11,118.76 higher (baseline is rankAsst), or a positive slope. The p value for rankAssoc is 3.22e-05 and statistically significant at the 0.00l level, so there is a strong effect.\nSalary and sexFemale - from the regression output we can see that if an employee is female and every other variable is held constant, we’d predict that their salary would increase by $1,166.37 (a positive slope). However, the p value for if your sex is female is .21 and statistically insignificant at the p = 0.05 level.\nSalary and year - We see that, holding all else constant, we predict that employees will see an increase of $476.31 per year. Thus we would see a positive slope. The p-value for year is 8.65e-06 and statistically significant at the 0.001 level.\nSalary and ysdeg - We can see that, holding all else constant, each incremental year since degree reduces salary by $124.57, a negative slope. However, the p value for ysdeg is .12 and statistically insignificant at p = 0.05.\nThe strongest predictors of salary are rank and years in rank. The adjusted R-squared value suggests that the coefficients explains 83.57% of the salary differences.\n(D) Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\nI changed the baseline category for the rank variable from Assistant to Professor.\n\n\n#salary$professor <- ifelse(salary$rank == 'Prof', 1, 0)\nsalary$rank <- relevel(salary$rank, ref = 'Prof')\nsummary(lm(salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nSalary and rankAsst - we can see that if an employee has a rank of Assistant we’d predict their salary would be $11118.76 lower than the baseline category of Professor. This indicates a negative slope. The p-value for rankAsst with the baseline of professor is 1.62e-10 and statistically significant at the 0.00l level, so there is a strong effect.\nSalary and rankAssoc - we can see that if an employee has a rank of Associate, holding all other variables constant, we’d predict salary to be $5826.40 lower than the baseline category of Professor. Here the p-value for rankAssoc is 7.28e-07 and statistically significant at the 0.00l level, so there is a strong effect.\n(E) Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\n#exclude the variable rank\n\nsummary(lm(salary ~ . -rank, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ . - rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\nIgnoring rank:\nSalary and degreePhD - We see that if an employee had a PhD, we’d predict that their salary would be $3299.35 lower (or a negative slope). This effect has a p-value of 0.014704 so it is significant at the 0.05 level.\nSalary and sexFemale - We see that if an employee is female, we’d predict their salary would be $1286.54 lower, or a negative slope. This effect has a p-value of 0.332209, so it is not statistically significant at the 0.5 level.\nSalary and year - We see that the model would predict that an employee’s salary would go up by $351.97 for every year at that rank (or a positive slope). This is significant at the 0.05 level.\nSalary and ysdeg - We see that the model would predict that an employee’s salary would go up by $339.40 for every year since their degree (or a positive slope).This is significant at the 0.001 level.\nNote: if rank is tainted by Sex, we could argue that other variables in this data set are tainted as well, so using data like these to resolve issues of discrimination will never satisfy everyone.\n(F) Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\nIn order to test this hypothesis I created a dummy variable (newdean). If ysdeg is 15 years or less it will code as 1. Otherwise it will be coded as 0.\nMulticollinearity could be a concern if two variables are strongly correlated. In order to avoid this I removed ysdegree because of the overlap with the new dummy variable (the dummy variable was based on ysdeg). [ELIZA TAKE ANOTHER LOOK HERE]\n\n\n#dummy variable\n\nsalary$newdean <- ifelse(salary$ysdeg <= 15, 1, 0)\n\nsummary(lm(salary ~ . - ysdeg, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ . - ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24425.32    1107.52  22.054  < 2e-16 ***\ndegreePhD      818.93     797.48   1.027   0.3100    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nyear           434.85      78.89   5.512 1.65e-06 ***\nnewdean       2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\nBased on this regression, it appears that employees hired uder the new dean are predicted to make $2163.46 more than employees hired before the new dean. However, this correlation is only significant at the 0.05 level - so it is significant, but it is not the strongest predictor of salary.\nQuestion 3\n(SMSS 13.7 & 13.8 combined, modified)\n(Data file: house.selling.price in smss R package)\n(A) Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of Size of home (in square feet) and whether the home is New (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\nI downloaded the data and then I ran a linear regression using the lm() function.\n\n\n#downloading the data\ndata(\"house.selling.price\")\n\n# price is the outcome variable and size and new are the explanatory variables\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\nOverall we can see that the coefficient explains 71.7% of the housing prices. We also see that both Size and New p-values are less than .05:\nWe reject the null hypothesis that there is no correlation between Size and selling price of the home.\nWe reject the null hypothesis that there is no correlation between New and selling price of the home.\nBoth variables are statically significant to the selling price of the home.\n(B) Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\nBased the above regression model, the prediction equation for the price of a home would be yhat = -40230.867 + 116.132x + 57736.283z where x = the size of the home and z = 1 if the home is new or z = 0 if the home is not new.\nFor new homes, z = 1 so the prediction equation for the selling price of a new home is yhat = -40230.867 + 116.132x + 57736.283 or yhat = 17505.416 + 116.132x\nFor not new homes, z = 0 so the prediction equation for the selling price of a not-new home is yhat = -40230.867 + 116.132x + 0 or yhat = -40230.867 + 116.132x\nWe see in the regression results and the formulas that Size and New both have a positive effect on the selling price of a home. For both new and old homes, every 1 sq foot increase in Size we predict an increase in the selling price of a home by ~$116. For houses of all sizes, where a home is new (z = 1), we predict a new home to be ~$57,736 more expensive. In this model, the impact of each variable is separate: there is no interaction. We see that both variables have small p-values; Size is significant at the 0.001 level and New is significant at the 0.01 level.\n(C) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n#predicted selling price for a new home\n\n17505.416 + (116.132*3000)\n\n\n[1] 365901.4\n\n\n\n#predicted selling price for a not new home\n\n-40230.867 + (116.132*3000)\n\n\n[1] 308165.1\n\nBased on the predictive formulas from the regression model, the predicted selling price for a new home that is 3000 sq. feet is $365,901 and the predicted selling price for a not new home of the same size is $308,165.\n(D) Fit another model, this time with an interaction term allowing interaction between Size and New, and report the regression results\n\n\n# price is the outcome variable and size and new are the explanatory variables - allowing interaction between size and new\nsummary(lm(Price ~ Size + New + Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\nWe now see that the coefficients explain 73.6% of the pricing differences. This is increased from just using size and new as variables.\nSelling Price and Size:New - We see the coefficient is $61.92 (a positive slope). The p-value for Size:New is 0.00527, so it is significant at the 0.01 level. We can reject the null hypothesis that there is no relationship between Size:New and selling price of the home.\nSelling Price and New - Now that we are including the interaction variable, New on it’s own is insignificant, as the p-value when from 0.003 to 0.12697.\n(E) Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nBased the above regression model, the prediction equation for the price of a home would be yhat = -22227.808 + 104.438x + -78527.502z + 601.916xz where x = the size of the home and z = 1 if the home is new or z = 0 if the home is not new.\nFor new homes, z = 1 so the prediction equation for the selling price of a new home is yhat = -22227.808 + 104.438x + -78527.502 + 61.916xz\nFor not new homes, z = 0 so the prediction equation for the selling price of a not-new home is yhat = -22227.808 + 104.438x + 0 + 0 or yhat = -22227.808 + 104.438x\n(F) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n#predicted selling price for a new home\n#x = size (3000)\n#z = new/not new (1)\n#yhat = -22227.808 + 104.438x + -78527.502z + 601.916xz\n\n-22227.808 + (104.438*3000) + (-78527.502*1) + (61.916*3000*1)\n\n\n[1] 398306.7\n\n\n\n#predicted selling price for a not new home\n\n-22227.808 + (104.438*3000) + (-78527.502*0) + (61.916*3000*0)\n\n\n[1] 291086.2\n\nBased on the predictive formulas from the regression model, the predicted selling price for a new home that is 3000 sq. feet is $398,306.70 and the predicted selling price for a not new home of the same size is $291,086.20.\n(G) Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\n#predicted selling price for a new home\n# yhat = -22227.808 + 104.438x + -78527.502z + 601.916xz\n\n-22227.808 + (104.438*1500) + (-78527.502*1) + (61.916*1500*1)\n\n\n[1] 148775.7\n\n\n\n#predicted selling price for a not new home\n# yhat = -22227.808 + 104.438x + -78527.502z + 601.916xz\n\n-22227.808 + (104.438*1500) + (-78527.502*0) + (61.916*1500*0)\n\n\n[1] 134429.2\n\nBased on the predictive formulas from the regression model, the predicted selling price for a new home that is 1500 sq. feet is $148,775.7 and the predicted selling price for a not new home of the same size is $134,429.20. For new homes, each additional sq. foot adds ~$706 (104.438 + 601.916) to the predicted selling price and for not new homes, each sq. food adds ~$104 in predicted selling price.\n(H) Do you think the model with interaction or the one without it represents the relationship of Size and New to the outcome price? What makes you prefer one model over another?\nI think the model that allows for interactions does a better job of representing the relationship between Size and New to the selling price. When comparing the two models we see that adjusted R-squared is greater for the model that allows the interaction, which suggests that the coefficients explain slightly more of the interaction between size, new and selling price. Additionally, we see that the created variable Size:New is statistically significant, so we can reject the null hypothesis that Size:New is not correlated to selling price.\n\n\n\n",
    "preview": "posts/httpsrpubscomegeeslindacss-603-hw3/distill-preview.png",
    "last_modified": "2022-04-03T21:42:35-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomemersonflemi884749/",
    "title": "EFleming DACSS 603 HW III",
    "description": "The following document contains my answers for DACSS 603 HW III.",
    "author": [
      {
        "name": "Emerson Fleming",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nQuestion 1\n(SMSS 11.2, except part (d))\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\\[\nFl_house_predicted =  −10,536 + 53.8*1240 + 2.84*18000\n\\]\n\n[1] 107296\n[1] 37704\n\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nEach coefficient of x2 is 53.8 in the regression equation. Therefore, ME of x2 is 53.8. In other words, each square-foot increase in home size will lead to a $53.8 higher price.\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nFor each square-foot increase in lot size, the selling price of the house increases by 2.84. Therefore, we need 53.8/2.84=18.9 sqft increase in lot size to create a 53.8 increase in selling price in dollars.\nQuestion 2\n(ALR, 5.17, slightly modified)\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\n[1] -0.2527824\n\n\n    Male   Female \n24696.79 21357.14 \n\nAs we can see, a weak negative correlation exists between gender and salary. Also, the difference in average salaries for male and female appears to be around $3,000. this correlation is not strong enough for us to reject the null hypothesis. In fact, we must fail to reject the null hypothesis as the p-value is larger than 0.05\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary[, c(-2, -9)])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \nProf        11118.76    1351.77   8.225 1.62e-10 ***\nAssoc        5292.36    1145.40   4.621 3.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\nWe can say we are 95% sure average salary of Females minus Males falls within -697.8183 and 3030.565. We use Females minus Males as R used dummy variables so we must compensate. You normally want the confidence interval to be outside of “0.” Here, “0” is included. This is not suprising as the p-value does not give a statistically significant result (the p-value is too large).\nInterpret your finding for each predictor variable; (i) discuss statistical significance “DegreePhd,” “ysdeg,” and “sexFemale” are not statistically significant as denoted by the p-value. Rank (which was converted into a numeric variable as academic rank as an ordinal variable) and “year” are all statistically significant as they have p-values close to zero.\ninterpretation of the coefficient / slope in relation to the outcome variable and other variables On average, Professors are earning $11118.76 more than Assistant Professors, while Associate Professors are earning 5292.36 dollars more than assistant professors. On average, one extra year in current rank leads to a $476.31 higher salary.\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCall:\nlm(formula = salary ~ ., data = salary[, c(-2, -7)])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \nAssoc        -5826.40    1012.93  -5.752 7.28e-07 ***\nAsst        -11118.76    1351.77  -8.225 1.62e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nThe baseline is the category on summary() that you do not see. When we change the baseline from Asst. Prof to Prof, we get the same exact number but with different signs on the correlations. This is because we are comparing Professors from all ranks that precede the Professor ranking. For this new correlation with a new baseline, we are given opposite signs. For example, an assistant professor will earn 11118.76 less than a Professor. Additionally, an Associate Professor earns -5826.40 less than a Professor.\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. (i)Exclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\nThe coefficient of the sex variable is still not statistically significant which indicates no sexual gender discrimination. However, the ysdef variable did become statistically significant.\n\n[1] 0.6957238\n\nThis result ysdeg is capturing (at least partially if not fully) the effect of rank versus as people with more years of experience after the highest degree also have higher academic ranks. The correlation between rank and ysdeg(0.696) also supports this argument.\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCall:\nlm(formula = salary ~ degree + ysdeg + sex + newhire, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8332.6 -2605.8  -865.1  2966.5 11030.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18233.70    2796.72   6.520 4.38e-08 ***\ndegreePhD   -4246.07    1415.78  -2.999 0.004320 ** \nysdeg         479.68     119.99   3.998 0.000224 ***\nsexFemale   -2731.90    1250.83  -2.184 0.033982 *  \nnewhire        78.15    2194.61   0.036 0.971743    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3979 on 47 degrees of freedom\nMultiple R-squared:  0.5833,    Adjusted R-squared:  0.5478 \nF-statistic: 16.45 on 4 and 47 DF,  p-value: 1.713e-08\n\nHere, we have created all new dummy variables that will serve as our means of analyzing a correlation between the new dean and offers being made. In order to avoid multicollinearity, we will remove variables with very high levels of correlation from our original model which would include rank and year.\nThe high p-value of salary with regards to “newhire” indicates we cannot claim new hires by the new dean are earning any higher salary than those hired by the previous dean.\n\n[1] -0.8434239\n[1] -0.7206038\n[1] 0\n\nThe correlation between new dummy var and ysdeg is very high and can cause an issue with reagrds to multicollinearity.\nI propose to simply drop ysdeg and run the regression with the degree, sex, year and newhire variables only. Additionally, we drop ysdeg and add rank dummies since the correlation between newhire and rank is a bit lower than the correlation between newhire and ysdeg. Rank can therefore proxy ysdeg.\n\n\nCall:\nlm(formula = salary ~ degree + year + sex + newhire + Prof + \n    Assoc, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13328.38    1483.38   8.985 1.33e-11 ***\ndegreePhD     818.93     797.48   1.027   0.3100    \nyear          434.85      78.89   5.512 1.65e-06 ***\nsexFemale     907.14     840.54   1.079   0.2862    \nnewhire      2163.46    1072.04   2.018   0.0496 *  \nProf        11096.95    1191.00   9.317 4.54e-12 ***\nAssoc        4972.66     997.17   4.987 9.61e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\nCorrelation between new dummy variable and ysdeg is very high. Multicollinearity inflates standard errors and thus makes us fail to reject more eften (higher p-values). Here, those hired by the new dean gain higher salaries by an average of $2163.46 compared to the old dean.\nQuestion 3\n(SMSS 13.7 & 13.8 combined, modified)\n(Data file: house.selling.price in smss R package)\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\nprice = -40230,867 + 116.132 * size + 57736.283*New -> For whole sample\nHouses with 1 more sqft in size on average sell at a $116 higher price. New homes on average are 57736 (dollars) more expensive than not new homes.\nprice = -40230,867 + 116.132 * size -> For not new homes only price = -40230,867 + 116.132 * size + 57736.283* 1 = 17505.42 + 116.32 * size -> For whole sample\nThere appears to be a very high correlation between price and size of the house and a high correlation between whether the house is new and price as denoted by the p-values. Overall, there is a larger correlation between price and size of the home as opposed to whether it is new. All coefficients are statistically significant and under 5%.\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n\n\n\n\n\n\n\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\nWhen an interaction is implemented between size and new is allowed, we end up with the same very high correlation between Price and Size. However, we obtain an even greater correlation between New and Price. We also gain a larger adjusted r-squared model–which indicates our new model is better and more representative than before.\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nPrice = -22227.808 + 104.438 * Size - 78527.502 * 1 + 61.916 * Size * 1 Price = -100755.3 + 166.354 * Size\nPrice = -22227.808 + 104.438 * Size - 78527.502 * 0 + 61.916 * Size * 0 Price = -22227.808 + 104.438 * Size (Old homes)\nFind the predicted selling price for a home of 3000 square feet that is\nnew, (ii) not new.\n\n       1        2 \n398307.5 291087.4 \n\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n       1        2 \n148776.1 134429.8 \n\n1500 sqft bigger size corresponds to 398307-148776.1=249531.4 and 291087.4-134429.8=156657.6 higher price for new and hold homes respectively.\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nI prefer the second model as the coefficient of interaction var is statistically significant and adjusted r^2 goes up when we add the interaction.\n\n\n\n",
    "preview": "posts/httprpubscomemersonflemi884749/distill-preview.png",
    "last_modified": "2022-04-03T21:42:39-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomclairebattagliahomework-3-603/",
    "title": "Homework 3",
    "description": "Multiple Linear Regression",
    "author": [
      {
        "name": "Claire Battaglia",
        "url": "https://rpubs.com/clairebattaglia"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\n\nContents\nQuestion 1\nAnswer\nSolution\n\nQuestion 2\nA\nB\nC\nD\nE\nF\n\nQuestion 3\nA\nB\nC\nD\nE\nF\nG\nH\n\n\nQuestion 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nAnswer\nThe predicted selling price is $107,296 and the residual is $37,704. This indicates that the house sold for $37,704 more than was predicted by the equation.\nControlling for lot size, we predict a $53.80 increase in selling price for every one-square-foot increase in size of home. In multiple regression, the coefficient/slope of each predictor variable describes the effect of that variable when all other variables are held constant (i.e. controlled for).\nA 18.94 square-foot increase in lot size would predict an increase in selling price equivalent to a one-square-foot increase in home size ($53.80).\nSolution\nThe prediction equation is\n\\[{selling\\;price}={-10,536}+{53.8}*{size\\;of\\;home}+{2.84}*{lot\\;size}\\] To calculate the predicted selling price, I’ll substitute the given values for the variables size of home and lot size.\n\n\nShow code\n\n# assign variables\na <- -10536\nb1 <- 53.8\nb2 <- 2.84\nx1 <- 1240\nx2 <- 18000\nactualP <- 145000\n\n# calculate predicted price\npredP <- a + (b1 * x1) + (b2 * x2)\n\n# view predicted price\npredP\n\n\n[1] 107296\n\nShow code\n\n# calculate residual\nresidual <- actualP - predP\n\n# view residual\nresidual\n\n\n[1] 37704\n\nA one-square-foot increase in home size predicts a $53.80 increase in selling price. To calculate the square-foot increase in lot size that would predict the same increase in selling price, I’ll use the following simple equation.\n\\[2.84*x2=53.80\\] or\n\\[x2=\\frac{53.80}{2.84}\\]\n\n\nShow code\n\n# calculate lot size increase\nlotInc <- b1/b2\n\n# view lot size increase\nlotInc\n\n\n[1] 18.94366\n\nQuestion 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “a variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary. Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\nA\nFirst I’ll inspect the data.\n\n\nShow code\n\n# load data\ndata(\"salary\")\n\n# create object\nsalary <- salary\n\n# get summary\nsummary(salary)\n\n\n     degree      rank        sex          year            ysdeg      \n Masters:34   Asst :18   Male  :38   Min.   : 0.000   Min.   : 1.00  \n PhD    :18   Assoc:14   Female:14   1st Qu.: 3.000   1st Qu.: 6.75  \n              Prof :20               Median : 7.000   Median :15.50  \n                                     Mean   : 7.481   Mean   :16.12  \n                                     3rd Qu.:11.000   3rd Qu.:23.25  \n                                     Max.   :25.000   Max.   :35.00  \n     salary     \n Min.   :15000  \n 1st Qu.:18247  \n Median :23719  \n Mean   :23798  \n 3rd Qu.:27258  \n Max.   :38045  \n\nThere are five predictor variables, three of which are categorical (degree, rank, sex) and two of which are quantitative (year, which refers to years in current rank, and ysdeg, which refers to years since highest degree earned).\nThe null hypothesis is that the mean salary for males and females is the same. That is, sex has no effect on salary. The alternative hypothesis, then, is that the mean salary for males and females is not the same.\nThus \\[H_0:\\mu_1=\\mu_2\\] and \\[H_a:\\mu_1\\ne\\mu_2\\]\nI’ll use the t.test() function to conduct a two-sample t-test.\n\n\nShow code\n\n# conduct t test\nt.test(salary ~ sex, data = salary, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  salary by sex\nt = 1.8474, df = 50, p-value = 0.0706\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -291.257 6970.550\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\nWhile we can see that the mean salary for males ($24,696.79) is different from that of females ($21,357.14), a \\(P\\)-value of .071 tells us that this difference wouldn’t be so unlikely if the null hypothesis were true that we are forced to reject it. We are thus unable to reject the null hypothesis at this time.\nB\nNow I’ll fit a multiple regression model and get a 95% confidence interval for the difference in mean salaries.\n\n\nShow code\n\n# fit model\nfitS <- lm(salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\n# get summary\nsummary(fitS)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678 < 0.0000000000000002 ***\ndegreePhD    1388.61    1018.75   1.363                0.180    \nrankAssoc    5292.36    1145.40   4.621       0.000032163431 ***\nrankProf    11118.76    1351.77   8.225       0.000000000162 ***\nsexFemale    1166.37     925.57   1.260                0.214    \nyear          476.31      94.91   5.018       0.000008653790 ***\nysdeg        -124.57      77.49  -1.608                0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 0.00000000000000022\n\nShow code\n\n# get CI\nconfint(fitS, \"sexFemale\", level = .95)\n\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\nThe baseline category for sex is male, which we know because sexFemale is included in the above regression output. The coefficient for sexFemale tells us that, controlling for all other variables, a female professor is predicted to earn an average of $1,166.37 more than a male professor.\nThe 95% confidence interval is -697.8183 to 3030.565. This tells us that a female professor is predicted to earn anywhere from $697.82 less than a male professor to $3,030.57 more than a male professor. Because the confidence interval includes zero, it’s possible that male professors earn more than female professors, female professors earn more than male professors, or male and female professors earn the same as each other.\nWe are thus unable to reject the null hypothesis at this time. This corresponds with the conclusion reached based on the \\(P\\)-value.\nC\nWe can see the coefficient/slope of each predictor variable by looking at the Estimate column in the summary of our lm() output (above). The coefficient/slope of each predictor variable indicates the mean expected increase in our response variable (salary) for each unit increase in that predictor variable if all other variables are held constant. The Std. Error column gives the standard error of the estimate for each predictor variable, the t value column gives the \\(t\\) statistic and the column Pr(>|t|) gives the \\(P\\)-value.\nLet’s look at each predictor variable individually.\nDegree: A one-unit increase in degree (i.e. moving from an MS to PhD) is estimated to yield a $1,388.61 increase in salary. However, the \\(P\\)-value is .180, which is not statistically significant. What this really means is that we can’t be sure that the coefficient/slope of this predictor variable isn’t zero.\nRank: Because this variable is a factor with 3 levels, the output is laid out a bit differently. We can interpret it as follows: moving from an Assistant Prof. to an Associate Prof. is estimated to yield a $5,292.36 increase in salary while moving from an Assistant Prof. to a Full Prof. is estimated to yield a $11,118.76 increase in salary. The \\(P\\)-value for both coefficients/slopes are statistically significant, meaning we can be fairly certain the coefficient/slope of this variable is not zero.\nSex: Being a female is estimated to yield a $1,166.37 increase in salary. However, the \\(P\\)-value is not statistically significant.\nYear: Each additional year in one’s current rank is estimated to yield a $476.31 increase in salary. The \\(P\\)-value is statistically significant.\nYsdeg: Each year that’s passed since one’s highest degree earned is estimated to yield a $124.57 decrease in salary. However, the \\(P\\)-value is not statistically significant.\nLooking at the predictor variables relative to one another, it appears that one’s rank and number of years in that rank are the most significant predictors of salary. The high \\(P\\)-values for every other predictor variable tell us that we cannot be sure that the effect of those variables is not zero.\nD\nWe can see from the summary of our lm() output (above) that the baseline category of the rank variable is Assistant Prof. because it’s not included in the output. R automatically drops the baseline (or reference) category to avoid multicollinearity. We can use the levels() function to retrieve factor levels of a particular variable and the relevels() function to manually assign the factor levels of a particular variable.\nHere I’ll change the baseline category from Assistant Prof. to Full Prof.\n\n\nShow code\n\n# get factor levels\nlevels(salary$rank)\n\n\n[1] \"Asst\"  \"Assoc\" \"Prof\" \n\nShow code\n\n# change baseline rank\nrankNew <- relevel(salary$rank, \"Prof\")\n\n# fit model again\nfitS2 <- lm(salary ~ degree + rankNew + sex + year + ysdeg, data = salary)\n\n# get summary\nsummary(fitS2)\n\n\n\nCall:\nlm(formula = salary ~ degree + rankNew + sex + year + ysdeg, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)   26864.81    1375.29  19.534 < 0.0000000000000002 ***\ndegreePhD      1388.61    1018.75   1.363                0.180    \nrankNewAsst  -11118.76    1351.77  -8.225       0.000000000162 ***\nrankNewAssoc  -5826.40    1012.93  -5.752       0.000000727809 ***\nsexFemale      1166.37     925.57   1.260                0.214    \nyear            476.31      94.91   5.018       0.000008653790 ***\nysdeg          -124.57      77.49  -1.608                0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 0.00000000000000022\n\nTwo things to note:\nThe baseline category of our rank variable has changed. We can see that the new baseline is Full Prof. because it’s been dropped from the regression output.\nThe sign of the coefficient/slope of the rank predictor variable has changed. If one were to move from full professorship to associate professorship, we would predict a decrease in salary. Likewise if one were to move from full professorship to assistant professorship. The actual values haven’t changed, however, because the relationship hasn’t changed, merely the direction. It’s worth noting that while this regression output is mathematically correct, it’s not the most meaningful way to structure our model. Professors generally move up in rank and rarely (if ever) move back down in rank so the original factor levels make the most sense.\nE\nWe can also exclude particular variables from a regression model. Here I’ll exclude rank and see what happens.\n\n\nShow code\n\n# fit model w/o rank\nfitS3 <- lm(salary ~ . - rank, data = salary)\n\n# get summary\nsummary(fitS3)\n\n\n\nCall:\nlm(formula = salary ~ . - rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969 < 0.0000000000000002 ***\ndegreePhD   -3299.35    1302.52  -2.533             0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980             0.332209    \nyear          351.97     142.48   2.470             0.017185 *  \nysdeg         339.40      80.62   4.210             0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 0.000000001048\n\nIgnoring rank altogether appears to change the model rather dramatically. The effects of having a PhD, being female, and increasing number of years since highest degree attained have reversed (i.e. the sign of the coefficient/slope has changed). In this model, the variables with the highest degree of statistical significance are the number of years since highest degree attained, number of years in current rank, and having a PhD.\nPlaced in the context of an investigation of sex discrimination, I’m not sure we could justify ignoring rank. Discrimination of any kind could easily manifest itself in the promotion of one group over another, indicating that we should control for it (i.e. include it in our regression model) instead of ignoring it.\nF\nIf the claim is that which dean one was hired by is a predictor of salary, a logical new variable would be the binary variable dean, with factor levels “new” and “old.” I’ll use the mutate() function to create this new variable, assigning each professor a category based on when he/she was hired. Those hired in the past fifteen years will be assigned to the new dean and those hired prior (over fifteen years ago) to the old dean.\n\n\nShow code\n\n# create new variable\nsalaryD <- salary %>%\n  mutate(dean = case_when(\n    ysdeg >= 1 & ysdeg <= 15 ~ \"new\",\n    ysdeg >= 16 ~ \"old\"\n  ))\n\n# view to check\nhead(salaryD, 20)\n\n\n    degree  rank    sex year ysdeg salary dean\n1  Masters  Prof   Male   25    35  36350  old\n2  Masters  Prof   Male   13    22  35350  old\n3  Masters  Prof   Male   10    23  28200  old\n4  Masters  Prof Female    7    27  26775  old\n5      PhD  Prof   Male   19    30  33696  old\n6  Masters  Prof   Male   16    21  28516  old\n7      PhD  Prof Female    0    32  24900  old\n8  Masters  Prof   Male   16    18  31909  old\n9      PhD  Prof   Male   13    30  31850  old\n10     PhD  Prof   Male   13    31  32850  old\n11 Masters  Prof   Male   12    22  27025  old\n12 Masters Assoc   Male   15    19  24750  old\n13 Masters  Prof   Male    9    17  28200  old\n14     PhD Assoc   Male    9    27  23712  old\n15 Masters  Prof   Male    9    24  25748  old\n16 Masters  Prof   Male    7    15  29342  new\n17 Masters  Prof   Male   13    20  31114  old\n18     PhD Assoc   Male   11    14  24742  new\n19     PhD Assoc   Male   10    15  22906  new\n20     PhD  Prof   Male    6    21  24450  old\n\nNow I can fit a new model that includes this new variable.\n\n\nShow code\n\n# fit model w/ dean\nfitS4 <- lm(salary ~ . , data = salaryD)\n\n# get summary\nsummary(fitS4)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salaryD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3621.2 -1336.8  -271.6   530.1  9247.6 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 15516.79     814.81  19.043 < 0.0000000000000002 ***\ndegreePhD    1135.00    1031.16   1.101                0.277    \nrankAssoc    5234.01    1138.47   4.597       0.000035985932 ***\nrankProf    11411.45    1362.02   8.378       0.000000000116 ***\nsexFemale    1084.09     921.49   1.176                0.246    \nyear          460.35      95.09   4.841       0.000016263785 ***\nysdeg         -47.86      97.71  -0.490                0.627    \ndeanold     -1749.09    1372.83  -1.274                0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2382 on 44 degrees of freedom\nMultiple R-squared:  0.8602,    Adjusted R-squared:  0.838 \nF-statistic: 38.68 on 7 and 44 DF,  p-value: < 0.00000000000000022\n\nWe can see from the regression output that the baseline category is those hired by the new dean. Based on this model, we expect those hired by the old dean to earn an average of $1,749.09 less than those hired by the new dean. This finding is not statistically significant, however, so based on this model we would not be able to reject the null hypothesis at this time.\nIt is possible that there is some multicollinearity contained within this model. Multicollinearity means that some or all of the predictor variables in a multiple regression model are highly correlated with one another. One indication of multicollinearity is the \\(R^2\\) values of our models. In our first model, \\(R^2\\)=.855, and in this newest one, \\(R^2\\)=.860. This slight increase is an indication that the new variable adds little predictive power to the model.\nIt’s also worth noting that larger sample sizes can help mitigate the problems posed by multicollinearity. A general rule is that a dataset should have 10x as many observations as predictor variables contained within the model. According to this guideline, we would ideally have 60 observations for our new model with 6 variables. Since we have only 52, we may be entering into the territory of a too-small sample size.\nGiven the potential for multicollinearity, I’ll fit a new model removing the variable ysdeg and compare the two.\n\n\nShow code\n\n# fit model w/ dean w/o ysdeg\nfitS5 <- lm(salary ~ . - ysdeg, data = salaryD)\n\n# get summary\nsummary(fitS5)\n\n\n\nCall:\nlm(formula = salary ~ . - ysdeg, data = salaryD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 15491.84     806.32  19.213 < 0.0000000000000002 ***\ndegreePhD     818.93     797.48   1.027               0.3100    \nrankAssoc    4972.66     997.17   4.987     0.00000961362451 ***\nrankProf    11096.95    1191.00   9.317     0.00000000000454 ***\nsexFemale     907.14     840.54   1.079               0.2862    \nyear          434.85      78.89   5.512     0.00000164625970 ***\ndeanold     -2163.46    1072.04  -2.018               0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 0.00000000000000022\n\nWe can see that the adjusted \\(R^2\\) value of this final model is 0.8407, which is higher than the adjusted \\(R^2\\) values for the original model (0.8357) and the model with both ysdeg and dean (0.838). This is a good indication that this final model is the best fit.\nQuestion 3\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nA\nFirst I’ll inspect the data.\n\n\nShow code\n\n# load data\ndata(\"house.selling.price\")\n\n# create object\nhSP <- house.selling.price\n\n# get string\nstr(hSP)\n\n\n'data.frame':   100 obs. of  7 variables:\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\n\nIt’s worth noting that while the variable New is actually a factor variable, R currently understands it as an integer. I’m not sure if that will matter for my analysis but I’ll bear in mind that I may need to change it at some point.\n\n\nShow code\n\n# fit model w/ new and size\nfitP <- lm(Price ~ New + Size, data = hSP)\n\n# get summary\nsummary(fitP)\n\n\n\nCall:\nlm(formula = Price ~ New + Size, data = hSP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738              0.00737 ** \nNew          57736.283  18653.041   3.095              0.00257 ** \nSize           116.132      8.795  13.204 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 0.00000000000000022\n\nThe coefficient/slope for the predictor variable new is 57736.283, indicating that we would expect an average increase in selling price of $57,736.28 if the house is new instead of not new.\nThe coefficient/slope for the predictor variable size is 116.132, indicating that we would expect an average increase in selling price of $116.13 per one-square-foot increase in house size.\nBoth coefficients are statistically significant at the .01 level so we can be confident in saying that the effect of size and of being new or not new is not zero.\nB\nNew is a binary (or dummy) variable where 1 = “new” and 0 = “not new”. Thus our regression equation will look a little different than when we have all quantitative variables. Since new has two factor levels (“new” and “not new”), our equation will have one \\(z\\) term.\nStarting with \\[E(y)=\\alpha+\\beta{x}+\\beta_1{z_1}\\] we get \\[{estimated\\;selling\\;price}=-40,230.867+116.132*size+57,736.283*z\\] where \\(z\\)=1 if the house is new and \\(z\\)=0 if it’s not new.\nIf the house is new then the final term of the prediction equation will be 57,736.283 and the equation can be simplified to \\[{estimated\\;selling\\;price}=17,505.42+116.132*size\\] If the house is not new then the final term of the above equation will be 0 and the equation can be simplified to \\[{estimated\\;selling\\;price}=-40,230.867+116.132*size\\]\nIn both cases, the effect of size remains the same—a one-square-foot increase in size predicts an average increase in price of $116.13 per square foot. Both coefficients/slopes are statistically significant so we can be confident neither are zero. Put more simply, a house that is new will be more expensive than one that is not but for both new and not new houses, bigger houses will likely be more expensive.\nC\nWe can calculate the price of a 3,000-square-foot house that is new as follows \\[{estimated\\;selling\\;price}=17,505.42+116.132*3,000\\]\nand for a house that is not new as follows \\[{estimated\\;selling\\;price}=-40,230.867+116.132*3,000\\]\n\n\nShow code\n\n# assign objects\na <- -40230.867\nb1 <- 116.132\nbZ <- 57736.283\n\n# calculate estimated price if new\npriceNew <- a + (b1*3000) + (bZ*1)\n\n# calculate price if not new\npriceNotNew <- a + (b1*3000) + (bZ*0)\n\n\n\nThus, the estimated price for a 3,000-square-foot house that is new is $365901.42. The estimated price for a 3,000-square-foot house that is not new is $308165.13.\nD\nIn addition to allowing us to include multiple predictor variables, multiple regression allows us to indicate interaction terms by creating cross-product terms. An interaction between two predictor variables exists when the value of one changes the effect of the other on the response variable.\nI’ll now fit a model in which size interacts with whether the house is new or not new.\n\n\nShow code\n\n# fit model w/ size * new interaction\nfitP2 <- lm(Price ~ Size*New, data = hSP)\n\n# get summary\nsummary(fitP2)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = hSP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432              0.15536    \nSize           104.438      9.424  11.082 < 0.0000000000000002 ***\nNew         -78527.502  51007.642  -1.540              0.12697    \nSize:New        61.916     21.686   2.855              0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 0.00000000000000022\n\nLooking at the output of our regression model, it appears that the interaction term \\(size*new\\) is statistically significant, which indicates interaction.\nE\nTo get a sense of whether there is interaction between size and new, we can plot the prediction lines for new houses and those that are not new separately.\n\n\nShow code\n\n# plot by dummy variable\ninteract_plot(fitP2, pred = Size, modx = New, data = hSP, modx.labels = c(\"Not New\", \"New\"))\n\n\n\n\nThis makes it easy to see what the summary of our lm() output (above) tells us: an increase in size predicts an increase in selling price for both houses that are new and those that are not, but being new confers an additional increase in selling price of $61.92 per one-square-foot increase in house size. The larger the house, the larger the difference in price based on being new or not.\nF\nBased on our new regression output, our new prediction equation is \\[{estimated\\;selling\\;price}=-22,227.808+104.438*size-78,527.502*z+61.916*size*z\\] where \\(z\\)=1 if the house is new and \\(z\\)=0 if it’s not.\n\n\nShow code\n\n# assign objects\na <- -22227.808\nb1 <- 104.438\nb2 <- -78527.502\nb3 <- 61.916\n\n# calculate price if new\npriceNew2 <- a + (b1 * 3000) + (b2 * 1) + (b3 * 3000 * 1)\n\n# calculate price if not new\npriceNotNew2 <- a + (b1 * 3000) + (b2 * 0) + (b3 * 3000 * 0)\n\n\n\nThus, when we include the interaction term \\(size*new\\), the estimated price for a 3,000-square-foot house that is new is $398306.69 and the estimated price for a 3,000-square-foot house that is not new is $291086.19.\nG\n\n\nShow code\n\n# calculate price if new\npriceNew3 <- a + (b1 * 1500) + (b2 * 1) + (b3 * 1500 * 1)\n\n# calculate price if not new\npriceNotNew3 <- a + (b1 * 1500) + (b2 * 0) + (b3 * 1500 * 0)\n\n\n\nThe estimated price for a 1,500-square-foot house that is new is $148775.69 and the estimated price for a 1,500-square-foot house that is not new is $134429.19.\nFor a 3,000-square-foot house, the difference in price between one that’s new and one that’s not is $107220.5.\nFor a 1,500-square-food-house, the difference in price between one that’s new and one that’s not is $14346.5.\nThis fits with what’s represented in the above plot. The difference between the lines for new houses and not new houses is relatively small around the 1,500-square-foot mark on the \\(x\\)-axis and much larger around the 3,000-square-foot mark. As the houses get bigger in size, the impact of whether the house is new or not new increases.\nH\nThe model with the interaction between size and new seems more appropriate to me, for a few reasons.\nIf we look at the adjusted \\(R^2\\) values, we can see that the value for the first model (no interaction term) is 0.7169 and for the second (interaction term) is 0.7363, indicating the second explains slightly more variation in selling price than the first.\nThe \\(P\\)-value for the interaction term \\(size*new\\) is statistically significant at the .01 level, indicating that we can be confident that the effect of the interaction is not zero.\nPlotting the two lines indicates that there is interaction between size and new. If there were no interaction, we would expect both lines to be parallel to one another.\nThe sample size is sufficiently large for the number of predictor variables.\n\n\n\n",
    "preview": "posts/httpsrpubscomclairebattagliahomework-3-603/distill-preview.png",
    "last_modified": "2022-04-03T21:42:42-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomjflatteryhomework3/",
    "title": "Homework 3",
    "description": "My HW 3 for 603",
    "author": [
      {
        "name": "Justin Flattery",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\n\nlibrary('smss')\nlibrary('alr4')\nlibrary(smss)\n\n\n\n1 A.\nŷ = −10,536 + 53.8x1 + 2.84x2.\n\n\n-10536 + (53.8*(1240)) + (2.84*(18000))\n\n\n[1] 107296\n\nResidual:\n\n\n145000 - 107296\n\n\n[1] 37704\n\nB.\nFor each square foot increase, there is an increase in price of $53.80. This is because this is the coefficient of the variable associated with size of home.\nC.\n\n\n53.8/2.84\n\n\n[1] 18.94366\n\n2.\nA\n\n\ndata <- salary\n\n\n\nh1 = men h2 = female null => h1 not = h2 alternate h1 = h2\n\n\nt.test(salary ~ sex,data = salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\nB)\n\n\nsummary(lm(salary ~ degree + sex + rank + year + ysdeg, data = salary))$coefficients\n\n\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 15746.0477  800.17827 19.678175 9.759111e-24\ndegreePhD    1388.6133 1018.74688  1.363060 1.796454e-01\nsexFemale    1166.3731  925.56888  1.260169 2.141043e-01\nrankAssoc    5292.3608 1145.39802  4.620543 3.216343e-05\nrankProf    11118.7640 1351.77241  8.225323 1.623713e-10\nyear          476.3090   94.91357  5.018345 8.653790e-06\nysdeg        -124.5743   77.48628 -1.607695 1.148967e-01\n\n\n\nfit_3<-(lm(salary ~ degree + sex + rank + year + ysdeg, data = salary))\n\n\n\n\n\nconfint(fit_3)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nsexFemale    -697.8183  3030.56452\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n95% confidence interval in difference in salarys from female compared to male is [-697,3030]\nC\n\n\nsummary(lm(salary ~ degree + sex + rank + year + ysdeg, data = salary))$coefficients\n\n\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 15746.0477  800.17827 19.678175 9.759111e-24\ndegreePhD    1388.6133 1018.74688  1.363060 1.796454e-01\nsexFemale    1166.3731  925.56888  1.260169 2.141043e-01\nrankAssoc    5292.3608 1145.39802  4.620543 3.216343e-05\nrankProf    11118.7640 1351.77241  8.225323 1.623713e-10\nyear          476.3090   94.91357  5.018345 8.653790e-06\nysdeg        -124.5743   77.48628 -1.607695 1.148967e-01\n\nRank:\nThe rank of a person had the largest influence on salary, with the associate rank and professor rank predicting to influence salary by $5292 and $11,119 respectively compared to the base Both had significance with p values less than 0.05\nDegree The degree of a person had the next largest influence on salary, with the PHD degree influencing by coefficient of 1388 however the significance is only at 10% level as p is greater than 0.05 (p = ~.1)\nSex:\nThe sex of a person had the next largest influence on salary, with being female predicting to influence salary by $1116 compared to the base It did not have much significance with p values ~ 0.2\nYear:\nThe year of a current rank of a person had the next largest influence on salary, with the years predicting to influence salary by $476 for every additional year at rank This had significance with p values less than 0.05\nysdeg:\nThe ysdegree of a person had the lowest influence on salary, with the years in degreee influencing by $-124 It did not have significance at 0.05 level with p values ~.1\nD)\n\n\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\n\n\n\n\n\nsummary(lm(salary ~ degree + sex + rank + year + ysdeg, data = salary))$coefficients\n\n\n               Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)  26864.8117 1375.28806 19.533953 1.313481e-23\ndegreePhD     1388.6133 1018.74688  1.363060 1.796454e-01\nsexFemale     1166.3731  925.56888  1.260169 2.141043e-01\nrankAsst    -11118.7640 1351.77241 -8.225323 1.623713e-10\nrankAssoc    -5826.4032 1012.93301 -5.752012 7.278088e-07\nyear           476.3090   94.91357  5.018345 8.653790e-06\nysdeg         -124.5743   77.48628 -1.607695 1.148967e-01\n\nThe rank variables now change in response, with rank of “Assitant” being -11,118 and Associate being -5826 compared to the baseline of Professor. The significance of both have a p value less than .05\nE)\n\n\nsummary(lm(salary ~ degree + sex + year + ysdeg, data = salary))$coefficients\n\n\n              Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) 17183.5717 1147.94172 14.9690280 1.659232e-19\ndegreePhD   -3299.3488 1302.51952 -2.5330514 1.470396e-02\nsexFemale   -1286.5443 1313.08854 -0.9797849 3.322090e-01\nyear          351.9686  142.48087  2.4702865 1.718541e-02\nysdeg         339.3990   80.62097  4.2098109 1.143695e-04\n\nThis changed the degree of PHD from a positve coefficent to a negative one, in comparison to base of Masters This changed sexFemale from a positive coefficient to a negative one, in comparison to base of Male. This changed ysdeg from a neg coefficient to a positve one, meaning more years in a degree now implies a higher salary This changed year from a coefficient of 476 to one of 352\nF)\n\n\nyear_15<-subset(salary, ysdeg > 15)\n\n\n\n\n\nyear_under_15 <-subset(salary, ysdeg <=15)\n\n\n\n\n\nsummary(lm(salary ~ ysdeg + degree + sex + rank, data = year_15))$coefficients\n\n\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 24446.112  3626.5574  6.740859 1.475017e-06\nysdeg         270.326   161.5251  1.673585 1.097816e-01\ndegreePhD   -2557.494  2091.0838 -1.223047 2.355281e-01\nsexFemale   -1968.769  1778.1339 -1.107210 2.813423e-01\nrankAsst    -8931.117  4127.8491 -2.163625 4.277483e-02\nrankAssoc   -6177.036  1951.1289 -3.165878 4.859967e-03\n\n\n\nsummary(lm(salary ~ ysdeg + degree + sex + rank, data = year_under_15))$coefficients\n\n\n               Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) 25910.57830 1816.74988 14.2620504 6.078443e-12\nysdeg         195.70869   98.28475  1.9912417 6.029085e-02\ndegreePhD    1916.61721 1038.11879  1.8462407 7.971122e-02\nsexFemale     -68.13685  823.90189 -0.0827002 9.349120e-01\nrankAsst    -9578.41641 1661.19417 -5.7659824 1.214619e-05\nrankAssoc   -5296.88254 1496.02290 -3.5406427 2.052461e-03\n\nMulticollinearity would be a concern for conducting this regression, since years in degree will be almost identical with the variable years here, since we are told that individuals were at their highest degree 15 years ago. Therefore years is removed from the regression.\nWe can see below there is support for this hypothesis, since the intercept (base value excluding other predictors) is higher for those highered in the last 15 years rather than those who have been tenured longer. This is a difference of ~ $1500, in both regressions these coefficients have significance in the p < 0.05 level.\n3)\n\n\ndata_3 <- data(house.selling.price)\n\n\n\nA)\n\n\nsummary(lm(Price ~ Size + New, data = house.selling.price))$coefficients\n\n\n               Estimate   Std. Error   t value     Pr(>|t|)\n(Intercept) -40230.8668 14696.139626 -2.737513 7.365064e-03\nSize           116.1316     8.794993 13.204284 2.153130e-23\nNew          57736.2828 18653.040780  3.095275 2.570122e-03\n\nB)\nSelling price = y size of home = x new = z\nGeneral Equation: y = 116x + 57736z -40231\nSignficance: Each coefficient has signficance at p <0.05 level. Interpretating these positive coefficients implies that for each additional square foot, price of a home increases by $116. Newer houses are worth ~57,700 more than non new houses.\nEquation for new home: y = 116x +57736(1) - 40231 = y = 116x - 17506\nEquation for non new homes y = 116x +57736(0) - 40231 = y = 116x - 40231\nC)\nNew: y = 116(3000) - 17506 =330494\nNon new y = 116(3000) - 40231 =307770\nD\n\n\nsummary(lm(Price ~ Size + New + Size*New, data = house.selling.price))$coefficients\n\n\n                Estimate   Std. Error   t value     Pr(>|t|)\n(Intercept) -22227.80793 15521.109973 -1.432102 1.553627e-01\nSize           104.43839     9.424079 11.082080 7.198590e-19\nNew         -78527.50235 51007.641896 -1.539524 1.269661e-01\nSize:New        61.91588    21.685692  2.855149 5.271610e-03\n\nE)\nGeneral y = 104x -78527z + 62(xz) - 22228\nNew:\ny = 104x -78527(1) + 62(x(1))- 22228\ny= 166x -100755\nNon-new\ny = 104x -78527(0) + 62(x(0))- 22228\ny = 104x - 22228\nF)\nNew: y= 166(3000) -100755\n=397245\nNon-new y = 104(3000) - 22228 289772\nG)\n(166*1500) - 100755 New: 148245\n(104*1500) - 22228 Non New 133772\nAs the home size increases, the difference in new vs. non new houses grows larger\nH)\nI prefer the latter model with interaction, since it accounts for the fact that size and newness may be related\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-03T21:42:46-04:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomalexanderhong86884812/",
    "title": "DACSS 603 HW#3",
    "description": "Third homework for DACSS 603.",
    "author": [
      {
        "name": "Alexander Hong",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nQuestion 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\n\n\n## x1 = size of home in sq ft\n## x2 = lot size in sq ft\n\noptions(scipen = 100)\n\nhw1_int <- -10536\nhw1_x1 <- 53.8\nhw1_x2 <- 2.84\n\nhw1a <- hw1_int + (hw1_x1 * 1240) + (hw1_x2 * 18000)\nhw1a_resid <- 145000 - hw1a\n\nt1 <- c(1240, 1241, 1242)\nt2 <- c(hw1_int + (hw1_x1 * t1) + (hw1_x2 * 18000))\nt12 <- data.frame(t1, t2)\ntable_t12 <- knitr::kable(t12)\n\n\n\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\nThe predicted selling price for this home is $107296 and the residual is $37704. In this case, the house was able to be sold for over $37,000 more than its predicted price\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nt1\nt2\n1240\n107296.0\n1241\n107349.8\n1242\n107403.6\nUsing some the same parameters from the question above, where lot size is set at 18000 sq ft, and increasing the square footage by 1 from 1240, the increase in selling price is $53.80 (which is the value for x1)\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n53.8 / 2.84 ≈ 19.84 sq/ft\nQuestion 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\nlibrary(alr4)\nlibrary(dplyr)\n\nhw2 <- salary\n\n#Gender Salary Tests\nhw2_male_salary <- subset(hw2, hw2$sex == \"Male\", select = (c(\"salary\")))\nhw2_female_salary <- subset(hw2, hw2$sex == \"Female\", select = (c(\"salary\")))\n\nhw2_saltest <- t.test(hw2_male_salary , hw2_female_salary)\n\n#Multiple LM\nhw2_lm <- lm(salary ~ degree + rank + sex + year + ysdeg, data = hw2)\n\n#Gender Confidence Interval\nhw2_sex_ci <- confint(hw2_lm, 'sexFemale', level = 0.95)\nc11 <- round(hw2_sex_ci[1], 4)\nc12 <- round(hw2_sex_ci[2], 4)\n\n#Coefficients Summary\nhw2_lm_summary <- summary(hw2_lm)\n\n#Level Change for Rank\nhw2$rank <- relevel(hw2$rank, ref = \"Assoc\")\nhw2_lm <- lm(salary ~ degree + rank + sex + year + ysdeg, data = hw2)\nhw2_lm_summary <- summary(hw2_lm)\n\n# Regression Without Rank\nhw2_lm2 <- lm(salary ~ degree + + sex + year + ysdeg, data = hw2)\nhw2_lm2_summary <- summary(hw2_lm2)\n\n# Variable for employees tenured 15 years or less\nhw2$rank <- relevel(hw2$rank, ref = \"Asst\")\nhw2$ysdeg15 <- ifelse(hw2$ysdeg <=  15, 1, 0)\n\nhw2_lm3 <- lm(salary ~ degree + sex + rank + ysdeg15, data = hw2)\nhw2_lm3_summary <- summary(hw2_lm3)\n\n\n\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\nHo : The mean salaries between men and women equal each other\nHa : The mean salaries between men and women are not the same\nGoing only on the basis of gender, I conducted a T-Test using the salaries of the male and female employees. With a T statistic of 1.774438 on 21.591032 degrees of freedom, the p-value is 0.0900941, which does not allow me to reject the null hypothesis that the mean salaries between men and women are equal.\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\nThe confidence Interval for the difference in salary between males and females is as follows (-697.8183 , 3030.5645 )\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n21038.4085\n1109.11624\n18.968623\n0.0000000\ndegreePhD\n1388.6133\n1018.74688\n1.363060\n0.1796454\nrankAsst\n-5292.3608\n1145.39802\n-4.620543\n0.0000322\nrankProf\n5826.4032\n1012.93301\n5.752012\n0.0000007\nsexFemale\n1166.3731\n925.56888\n1.260169\n0.2141043\nyear\n476.3090\n94.91357\n5.018345\n0.0000087\nysdeg\n-124.5743\n77.48628\n-1.607695\n0.1148967\nFor degree - Whether or not an employee had a Masters or a PhD was not considered to be statistically significant. If a employee were to get a PhD, their salary would increase by $1388.61.\nFor rank - The employees rank within the college was considered to be statistically significant. Using the assistant rank as a baseline, an employee with an associate rank would contribute towards $5269.36 towards their salary, and $11,118.76 if the employee was a professor.\nFor sex - The employee sex was not considered to be statistically significant.\nFor year - The number of years an employee is at their current rank was deemed to be considered statistically significant. An employee’s rank would count towards $476.31 of their salary, multiplied by the number of years spent at the rank.\nFor ysdeg - The number of years since highest degree earned was not deemed to be statistically significant towards predicting the employees salary. As it is modeled, for every year that an employee spends working, that employee would lose out on $124.57 for each passing year from the highest degree obtained.\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\nUsing the Associate rank as a baseline, an employee with an Assistant rank would lose out on $5292.36 in salary, while a Professor would earn $5826.40 in salary.\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n17183.5717\n1147.94172\n14.9690280\n0.0000000\ndegreePhD\n-3299.3488\n1302.51952\n-2.5330514\n0.0147040\nsexFemale\n-1286.5443\n1313.08854\n-0.9797849\n0.3322090\nyear\n351.9686\n142.48087\n2.4702865\n0.0171854\nysdeg\n339.3990\n80.62097\n4.2098109\n0.0001144\nBy not fitting rank, the variables that are are deemed statistically significant in predicting salary are degree, years in current rank, and years since the highest degree earned. The most notable change is that the years since highest degree and the employee’s degree is now deemed to be statistically signifcant in this model, as opposed to the former model in which these two variables weren’t.\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary. Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nlibrary(alr4)\nlibrary(dplyr)\n\nhw2 <- salary\nplot(hw2$year, hw2$ysdeg)\n\n\n\n\nAfter plotting year and ysdeg against each other, these two variables seem to be very correlated with each other. Thus, in fitting our model, I opted to remove year from the model, given the year variable might mask the true effect the ysdeg variable, since that is our variable of interest.\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n17585.646\n1621.1719\n10.8474898\n0.0000000\ndegreePhD\n1126.187\n1018.3701\n1.1058716\n0.2745324\nsexFemale\n-829.240\n997.5536\n-0.8312737\n0.4101134\nrankAssoc\n4825.252\n1276.0371\n3.7814356\n0.0004482\nrankProf\n11925.702\n1512.4142\n7.8852089\n0.0000000\nysdeg15\n319.032\n1303.7673\n0.2447001\n0.8077769\nAs it turns out, there does not seem to be enough evidence that those hired by the new Dean had a significant effect in salary predictions.\nQuestion 3\n(SMSS 13.7 & 13.8 combined, modified)\n(Data file: house.selling.price in smss R package)\n\n\nlibrary(smss)\n\ndata(house.selling.price)\nhw3_lm <- lm(Price ~ Size + New, house.selling.price)\nhw3_lm_summary <- summary(hw3_lm)\n\nhw3_lm1 <- lm(Price ~ Size, house.selling.price)\nhw3_lm1_summary <- summary(hw3_lm1)\nhw3_lm2 <- lm(Price ~ New, house.selling.price)\nhw3_lm2_summary <- summary(hw3_lm2)\n\nnew3000 <- round(hw3_lm$coefficients[1] + (hw3_lm$coefficients[2] * 3000) + hw3_lm$coefficients[3], 2)\nold3000 <- round(hw3_lm$coefficients[1] + (hw3_lm$coefficients[2] * 3000), 2)\n\n# Interaction Model\nhw3_lm_int <- lm(Price ~ Size + New + Size*New, house.selling.price)\nhw3_lm_int_summary <- summary(hw3_lm_int)\n\nnew3000_int <- round(hw3_lm_int$coefficients[1] + (hw3_lm_int$coefficients[2] * 3000) + hw3_lm_int$coefficients[3] + (hw3_lm_int$coefficients[4] * 3000), 2)\nold3000_int <- round(hw3_lm_int$coefficients[1] + (hw3_lm_int$coefficients[2] * 3000) + hw3_lm_int$coefficients[4], 2)\n\nnew1500_int <- round(hw3_lm_int$coefficients[1] + (hw3_lm_int$coefficients[2] * 1500) + hw3_lm_int$coefficients[3] + (hw3_lm_int$coefficients[4] * 1500), 2)\nold1500_int <- round(hw3_lm_int$coefficients[1] + (hw3_lm_int$coefficients[2] * 1500) + hw3_lm_int$coefficients[4], 2)\n\n\n\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n-40230.8668\n14696.139626\n-2.737513\n0.0073651\nSize\n116.1316\n8.794993\n13.204284\n0.0000000\nNew\n57736.2828\n18653.040780\n3.095275\n0.0025701\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n-50926.2547\n14896.372755\n-3.418702\n0.0009181\nSize\n126.5941\n8.467517\n14.950559\n0.0000000\nFor predicting the price of a home, the price of the home will go up by almost $127 per square foot. In this model, Size is a deemed to be statistically significant in predicting the price of ahouse.\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n138567.4\n9503.741\n14.580302\n0.0000000\nNew\n152396.2\n28654.858\n5.318338\n0.0000007\nFor predicting the price of a home, the price of the home will go up by $152,396. In this model, whether or not the house is new is a deemed to be statistically significant in predicting the price of a house.\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nFor a new, 3000 sqaure foot house, the predicted price will be $365900.18\nFor an older, 3000 sqaure foot house, the predicted price will be $308163.9\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n-22227.80793\n15521.109973\n-1.432102\n0.1553627\nSize\n104.43839\n9.424079\n11.082080\n0.0000000\nNew\n-78527.50235\n51007.641896\n-1.539524\n0.1269661\nSize:New\n61.91588\n21.685692\n2.855149\n0.0052716\nFor this model, the size of the house and the interaction between “newness” and size were deemed to be significant in predicting the price of a house. Unlike the previous models, “New” in itself was not considered to be significant.\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nSelling Price = -22227.81 + 104.44(Size) + (-78527.5 for a new house) + (61.92 * size * new house)\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nFor a new, 3000 square foot house, the predicted price will be $398307.51\nFor an older, 3000 square foot house, the predicted price will be $291149.28\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\nFor a new, 1500 square foot house, the predicted price will be $148776.1\nFor an older, 1500 square foot house, the predicted price will be $134491.69\nThe size in home will raise the predicted selling price of the home, which is a common reality. - Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nUsing adjust R Squared as a way to determine which model does better, the interaction model wins out as its Adjusted RSquared of .736 is slightly higher than the regular model, which has an Adjusted RSquared of .717. I think both models do a sufficient job of explaining the reality that the size of a home does increase the cost. I would prefer the interaction model only because it does help validate my thinking that the size of a house combined with its “newness” makes a difference, which happens to be reflected by the fact that the interaction term was deemed to be statistically significant, at least using these two parameters as predictors.\n\n\n\n",
    "preview": "posts/httpsrpubscomalexanderhong86884812/distill-preview.png",
    "last_modified": "2022-04-03T21:42:50-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscommegangeorgesdacss603-hw3/",
    "title": "DACSS 603: Homework 3",
    "description": "Homework # 3 questions and answers for DACSS 603: Introduction to Quantitative Analysis",
    "author": [
      {
        "name": "Megan Georges",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\r\nQuestion 1\r\n(SMSS 11.2, except part (d))\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\r\na. A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\n\r\n\r\n# Calculating predicted selling price\r\nPSPfun <- function(a, b)\r\n{-10536 + 53.8*a + 2.84*b}\r\n\r\nPSPfun(1240, 18000)\r\n\r\n\r\n[1] 107296\r\n\r\n\r\n\r\n# Calculating residual\r\nResidualFUN <- function(real, prd){real - prd}\r\n\r\nResidualFUN(145000, 107296)\r\n\r\n\r\n[1] 37704\r\n\r\nThe predicted selling price is 107,296 dollars and the actual selling price is 145,000 dollars. Therefore, the residual is 37,704 dollars, meaning that the house was sold for 37,704 dollars greater than predicted. The homeowners made out well on this sale!\r\nb. For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\nUsing the prediction equation ŷ = −10,536 + 53.8x1 + 2.84x2, where x2 equals lot size, the house selling price is expected to increase by 53.8 dollars per each square-foot increase in home size given the lot sized is fixed. This is because a fixed lot size would make 2.84x2 a set number in the prediction equation. Therefore, we would not need to factor in a change in the output based on any input. Then, we are left with the coefficient for the home size variable, which is 53.8. For x1=1, representing one square-foot of home size, the output would increase by 53.8*1 = 53.8.\r\nc. According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\n\r\n\r\n# Calculating lot size needed for equal impact of 1 unit increase in home size\r\n\r\n# 53.8(1) = 2.84x2\r\nx2 <- 53.8/2.84\r\nx2\r\n\r\n\r\n[1] 18.94366\r\n\r\nAn increase in lot size of about 18.94 square-feet would have the same impact as an increase of 1 square-foot in home size on the predicted selling price.\r\nQuestion 2\r\n(ALR, 5.17, slightly modified)\r\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\na. Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\n\r\n\r\n# Load data and preview\r\ndata(salary)\r\nhead(salary)\r\n\r\n\r\n   degree rank    sex year ysdeg salary\r\n1 Masters Prof   Male   25    35  36350\r\n2 Masters Prof   Male   13    22  35350\r\n3 Masters Prof   Male   10    23  28200\r\n4 Masters Prof Female    7    27  26775\r\n5     PhD Prof   Male   19    30  33696\r\n6 Masters Prof   Male   16    21  28516\r\n\r\n\r\n\r\n# Testing hypothesis that mean salary for men and women is the same\r\nlmSalSex <- lm(salary ~ sex, salary)\r\nsummary(lmSalSex)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ sex, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8602.8 -4296.6  -100.8  3513.1 16687.9 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)    24697        938  26.330   <2e-16 ***\r\nsexFemale      -3340       1808  -1.847   0.0706 .  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 5782 on 50 degrees of freedom\r\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \r\nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\r\n\r\nTo start, the null hypothesis would be that mean salary for men and mean salary for women are equal, and the alternative hypothesis would be that the salaries are not equal. I ran a regression with sex as the explanatory variable and salary as the outcome variable. The female coefficient is -3340, which means that women do make less than men (not considering any other variables). However, there is a significance level of .07, so we fail to reject the null hypothesis and therefore cannot conclude that there is a difference between mean salaries for men and women.\r\nb. Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\n\r\n\r\nlmSalAll <- lm(salary ~ degree + rank + sex + year + ysdeg, salary)\r\nconfint(lmSalAll)\r\n\r\n\r\n                 2.5 %      97.5 %\r\n(Intercept) 14134.4059 17357.68946\r\ndegreePhD    -663.2482  3440.47485\r\nrankAssoc    2985.4107  7599.31080\r\nrankProf     8396.1546 13841.37340\r\nsexFemale    -697.8183  3030.56452\r\nyear          285.1433   667.47476\r\nysdeg        -280.6397    31.49105\r\n\r\nAssuming there is no interaction between sex and other predictors, we can be 95% confident that the difference in mean salary for women compared to men falls between -697.8183 dollars and 3030.5645 dollars.\r\nc. Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\r\n\r\n\r\nsummary(lmSalAll)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nWhen running a regression with salary as the outcome variable and all other variables as predictors, we have the following findings:\r\nFor degree as the predictor, a PHD would be expected to increase salary by 1388.61 dollars in reference to a Masters degree salary. However, at a significance level of .18, we cannot conclude that degree level has a statistically significant impact on salary.\r\nFor the rank variable, an Associate can expect a 5292.36 dollar increase in salary compared to Assistant, while a Professor can expect a 11118.76 dollar salary increase compared to Assistant. Both ranks have significance levels well below .05 and we can determine that rank does have a statistically significant effect on salary.\r\nFor the variable of sex, a Female can expect a salary increase of 1166.37 dollars in comparison to Male salary, but the significance level is .214, so this is not a statistically significant relationship.\r\nFor year, a faculty member can expect a salary increase of 476.31 dollars for an increase in 1 year of employment in his/her/their position. Additionally, the level of significance is less than .01 so the relationship between year and salary appears to be significant.\r\nFor the ysdeg variable, an increase in years since earning highest degree can expect a decrease in salary, with a coefficient of -124.57. However, with a 0.115 level of significance, this relationship cannot be found to be statistically significant, thus it is not a reliable predictor of salary decrease.\r\nd. Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\r\n\r\n\r\nlmSalRank <- lm(salary ~ rank, salary)\r\nsummary(lmSalRank)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ rank, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5209.0 -1819.2  -417.8  1586.6  8386.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  17768.7      705.5   25.19  < 2e-16 ***\r\nrankAssoc     5407.3     1066.6    5.07 6.09e-06 ***\r\nrankProf     11890.3      972.4   12.23  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2993 on 49 degrees of freedom\r\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \r\nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\r\n\r\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\r\nlmSalRankNew <- lm(salary ~ rank, salary)\r\nsummary(lmSalRankNew)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ rank, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5209.0 -1819.2  -417.8  1586.6  8386.0 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  29659.0      669.3  44.316  < 2e-16 ***\r\nrankAsst    -11890.3      972.4 -12.228  < 2e-16 ***\r\nrankAssoc    -6483.0     1043.0  -6.216 1.09e-07 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2993 on 49 degrees of freedom\r\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \r\nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\r\n\r\nWhen using Assistant as the baseline for the rank variable, which is the lowest ranking between Assistant, Associate, and Professor, the coefficients in the regression show an expected increase in salary as rank increases. A ranking of Associate would predict an increase in salary of 5407.3 dollars, while increasing rank to Professor (highest achievement) would predict an increase in salary of 11890.3 dollars in comparison to the salary of an Assistant. Each relationship has significance levels well below the threshold of .05, thus the results are statistically significant.\r\nWhen changing the baseline for regression on the rank variable with salary as the output, the coefficients look much different. With Professor as the baseline, the rankings of Associate and Assistant are predicted to have salaries that fall below that of the Professor salary. The Associate rank is predicted to have a salary decrease of 6483 dollars compared to the Professor salary, and the Assistant salary is predicted to have a salary 11890.3 dollars below that of a Professor. The significance levels remain well below the threshold of .05, thus this does not change the above conclusion that rank and salary have a statistically significant relationship.\r\ne. Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\r\n\r\n\r\nlmSalnoRank <- lm(salary ~ degree + sex + year + ysdeg, salary)\r\nsummary(lmSalnoRank)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\r\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \r\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \r\nyear          351.97     142.48   2.470 0.017185 *  \r\nysdeg         339.40      80.62   4.210 0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\r\n\r\nWhen removing the variable “rank”, the coefficient for sex is -1286.54 compared to the above regression that included rank with a coefficient for sex at 1166.37. The new coefficient predicts that a female salary would be 1286.54 less than a male salary, when excluding the variable of rank. However, the significance level is 0.332, which is very high and therefore the results cannot be found to be statistically significant. While the change of the coefficient to negative upon removal of rank is interesting, the significance level would likely prevent these results from holding up in court as an indication of discrimination on the basis of sex.\r\nf. Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary. Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\n\r\n\r\n# Create dummy variable for faculty the Dean hired\r\n# Faculty with ysdeg equal to or less than 15 = 1\r\n# Faculty with ysdeg greater than 15 = 0\r\nsalary <- salary %>%\r\n  add_column(hired = ifelse(salary$ysdeg <=15, 1, 0)) \r\n\r\n\r\n\r\n\r\n\r\nlmHired <- lm(salary ~ hired, salary)\r\nsummary(lmHired)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ hired, data = salary)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n -8294  -3486  -1772   3829  10576 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  27469.4      913.4  30.073  < 2e-16 ***\r\nhired        -7343.5     1291.8  -5.685 6.73e-07 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 4658 on 50 degrees of freedom\r\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3804 \r\nF-statistic: 32.32 on 1 and 50 DF,  p-value: 6.734e-07\r\n\r\n\r\n\r\nlmDean <- lm(salary ~ sex + rank + degree + hired, salary)\r\nsummary(lmDean)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ sex + rank + degree + hired, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6187.5 -1750.9  -438.9  1719.5  9362.9 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  29511.3      784.0  37.640  < 2e-16 ***\r\nsexFemale     -829.2      997.6  -0.831    0.410    \r\nrankAsst    -11925.7     1512.4  -7.885 4.37e-10 ***\r\nrankAssoc    -7100.4     1297.0  -5.474 1.76e-06 ***\r\ndegreePhD     1126.2     1018.4   1.106    0.275    \r\nhired          319.0     1303.8   0.245    0.808    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3023 on 46 degrees of freedom\r\nMultiple R-squared:  0.7645,    Adjusted R-squared:  0.7389 \r\nF-statistic: 29.87 on 5 and 46 DF,  p-value: 2.192e-13\r\n\r\nFirst, I created a dummy variable called “hired” which coded those employed for 15 years or less (thus hired by the new Dean) as 1 and those who have been employed for over 15 years as 0. Then, I fit a new regression model and decided to include the variables of sex, rank, degree, and hired. I omitted the year and ysdeg variables to prevent overlapping or multicollinearity. Multicollinearity can be a concern when variables are highly correlated or related in some way. The idea of regression is to observe how each variable partially effects the output while holding the other variables fixed. We cannot reasonably change the year or ysdeg or hired variables individually while holding the other two fixed since they tend to “grow” in similar manners. Since the variable hired is a product of the ysdeg variable, we could not include both. Likewise, the year variable is highly correlated with ysdeg and hired because it looks at a similar predictor (number of years employed vs number of years since obtaining degree vs over/under 15 years at the university).\r\nIn the first of the two linear models displayed, I looked just at the hired variable as a predictor and salary as the output. The coefficient of -7343.5 predicts that those hired within the past 15 years make -7343.5 dollars less than those hired more than 15 years ago, with statistically significant level (less than .001). However, this fails to account for other key variables and the adjusted r-squared is just .38, which shows that the model is not strongly predictive of salary.\r\nThe second model, which uses the above described variables of sex, rank, degree, and hired as predictors and salary as the output, has an adjusted r-squared value of about .74, which suggests this model accounts for better prediction of the output for this data set.\r\nBased on the regression model, those hired by the current Dean are predicted to make 319 dollars more than those not hired by the Dean (employed at the university since before the current Dean’s hire). When it comes to salary, this is a rather insignificant number. Furthermore, the level of significance for the hired variable is .81, which is astronomical and indicates that the relationship between hired and salary is not statistically significant. Based on these factors, I would state that findings do not indicate any favorable treatment (through better salary offers) by the Dean toward faculty that the Dean specifically hired.\r\nQuestion 3\r\n(SMSS 13.7 & 13.8 combined, modified)\r\n(Data file: house.selling.price in smss R package)\r\na. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\n\r\n\r\n# Load and preview data\r\ndata(\"house.selling.price\")\r\nhead(house.selling.price)\r\n\r\n\r\n  case Taxes Beds Baths New  Price Size\r\n1    1  3104    4     2   0 279900 2048\r\n2    2  1173    2     1   0 146500  912\r\n3    3  3076    4     2   0 237700 1654\r\n4    4  1608    3     2   0 200000 2068\r\n5    5  1454    3     3   0 159900 1477\r\n6    6  2997    3     2   1 499900 3153\r\n\r\n\r\n\r\nlmPrice <- lm(Price ~ Size + New, house.selling.price)\r\nsummary(lmPrice)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \r\nSize           116.132      8.795  13.204  < 2e-16 ***\r\nNew          57736.283  18653.041   3.095  0.00257 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nb. Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\nWhen running regression on the house selling price dataset, with selling price as the output and Size and Newness as predictors, both the Size and the designation as New can be expected to increase the expected selling price of a house. An increase in square-footage of a house is expected to increase selling price by 116.13 dollars and if a house is new it is expected to increase the selling price by 57736.28 dollars compared to a not-new house. The significance level for size is less than .001, and the significance level for New is about 0.003. Both are below the significance level threshold of .05 and thus the relationships to selling price are significant based on this regression model.\r\nThe prediction equations for selling price using size and newness as predictors is:\r\n\\(\\hat{y}\\)New = 17505.42 + 116.13x, where x = size of home (square feet)\r\n\\(\\hat{y}\\)NotNew = -40230.87 + 116.13x\r\nThe equation for not new houses omits the value that a house being new adds (57736.28). They are essentially the same equation: \\(\\hat{y}\\) = -40230.87 + 116.13x + 57736.28y, where x = house size and y = status of newness. Because the New variable is a dummy variable, it will be valued at 0 if the house is not new, so the coefficient/value will not be added to the predicted selling price. Therefore, for the \\(\\hat{y}\\)New equation, I just added the value of a new house to the intercept value.\r\nc. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\n# New\r\n17505.42 + (116.13*3000)\r\n\r\n\r\n[1] 365895.4\r\n\r\n# Not new\r\n-40230.87 + (116.13*3000)\r\n\r\n\r\n[1] 308159.1\r\n\r\nThe predicted selling price of a house that is 3000 square feet in size will be 365895.4 dollars for a new house and 308159.1 dollars for a not new house.\r\nd. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\r\n\r\n\r\nlmSizeNew <- lm(Price ~ Size*New, house.selling.price)\r\nsummary(lmSizeNew)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432  0.15536    \r\nSize           104.438      9.424  11.082  < 2e-16 ***\r\nNew         -78527.502  51007.642  -1.540  0.12697    \r\nSize:New        61.916     21.686   2.855  0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\ne. Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\r\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\r\n\\(\\hat{y}\\)New = -22227.81 + 104.44x - 78527.50y + 61.92(xy)\r\n\\(\\hat{y}\\)NotNew = -22227.81 + 104.44x\r\nwhere x = size of house (square-feet) and y = 1 for New, 0 for not New\r\nf. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\n# New house\r\n-22227.81 + (104.44*3000) - 78527.50 + (61.92*3000)\r\n\r\n\r\n[1] 398324.7\r\n\r\n# Not new house\r\n-22227.81 + (104.44*3000)\r\n\r\n\r\n[1] 291092.2\r\n\r\nThe predicted selling price for a house of 3000 square-feet that is New is 398324.7 dollars and for a house that is not new is 291092.2 dollars.\r\ng. Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\n\r\n\r\n# New house\r\n-22227.81 + (104.44*1500) - 78527.50 + (61.92*1500)\r\n\r\n\r\n[1] 148784.7\r\n\r\n# Not new house\r\n-22227.81 + (104.44*1500)\r\n\r\n\r\n[1] 134432.2\r\n\r\nThe predicted selling price for a house of 3000 square-feet that is New is 148784.7 dollars and for a house that is not new is 134432.2 dollars. Compared to the previous question, with a house double the size in square-feet, the difference in these predicted selling prices is much smaller and it seems that size has less of an impact on selling price once the size gets so small, and as the size increases it has a greater impact on the predicted selling price.\r\nh. Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\r\nIt’s interesting that the prediction model with interaction has a significantly large negative coefficient for the New variable (but the interaction coefficient can make up for it once the house surpasses about 1268 square-feet). The adjusted r-squared for the model with interaction is 0.7363 and the adjusted r-squared for the first model without interaction is 0.7169. The increase in the adjusted r-squared with the interaction model could be due to an additional variable (interaction variable) or could indicate a slightly better fit for the prediction of the data. Since the models do have similar adjusted r-squared values, I would prefer the model with interaction because the regression indicates that the interaction term is statistically significant to selling price prediction, so I feel it is necessary to utilize an equation that factors for this.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-03T21:42:58-04:00",
    "input_file": {}
  },
  {
    "path": "posts/httprpubscomkpopiela881714/",
    "title": "Homework 2",
    "description": "DACSS-603",
    "author": [
      {
        "name": "Katie Popiela",
        "url": {}
      }
    ],
    "date": "2022-03-27",
    "categories": [],
    "contents": "\r\nQuestion 1\r\n(Problem 1.1 in ALR) United Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\na. Identify the predictor and the response\r\nPredictor: ppgdp\r\nResponse: fertility\r\nb. Draw the scatterplot of “fertility” on the vertical axis versus “ppgdp” on the horizontal axis and summarize the information in the graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n\r\n\r\nlibrary(alr4)\r\ndata(UN11)\r\n\r\nlibrary(ggplot2)\r\nggplot(data = UN11, aes(x=ppgdp,y=fertility)) + geom_point()\r\n\r\n\r\n\r\n\r\nThe scatterplot shows a marked decline in fertility rates as GDP increases. I will now recreate the scatterplot with a straight-line function to see if it appears to be appropriate for this presentation of the data.\r\n\r\n\r\ndata(UN11)\r\nggplot(data = UN11, aes(x=ppgdp,y=fertility)) + geom_point()+\r\n  geom_smooth(method=\"lm\",se=FALSE)\r\n\r\n\r\n\r\n\r\nAs can be seen above, a straight-line function is not appropriate; the data is not currently presented in a linear manner (the data is L-shaped). I will now try a linear regression model to see if a straight-line funtion is applicable there.\r\nc. Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\n\r\n\r\ndata(UN11)\r\nggplot(data = UN11, aes(x=log(ppgdp),y=log(fertility))) + geom_point() +\r\n  geom_smooth(method=\"lm\",se=FALSE)\r\n\r\n\r\n\r\n\r\nA simple linear regression model is much more plausible for a straight-line function.\r\nQuestion 2\r\n(Problem 9.47 in SMSS) Annual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\na. How, if at all, does the slope prediction equation change?\r\nTo account for the conversion rate from USD to GBP, the value of the response must be divided by 1.33. The slope shall also be divided by 1.33.\r\nb. How, if at all, does the correlation change?\r\nCorrelation isn’t affected by units of measurement, so it would not change in this scenario.\r\nQuestion 3\r\n(Problem 1.5 in ALR) Water runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\nlibrary(alr4)\r\ndata(water)\r\npairs(water)\r\n\r\n\r\n\r\n\r\nSince this matrix presents a lot of information, I’ll summarize:\r\n* Year doesn’t seem to be related to runoff or water levels\r\n* The following variables appear to be correlated with each other: OPBPC, OPRC, OPSLAKE. All parts of the matrix with 2 of these variables exhibit a dependence among themselves that is not present between OPBPC, OPRC, and OPSLAKE and APMAM, APSAB, APSLAKE. That being said, though, there also appears to be a correlation among APMAM, APSAB, APSLAKE.\r\n* BSAAM is more closely related to OPBPC, OPRC, and OPSLAKE than to APMAM, APSAB, APSLAKE.\r\nQuestion 4\r\n(Problem 1.6 in ALR, modified) Professor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n\r\n\r\nlibrary(alr4)\r\ndata(Rateprof)\r\npairs(Rateprof[c(\"quality\",\"clarity\",\"helpfulness\",\"easiness\",\"raterInterest\")])\r\n\r\n\r\n\r\n\r\nThere is a strong correlation among “quality”, “clarity”, and “helpfulness.” As those variables increase, so too do the professors’ ratings. This makes sense as the quality and clarity of the material, as well as the professor’s helpfulness are major factors in undergraduate learning. There appears to be some correlation among “helpfulness” and “easiness” but the data is much more dispersed. “raterInterest” seems pretty consistent in the middle of each graph, indicating that the rater is at least moderately interested in the subject matter of the courses they are rating.\r\nQuestion 5\r\n(Problem 9.34 in SMSS) For the “student.survey” data file in the smss package, conduce regression analyses relating:\r\na. y = political ideology and x = religiosity\r\nb. y = high school GPA and x = hours of TV watching\r\n(You can use ?student.survey in the R console after loading the package to see what each variable means)\r\n* Use graphical ways to portray the individual variables and their relationships.\r\n* Interpret descriptive statistics for summarizing the individual variables and their relationships.\r\n* Summarize and interpret the results of inferential analyses.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nlibrary(smss)\r\ndata(\"student.survey\")\r\ncolnames(student.survey)\r\n\r\n\r\n [1] \"subj\" \"ge\"   \"ag\"   \"hi\"   \"co\"   \"dh\"   \"dr\"   \"tv\"   \"sp\"  \r\n[10] \"ne\"   \"ah\"   \"ve\"   \"pa\"   \"pi\"   \"re\"   \"ab\"   \"aa\"   \"ld\"  \r\n\r\nThe variables I will be focusing on (as per the problem) are “re” (x) and “pi” (y) for subsection (a); and then “hi” (x) and “tv” (y) for subsection (b)\r\n\r\n\r\nlibrary(smss)\r\ndata(\"student.survey\")\r\nggplot(data=student.survey,aes(x=re,fill=pi))+\r\n  geom_bar() + labs(x=\"Religiosity\", fill =\"Political Ideology\")\r\n\r\n\r\n\r\n\r\nThe graph above is one (of several possible) visualizations of the relationship between religiosity and political ideology. I couldn’t figure out how to get more info on what exactly the variables mean, so I’m assuming “Religiosity” refers to the frequency individuals of different political ideologies go to church/temple/mosque/etc.. From left to right, the frequency goes from “never” to “every week.” As frequency increases, so too does conservatism. While not a majority by any means, it is still significant to note that those who identify as very conservative only appear in the bar labelled “every week,” whereas those who identify as very liberal are not even present on the graph to the right of “occasionally.” This, therefore, indicates that those who are heavily liberal-leaning in political ideology are far less likely to go to church/temple/mosque/etc. regularly/frequently than those who are more conservative.\r\nb. y = high school GPA and x = hours of TV watching\r\n\r\n\r\ndata(\"student.survey\")\r\nggplot(data=student.survey,aes(x=hi, y=tv)) +\r\n  geom_point() + labs(x=\"High School GPA\", y=\"Hours Watching TV\")\r\n\r\n\r\n\r\n\r\nOnce again, this graph is just one of several visualizations that could be used. I chose a scatterplot to reflect individual responses; a standard bar graph for this scenario is, in my opinion, misleading as outliers appear to be a much higher concentration of responses. Given the measurements on this graph, I am also assuming that the y-axis refers to hours of TV watched per week. While this graph does not show a linear relationship between the two variables, there is a higher concentration of responses with higher GPA’s and lower # of hours watching TV. I will conduct a simple regression model to test whether a linear realtionship exists.\r\n\r\n\r\nggplot(data = student.survey, aes(x=log(hi),y=log(tv))) + geom_point() +\r\n  labs(x=\"High School GPA\",y=\"Hours Watching TV\")\r\n\r\n\r\n\r\n\r\nEven with a linear regression model, there does not appear to be a linear relationship between these 2 variables. There is still a higher concentration of responses on the higher end of the spectrum, but there is enough variation in the responses to argue that “Hours Watching TV” does not have a correlative affect on “High School GPA.”\r\nNow I will present some descriptive/summary statistics of all 4 variables to show their statistical significance.\r\n\r\n\r\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\r\n\r\n\r\n                     pi                re           hi       \r\n very liberal         : 8   never       :15   Min.   :2.000  \r\n liberal              :24   occasionally:29   1st Qu.:3.000  \r\n slightly liberal     : 6   most weeks  : 7   Median :3.350  \r\n moderate             :10   every week  : 9   Mean   :3.308  \r\n slightly conservative: 6                     3rd Qu.:3.625  \r\n conservative         : 4                     Max.   :4.000  \r\n very conservative    : 2                                    \r\n       tv        \r\n Min.   : 0.000  \r\n 1st Qu.: 3.000  \r\n Median : 6.000  \r\n Mean   : 7.267  \r\n 3rd Qu.:10.000  \r\n Max.   :37.000  \r\n                 \r\n\r\nSince each sample has a different number of subjects/respondents, the results are somewhat skewed.The distribution of high school GPA looks relatively normal and unimodal. The distribution of “Hours Watching TV” on the other hand, has several outliers which can be seen both on the above graph and in the summary statistics via the difference between the 3rd quartile and maximum values.\r\nQuestion 6\r\n(Problem 9.50 in SMSS)\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\r\nRegression toward the mean implies that outlying/extreme values will always occur each time the test or experiment is conducted. Additionally the values found in any reproduction of the test or experiment will be the same as the previous. In this scenario, the students could have been chosen from the group of those not doing well by chance, and the “improvement” seen in the graph might just be regression toward the mean and not actual academic improvement.\r\n\r\n\r\n\r\n",
    "preview": "posts/httprpubscomkpopiela881714/distill-preview.png",
    "last_modified": "2022-03-27T15:55:59-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomrhyslong96877697/",
    "title": "Homework 2",
    "description": "Here is Rhys Long's submission for Homework 2",
    "author": [
      {
        "name": "Rhys Long",
        "url": {}
      }
    ],
    "date": "2022-03-23",
    "categories": [],
    "contents": "\n\n\nlibrary(\"alr4\")\nlibrary(\"ggplot2\")\nlibrary(\"smss\")\nlibrary(\"tidyverse\")\nlibrary(\"numbers\")\n\n\n\nQuestion 1\nFor the first question, the predictor variable is pddgdp and the response variable is fertility because the amount of fertility is influenced by the amount of ppgdp.\n\n\ndata(UN11)\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\nWhen placing ppgdp on the x axis and fertility on the y axis, the correlation is negative. In other words, as the amount of ppgdp increases, the amount of fertility decreases. In the scatter plot below, a straight line mean function does not appear to be plausible for summarizing the data because the points appear to form a curve.\n\n\nplot(x=UN11$ppgdp,y=UN11$fertility)\n\n\n\n\nWhen using the log() function for the predictor and response variables, there is still a negative correlation between ppgdp and fertility. However, unlike the plot without the log() function, a simple linear regression is appropriate for the corresponding scatter plot because the points are distributed in a more linear manner.\n\n\nplot(x=log(UN11$ppgdp),y=log(UN11$fertility))\n\n\n\n\nQuestion 2\nIf annual income is an explanatory variable and the response variable is expressed in terms of currency, the slope of the prediction equation wouldn’t change at all because both the rise and the run would be converted to pounds in this scenario.\n\n\n#Slope When The Income, X, Is In Dollars And Y Is Also Currency\nDollar_Rise=500\nDollar_Run=2500\nDollar_Slope=Dollar_Rise/Dollar_Run\nc(\"Slope When Income Is In Dollars\", Dollar_Slope)\n\n\n[1] \"Slope When Income Is In Dollars\"\n[2] \"0.2\"                            \n\n#Slope When The Income, X, Is In Pounds And Y Is Also Currency\nPound_Rise=Dollar_Rise/1.33\nPound_Run=Dollar_Run/1.33\nPound_Slope=Pound_Rise/Pound_Run\nc(\"Slope When Income Is In Pounds\", Pound_Slope)\n\n\n[1] \"Slope When Income Is In Pounds\" \"0.2\"                           \n\nIf the response variable is not expressed in terms of currency, I hypothesize that “New Slope”=“Rise Of Old Slope”/(“Run Of Old Slope”/1.33). My hypothesis differs from the hypothesis provided in the textbook. In the book, the hypothesis is “New Slope”=“Old Slope”/1.33. To determine which hypothesis is more accurate, I will first come up with my own data set using vectors and the data.frame() function.\n\n\nIncome_Dollars <- c(100000, 2800, 52000, 840, 590000, 3400, 2700, 430)\nResponse_Variable <- c(200, 23400, 7600, 864000, 520, 64200, 4920, 658300)\nData_In_Dollars <- data.frame(Income_Dollars, Response_Variable)\nData_In_Dollars\n\n\n  Income_Dollars Response_Variable\n1         100000               200\n2           2800             23400\n3          52000              7600\n4            840            864000\n5         590000               520\n6           3400             64200\n7           2700              4920\n8            430            658300\n\nAfter creating a data set with the income in dollars, I will now use the lm() function to figure out the slope and y intercept. The y intercept is listed under “(Intercept)” and the slope is listed under “Income_Dollars”.\n\n\nDollar_Regression <- lm(formula = Response_Variable~Income_Dollars, data=Data_In_Dollars)\nprint(Dollar_Regression)\n\n\n\nCall:\nlm(formula = Response_Variable ~ Income_Dollars, data = Data_In_Dollars)\n\nCoefficients:\n   (Intercept)  Income_Dollars  \n    251694.004          -0.519  \n\nBefore testing the hypotheses, I will now create a Data_In_Pounds data set, which has the same response values as Data_In_Dollars, but has the income values converted to pounds. I will also use lm() to determine what the actual slope would be if the income is converted to pounds.\n\n\nData_In_Pounds <- rename(Data_In_Dollars, Income_Pounds=Income_Dollars) %>%\n  mutate(Income_Pounds=Income_Pounds/1.33)\nData_In_Pounds\n\n\n  Income_Pounds Response_Variable\n1    75187.9699               200\n2     2105.2632             23400\n3    39097.7444              7600\n4      631.5789            864000\n5   443609.0226               520\n6     2556.3910             64200\n7     2030.0752              4920\n8      323.3083            658300\n\nPound_Regression <- lm(formula = Response_Variable~Income_Pounds, data=Data_In_Pounds)\nprint(Pound_Regression)\n\n\n\nCall:\nlm(formula = Response_Variable ~ Income_Pounds, data = Data_In_Pounds)\n\nCoefficients:\n  (Intercept)  Income_Pounds  \n    2.517e+05     -6.903e-01  \n\nNow, I will test the book’s hypothesis and my hypothesis to see which one is more accurate. The equation I’m using to test the book’s hypothesis is M2=M1/1.33 and the equation I’m using to test my hypothesis is M2=(Y-B)/(X2). In both equations, M1 is the slope of the data when the income is in dollars and M2 is the predicted slope when the income is converted to pounds. In my equation, X1 is the “run value” of M1, X2 is the predicted “run value” of M2, Y1 and Y2 are 0, B1 is the y intercept of the prediction equation when income is in dollars, and B2 is the y intercept of the of the prediction equation when income is in pounds (and is identical to B1).\n\n\n#The Book's Hypothesis\nM1=-0.519\nBook_M2=M1/1.33\n\n#My Hypothesis\nY1=0\nB1=251694.004\nX1=(Y1-B1)/M1\nY2=Y1\nB2=B1\nX2=X1/1.33\nRhys_M2=(Y2-B2)/(X2)\n\n#The Actual Answer\nActual_M2=-6.903e-1\n\n\n\nBased on the results below, it is safe to conclude that if income is converted to pounds, the new slope would have the same rise value as the old slope, but the run value would be different. It is also safe to conclude that converting the income to pounds will not change the correlation of the data because the slope would still negative and a negative slope is indicative of a negative correlation.\n\n\n#Comparing Slopes\nc(\"Book's Predicted Slope\", Book_M2)\n\n\n[1] \"Book's Predicted Slope\" \"-0.390225563909774\"    \n\nc(\"My Predicted Slope\", Rhys_M2)\n\n\n[1] \"My Predicted Slope\" \"-0.69027\"          \n\nc(\"Actual Slope\", Actual_M2)\n\n\n[1] \"Actual Slope\" \"-0.6903\"     \n\nQuestion 3\nBased on the scatter plot matrix below it is definitely safe to conclude that the runoff volumes of similar venues are correlated with each other. More specifically, the runoff volumes of APMAM, APSAB, and APSLAKE are all positively correlated with each other and the runoff volumes of OPBPC, OPRC, and OPSLAKE are all positively correlated with each other. It is also possible to conclude that the runoff volume of BSAAM is correlated with the runoff volumes of OPBPC, OPRC, and OPSLAKE, but not with the runoff volumes of APMAM, APSAB, and APSLAKE. However, it isn’t possible to conclude whether the runoff volumes of future years can be predicted because even though there appears to be a very slight positive correlation with year and runoff volumes for each venue, the correlations look too weak to be significant.\n\n\ndata(water)\npairs(water)\n\n\n\n\nQuestion 4\nTo find the relationships between the quality, helpfulness, clarity, easiness of instructor’s courses, and rater interest, I can’t just use the pairs(Rateprof) function because when I do, I won’t get an scatter plot that contains the variables I’m interested in analyzing.\n\n\ndata(Rateprof)\npairs(Rateprof)\n\n\n\n\nIn order to fix the problems shown in the scatter plot above, I will now create a new data set with only the quality, helpfulness, clarity, easiness of instructor’s course, and rater interest variables selected before using the pairs() function to form a scatter plot matrix.\n\n\nRateprof_Data <- select(Rateprof, quality:raterInterest)\npairs(Rateprof_Data)\n\n\n\n\nBased on the scatter plot above, quality ratings have a strong positive correlation with helpfulness and clarity ratings. In other words, if a professor received high quality ratings, chances are that they received high helpfulness and clarity ratings as well. There is also a strong positive correlation between helpfulness and clarity, but the correlation isn’t as strong as the first two correlations I mentioned. The correlation between easiness and quality, easiness and helpfulness, and clarity and easiness are also positive, but the correlations are somewhat weak. Out of the possible correlations depicted in the scatterplot, the correlations containing the “raterInterest” variable are the weakest, meaning that a rater’s interest in the subject matter doesn’t influence how they rate their professor.\nQuestion 5\nPart 1: Analyzing The Correlation Between Frequency of Religious Service Attendance and Political Ideology\nSince the religiosity and political ideology are both categorical variables, I will first provide bar charts depicting which political ideology and religiosity choices were most common in the survey.\n\n\n#Data Set\ndata(\"student.survey\")\nggplot(student.survey, aes(x=re)) + geom_bar() + labs(title=\"Religiosity Of Respondents\", x=\"Frequency Of Religious Service Attendance\")\n\n\n\nggplot(student.survey, aes(x=pi)) + geom_bar() + labs(title=\"Political Ideology Of Respondents\", x=\"Political Ideology\")\n\n\n\n\nI suspect that infrequent religious service attendance is correlated with being liberal and frequent religious service attendance is correlated with being conservative because both bar charts are skewed to the left. To get a clearer idea of how correlated the two variables are with each other, I am going to find the central tendencies. Given that both variables are categorical, this will require using mutate() to assign numerical scales to each variable.\n\n\nConverted_Survey <- select(student.survey, re, pi) %>%\n  mutate(re=case_when(\n    re==\"never\"~0,\n    re==\"occasionally\"~1,\n    re==\"most weeks\"~2,\n    re==\"every week\"~3,\n    )) %>%\n  mutate(pi=case_when(\n    pi==\"very liberal\"~-3,\n    pi==\"liberal\"~-2,\n    pi==\"slightly liberal\"~-1,\n    pi==\"moderate\"~0,\n    pi==\"slightly conservative\"~1,\n    pi==\"conservative\"~2,\n    pi==\"very conservative\"~3,\n  ))\nConverted_Survey\n\n\n   re pi\n1   2  2\n2   1 -2\n3   2 -2\n4   1  0\n5   0 -3\n6   1 -2\n7   1 -2\n8   1 -2\n9   1 -3\n10  0 -1\n11  1  1\n12  1 -2\n13  1 -3\n14  1  0\n15  0 -3\n16  3 -2\n17  3 -1\n18  0 -2\n19  0 -2\n20  1 -3\n21  1  0\n22  1 -2\n23  3  3\n24  0  0\n25  0 -3\n26  2 -1\n27  1 -2\n28  2  1\n29  1  0\n30  1 -2\n31  1 -2\n32  1 -2\n33  0 -2\n34  1 -2\n35  1 -3\n36  3  1\n37  0 -2\n38  0 -2\n39  2  1\n40  2  2\n41  1 -2\n42  0 -1\n43  3  2\n44  0  1\n45  1  0\n46  0 -2\n47  1 -1\n48  3  3\n49  0 -2\n50  1 -3\n51  3  1\n52  1  0\n53  1 -2\n54  1 -2\n55  0 -2\n56  3  2\n57  1  0\n58  3 -1\n59  2  0\n60  1  0\n\nNow that I have scales for each response, I will use summary to figure out the central tendencies of each variable. My central tendency findings appear to be consistent with the findings displayed in the bar charts. The median of my religious service attendance data, 1, is slightly smaller than the mean, which is 1.17. This is indicative of the data being subtly skewed towards the left. The political ideology data is also skewed to left because the mean, -0.967, is higher than the median, which is -2, but the skewness of the political ideology data is more pronounced than that of the religious service attendance data.\n\n\nsummary(Converted_Survey, re, pi)\n\n\n       re             pi        \n Min.   :0.00   Min.   :-3.000  \n 1st Qu.:0.75   1st Qu.:-2.000  \n Median :1.00   Median :-2.000  \n Mean   :1.17   Mean   :-0.967  \n 3rd Qu.:2.00   3rd Qu.: 0.000  \n Max.   :3.00   Max.   : 3.000  \n\nNow, I will use regressions to figure out how correlated religious service attendance frequency is with political ideology. Given that religious service attendance frequency could be responsible for shaping political ideologies and politics may be responsible for turning people towards or away from religion, I am going to create two scatter plots with 2 separate regression equations. In the first scatter plot, religious service attendance frequency is the explanatory variable and political ideology is the response variable. In the second scatter plot, the vice versa is true.\n\n\nScenario_1 <- lm(formula = pi~re, data=Converted_Survey)\nprint(Scenario_1)\n\n\n\nCall:\nlm(formula = pi ~ re, data = Converted_Survey)\n\nCoefficients:\n(Intercept)           re  \n    -2.0988       0.9704  \n\nggplot(Converted_Survey, aes(x=re, y=pi)) +  geom_point() + geom_smooth(method=\"lm\") + labs(title = \"Scenario 1: y=0.9704x-2.0988\", x=\"Religiosity\", y=\"Political Ideology\")\n\n\n\n\n\n\nScenario_2 <- lm(formula = re~pi, data=Converted_Survey)\nprint(Scenario_2)\n\n\n\nCall:\nlm(formula = re ~ pi, data = Converted_Survey)\n\nCoefficients:\n(Intercept)           pi  \n     1.5013       0.3461  \n\nggplot(Converted_Survey, aes(x=pi, y=re)) +  geom_point() + geom_smooth(method=\"lm\") + labs(title = \"Scenario 2: y=0.3461x+1.5013\", x=\"Political Ideology\", y=\"Religiosity\")\n\n\n\n\nBased on the scatter plots above, it is safe to assume that being conservative is positively correlated with frequent religious service attendance and frequent religious service attendance is positively correlated with being conservative.\nPart II: Analyzing The Correlation Between Time Spent Watching TV and High School GPA.\nGiven that GPA and time spent watching TV are both quantitative variables, I will start off by displaying the GPA and TV time data as histograms.\n\n\nggplot(student.survey, aes(x=hi)) + geom_histogram(bins=5) + labs(title=\"High School GPA Of Respondents\", x=\"High School GPA\")\n\n\n\nggplot(student.survey, aes(x=tv)) + geom_histogram(bins=5) + labs(title=\"Hours Respondents Spend Watching TV Per Week\", x=\"Hours of TV Watched\")\n\n\n\n\nI suspect that hours spent watching TV per week is negatively correlated with high school GPA because the high school GPA histogram is skewed to the left and the TV time histogram is skewed to the right. I also suspect that both variables contain outliers because GPAs below 3.0 are uncommon and the amount of students who watch less than 10 hours of TV per week far outweighs the amount of students who watch more than 10 hours of TV per week. Before figuring out whether there are outliers, I must figure out the first quartile value for high school GPA, the third quartile value for hours spent watching TV, and the innerquartile ranges for botth variables\n\n\nselect(student.survey, hi, tv) %>%\n  summary()\n\n\n       hi              tv        \n Min.   :2.000   Min.   : 0.000  \n 1st Qu.:3.000   1st Qu.: 3.000  \n Median :3.350   Median : 6.000  \n Mean   :3.308   Mean   : 7.267  \n 3rd Qu.:3.625   3rd Qu.:10.000  \n Max.   :4.000   Max.   :37.000  \n\nsummarize(student.survey, IQR(hi), IQR(tv))\n\n\n  IQR(hi) IQR(tv)\n1   0.625       7\n\nNow that I know the inner quartile range and quartile values for both variables, I can now determine whether there are outliers.\n\n\nTV_IQR=7\nTV_Q3=10\nUpper_TV_Outlier=TV_Q3+(TV_IQR*1.5)\nc(\"TV Time Outlier:\", Upper_TV_Outlier)\n\n\n[1] \"TV Time Outlier:\" \"20.5\"            \n\nHI_IQR=0.625\nHI_Q1=3\nLower_HI_Outlier=HI_Q1-(HI_IQR*1.5)\nc(\"High School GPA Outlier:\", Lower_HI_Outlier)\n\n\n[1] \"High School GPA Outlier:\" \"2.0625\"                  \n\nAccording to my calculations, students with GPAs below 2.0625 and students who watch TV for more than 20 hours per day are considered outliers, so I will create a new version of the dataset that doesn’t contain outliers.\n\n\nNo_Outliers <- select(student.survey, tv, hi) %>%\n  filter(tv <= 20.5) %>%\n  filter(hi >= 2.0625)\n\n\n\nEven with the outliers removed, the high school GPA data remains skewed to the left and the TV data remains skewed to the right. According to the central tendency data below, the median GPA is still higher than the mean GPA and the median amount of TV time remains lower than the mean amount of TV time.\n\n\nselect(student.survey, hi, tv) %>%\n  summary()\n\n\n       hi              tv        \n Min.   :2.000   Min.   : 0.000  \n 1st Qu.:3.000   1st Qu.: 3.000  \n Median :3.350   Median : 6.000  \n Mean   :3.308   Mean   : 7.267  \n 3rd Qu.:3.625   3rd Qu.:10.000  \n Max.   :4.000   Max.   :37.000  \n\nselect(No_Outliers, hi, tv)%>%\n  summary()\n\n\n       hi              tv       \n Min.   :2.100   Min.   : 0.00  \n 1st Qu.:3.000   1st Qu.: 2.75  \n Median :3.400   Median : 5.50  \n Mean   :3.352   Mean   : 6.00  \n 3rd Qu.:3.700   3rd Qu.: 8.50  \n Max.   :4.000   Max.   :16.00  \n\nGiven that the data remains skewed with and without outliers, I suspect that TV time and High School GPA are negatively correlated. In other words, I suspect that students who spend more time watching TV have lower grades than students who spend less time watching TV. To test whether my hypothesis is true, I will create two scatter plots. One of the scatter plots will be based on the data set that includes outliers and the other regression will be based on the data without outliers. Both scatter plots will have TV time as the explanatory variable and GPA as the response variable because it is a lot more probable that watching too much TV causes bad grades.\n\n\nggplot(student.survey, aes(x=tv, y=hi)) + geom_point() + geom_smooth(method=\"lm\") + labs(title=\"Regression With Outliers\", x=\"Hours Spent Watching TV Per Week\", y=\"High School GPA\")\n\n\n\nggplot(No_Outliers, aes(x=tv, y=hi)) + geom_point() + geom_smooth(method=\"lm\") + labs(title=\"Regression Without Outliers\", x=\"Hours Spent Watching TV Per Week\", y=\"High School GPA\")\n\n\n\n\nAs shown in the scatter plots above, the correlation between TV time and GPA being negative regardless of whether outliers are present or not. Visually, it appears that excluding outliers makes the correlation a lot weaker. However, the visual differences are misleading because the slope and y intercept of the data without outliers aren’t significantly different from the slope and y intercept of the data with outliers.\n\n\nFull_Data_Regression <- lm(formula=hi~tv, data=student.survey)\nprint(Full_Data_Regression)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nCoefficients:\n(Intercept)           tv  \n    3.44135     -0.01831  \n\nNo_Outlier_Regression <- lm(formula=hi~tv, data=No_Outliers)\nprint(No_Outlier_Regression)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = No_Outliers)\n\nCoefficients:\n(Intercept)           tv  \n    3.44853     -0.01612  \n\nQuestion 6\nThere is not sufficient evidence to conclude that the tutoring program was successful because according to the concept of regression to the mean, if you do well on one test, you won’t do as well on the next test and at the end of the day, the overall average of test scores will remain the same. Even if this isn’t entirely true, there are spurrious variables that are not being considered such as whether each student studied beforehand. I also determined that the control group got a 72.22 average on the first test and a 71.11 average on the second test using the code below.\n\n\n#First Test Everyone\nEveryone_T1_Mean=70\nEveryone_T1_Total=Everyone_T1_Mean*100\n#First Test Treatment Group\nTreatment_T1_Mean=50\nTreatment_T1_Total=Treatment_T1_Mean*10\n#First Test Control Group\nControl_T1_Total=Everyone_T1_Total-Treatment_T1_Total\nControl_T1_Mean=Control_T1_Total/90\nControl_T1_Mean\n\n\n[1] 72.22222\n\n#Second Test Everyone\nEveryone_T2_Mean=70\nEveryone_T2_Total=Everyone_T2_Mean*100\n#Second Test Treatment Group\nTreatment_T2_Mean=60\nTreatment_T2_Total=Treatment_T2_Mean*10\n#Second Test Control Group\nControl_T2_Total=Everyone_T2_Total-Treatment_T2_Total\nControl_T2_Mean=Control_T2_Total/90\nControl_T2_Mean\n\n\n[1] 71.11111\n\n\n\n\n",
    "preview": "posts/httprpubscomrhyslong96877697/distill-preview.png",
    "last_modified": "2022-03-23T19:46:47-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscommegangeorgesdacss603-hw2/",
    "title": "DACSS 603: Homework 2",
    "description": "Homework # 2 questions and answers for DACSS 603: Introduction to Quantitative Analysis",
    "author": [
      {
        "name": "Megan Georges",
        "url": {}
      }
    ],
    "date": "2022-03-23",
    "categories": [],
    "contents": "\r\nQuestion 1:\r\n(Problem 1.1 in ALR) United Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n1.1.1. Identify the predictor and the response.\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nAnswer to Question 1:\r\n\r\n\r\n# Load dataset\r\ndata(\"UN11\") \r\n\r\n# Select variables of focus\r\nUN11 <- UN11 %>%\r\n  select(c(ppgdp, fertility))  \r\n\r\n# Preview data\r\nhead(UN11)\r\n\r\n\r\n              ppgdp fertility\r\nAfghanistan   499.0     5.968\r\nAlbania      3677.2     1.525\r\nAlgeria      4473.0     2.142\r\nAngola       4321.9     5.135\r\nAnguilla    13750.1     2.000\r\nArgentina    9162.1     2.172\r\n\r\n1.1.1\r\nThe predictor variable is ppgdp (gross national product per person, in US dollars) and the response variable is fertility (birth rate per 1000 females).\r\n1.1.2\r\n\r\n\r\n# Create scatterplot\r\n# fertility on vertical axis, ppgdp on horizontal axis\r\nplot(x = UN11$ppgdp, y = UN11$fertility, xlab = 'ppgdp', ylab = 'fertility', main = 'Scatterplot for Question 1.1.2')\r\n\r\n\r\n\r\n\r\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at first (up to about $10000 ppgdp), then there appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph.\r\n1.1.3\r\n\r\n\r\n# Create scatterplot\r\n# log(fertility) on vertical axis, log(ppgdp) on horizontal axis\r\nplot(x = log(UN11$ppgdp), y = log(UN11$fertility), xlab = 'log(ppgdp)', ylab = 'log(fertility)', main = 'Scatterplot for Question 1.1.3')\r\n\r\n\r\n\r\n\r\nThe simple linear regression seems plausible for summary of this graph. The relationship between the variables (when a log-scale is applied) appears to be negative and rather consistent throughout the graph (as opposed to the graph in 1.1.2, which has a dramatic drop at first then plateaus for the majority of the plot).\r\nQuestion 2:\r\n(Problem 9.47 in SMSS) Annual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\nHow, if at all, does the slope of the prediction equation change?\r\nHow, if at all, does the correlation change?\r\nAnswer to Question 2:\r\na.\r\nThe slope of the prediction equation would change. It would be the initial version’s slope divided by 1.33 to account for the change in unit to pounds.\r\nb.\r\nThe correlation does not change, because it standardizes the slope (thus is not impacted by unit of measure).\r\nQuestion 3:\r\n(Problem 1.5 in ALR) Water runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\nAnswer to Question 3:\r\n\r\n\r\n# load and preview dataset \r\ndata(water)\r\nhead(water)\r\n\r\n\r\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\r\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\r\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\r\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\r\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\r\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\r\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\r\n\r\n\r\n\r\n# create scatterplot matrix\r\npairs(water, main = 'Scatterplot Matrix for Question 3')\r\n\r\n\r\n\r\n\r\nLooking at this scatterplot matrix, it appears that precipitation levels for the ‘A’ named lakes seem to have a positive (relatively linear) correlation (although unsure how strong) with each other and the ‘O’ named lakes seem to have one as well with each other. The year variable does not appear to have a relationship to any of the variables. Also, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes.\r\nQuestion 4:\r\n(Problem 1.6 in ALR - slightly modified) Professor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\nAnswer to Question 4:\r\n\r\n\r\n# load dataset, select variables, preview dataset\r\ndata(Rateprof)\r\n\r\nRateprof <- Rateprof %>%\r\n  select(c(quality, clarity, helpfulness, easiness, raterInterest))  \r\n\r\nhead(Rateprof)\r\n\r\n\r\n   quality  clarity helpfulness easiness raterInterest\r\n1 4.636364 4.636364    4.636364 4.818182      3.545455\r\n2 4.318182 4.090909    4.545455 4.363636      4.000000\r\n3 4.790698 4.860465    4.720930 4.604651      3.432432\r\n4 4.250000 4.041667    4.458333 2.791667      3.181818\r\n5 4.684211 4.684211    4.684211 4.473684      4.214286\r\n6 4.233333 4.200000    4.266667 4.533333      3.916667\r\n\r\n\r\n\r\n# create scatterplot matrix\r\npairs(Rateprof, main = 'Plot for Question 4')\r\n\r\n\r\n\r\n\r\nReferring to the scatterplot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables. There are a few notable outliers in the matrix, for example the data point rating higher for clarity and lower for quality than the trend of other points on the clarity/quality plot. The variables with stronger correlations to each other may suggest that there is a relationship between certain qualities in the professors from the selected university (like professors that tend to be perceived as more helpful by students also tend to have higher clarity) or this could mean that students associate these certain qualities together (thus rating similarly for helpfulness and clarity).\r\nQuestion 5:\r\n(Problem 9.34 in SMSS) For the student.survey data file in the smss package, conduct regression analyses relating:\r\ny = political ideology and x = religiosity,\r\n\r\ny = high school GPA and x = hours of TV watching.\r\n\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\nUse graphical ways to portray the individual variables and their relationship.\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\nSummarize and interpret results of inferential analyses.\r\nAnswer to Question 5:\r\n\r\n\r\n# load and preview data\r\ndata(student.survey)\r\n\r\nhead(student.survey)\r\n\r\n\r\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\r\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\r\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\r\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\r\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\r\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\r\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal\r\n            re    ab    aa    ld\r\n1   most weeks FALSE FALSE FALSE\r\n2 occasionally FALSE FALSE    NA\r\n3   most weeks FALSE FALSE    NA\r\n4 occasionally FALSE FALSE FALSE\r\n5        never FALSE FALSE FALSE\r\n6 occasionally FALSE FALSE    NA\r\n\r\na.\r\n\r\n\r\n# graph: x=religiosity, y=political ideology\r\nrepa <- student.survey %>%\r\n  select(re, pi)\r\n\r\nggplot(data = repa) +\r\n  geom_bar(mapping = aes(x = re, fill = pi)) +\r\n  labs(title = \"Relationship between Religiosity and Political Ideology\", x = \"Religiosity (how often you attend services)\", y = \"Political Ideology (pi)\") \r\n\r\n\r\n\r\n\r\n\r\n\r\n# graph: x=hours of watching tv, y=high school gpa\r\ntvhi <- student.survey %>%\r\n  select(tv, hi)\r\n\r\nggplot(data = tvhi) +\r\n  geom_point(mapping = aes(x = tv, y = hi)) +\r\n  labs(title = \"Relationship between Hours Watching TV and GPA\", x = \"Average Hours of TV watched per Week\", y = \"High School GPA\")\r\n\r\n\r\n\r\n\r\nb.\r\n\r\n\r\n# descriptive statistics: religiosity and political ideology\r\nsummary(repa)\r\n\r\n\r\n            re                         pi    \r\n never       :15   very liberal         : 8  \r\n occasionally:29   liberal              :24  \r\n most weeks  : 7   slightly liberal     : 6  \r\n every week  : 9   moderate             :10  \r\n                   slightly conservative: 6  \r\n                   conservative         : 4  \r\n                   very conservative    : 2  \r\n\r\nBoth the religiosity and political ideology variables are skewed right, with significantly higher counts for “never” and “occasional” service attendance and “liberal” and “moderate” political ideologies in respondents.\r\n\r\n\r\n# descriptive statistics: hours of tv watched and GPA\r\nsummary(tvhi)\r\n\r\n\r\n       tv               hi       \r\n Min.   : 0.000   Min.   :2.000  \r\n 1st Qu.: 3.000   1st Qu.:3.000  \r\n Median : 6.000   Median :3.350  \r\n Mean   : 7.267   Mean   :3.308  \r\n 3rd Qu.:10.000   3rd Qu.:3.625  \r\n Max.   :37.000   Max.   :4.000  \r\n\r\nThe variable average hours of tv watched has a wide range, and the large distance between the 3rd quantile and maximum suggest that there is at least one outlier (which there are multiple when viewing the scatterplot in part a.). Additionally, the median being less than the mean suggests a right skew in the data. The summary for the high school gpa suggests a relatively normal distribution, as the mean and median are similar and lie relatively in the center of the range. However, the gpa is skewed left (although the mode lies directly in the center of the range, there is a higher count of individuals with a gpa above 3.0, as reflected by the mean being above the mode).\r\nc.\r\n\r\n\r\n# inferential analysis: religiosity and political ideology\r\nlmrepa <- lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re))\r\nsummary(lmrepa)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.81243 -0.87160  0.09882  1.12840  3.09882 \r\n\r\nCoefficients:\r\n               Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \r\nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.345 on 58 degrees of freedom\r\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \r\nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\r\n\r\n# looking at Pearson's correlation\r\ncor.test(as.numeric(student.survey$re), as.numeric(student.survey$pi))\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  as.numeric(student.survey$re) and as.numeric(student.survey$pi)\r\nt = 5.4163, df = 58, p-value = 1.221e-06\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 0.3818345 0.7265650\r\nsample estimates:\r\n      cor \r\n0.5795661 \r\n\r\nDue to the political ideology and religiosity variables being categorical, we need to use the as.numeric argument in the linear model to convert the variables into numerical data. At a significance level of .01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\r\n\r\n\r\n# inferential analysis: hours of tv and high school gpa\r\nlmtvhi <- lm(data = student.survey, formula = hi ~ tv)\r\nsummary(lmtvhi)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = hi ~ tv, data = student.survey)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.2583 -0.2456  0.0417  0.3368  0.7051 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\r\ntv          -0.018305   0.008658  -2.114   0.0388 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4467 on 58 degrees of freedom\r\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \r\nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\r\n\r\n# looking at Pearson's correlation\r\ncor.test(student.survey$tv, student.survey$hi)\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  student.survey$tv and student.survey$hi\r\nt = -2.1144, df = 58, p-value = 0.03879\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n -0.48826914 -0.01457694\r\nsample estimates:\r\n       cor \r\n-0.2675115 \r\n\r\nWith a slope of -.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of .05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatterplot with hours of tv watched a GPA, since there does not appear to be a linear trend in the data.\r\nQuestion 6:\r\n(Problem 9.50 in SMSS) For a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\nAnswer to Question 6:\r\nApplying the concept of regression toward the mean to this example, the low midterm scores in the sample could be explained as being an extreme in the sample by chance. Thus, in the next sample (in this case the final exam), we can expect that those 10 students’ scores will be closer to the mean this time (which remained to be 70, an average larger than the tutored students’ midterm score). Thus, we cannot conclude that the tutoring program was the cause of increase in the 10 students’ test scores.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscommegangeorgesdacss603-hw2/distill-preview.png",
    "last_modified": "2022-03-23T19:46:54-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomtpaske878207/",
    "title": "Homework #2",
    "description": "A new article created using the Distill format.",
    "author": [
      {
        "name": "Tyler J Paske",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-03-23",
    "categories": [],
    "contents": "\r\n(Problem 1.1 in ALR)\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, (the gross national product per person in U.S. dollars), and fertility, (the birth rate per 1000 females), both from the year 2009.The data is for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries.The data was collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.Identify the predictor and the response.“x” The Predictor in this case would be the ppgdp (gross national product per person) “y” and The Response is fertility (the birth rate per 1000 female)\r\nQuestion 1.1\r\nIdentify the predictor and the response.\r\n                                                  ANSWER\r\nI started by first loading the required package alr4 to which I then was able to plot my data as shown below.\r\nCode: ##plot(x = UN11\\(ppgdp, y = UN11\\)fertility)\r\n\r\n\r\n\r\nQuestion 1.2\r\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n                                                  ANSWER\r\nAs the plotted data below demostates a natural linear regression I would conclude that yes the simple linear regression model does seem plausible for a summary of this graph.\r\nCode: ##plot(log(UN11\\(fertility)~log(UN11\\)ppgdp), ylab =“UN11\\(fertility\", xlab = \"UN11\\)ppgdp”)\r\n\r\n\r\n\r\nQuestion 1.3\r\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\n                                                ANSWER\r\nIt seems as though that the graph above as a result shows a good summary as the average or the mean of the data supports linearity except for a few outliers.\r\nQuestion 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n(a)\r\nHow, if at all, does the slope of the prediction equation change?\r\n                                               ANSWER\r\nThe prediction equation: yˆ = a + bx, provides a prediction yˆ for every value of x or in other words for every value of x, we can calculate a y value. x being the explanatory variable (British pound) and yˆ being the estimated response outcome aka the prediction. In conclusion the slope of the prediction equation changes by whatever the prediction value of “y” is.\r\n(b)\r\nHow, if at all, does the correlation change?\r\n                                               ANSWER\r\nThe correlation changes between variables as one of the variables changes in value. To test the correlation, one needs to make predictions that are testable. Depending on the prediction, the correlation can change to a negative or positive correlation. In summary the correlation changes by the prediction.\r\nQuestion 3\r\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\n\r\n                                             ANSWER\r\nYear appears to be largely unrelated to each of the other variables; All streams seem to positively correlate with each other however all streams starting with O don’t seem to correlate with streams starting with A showing that the water can be correlated to other streams but can’t be predicted from past years of where the water comes from.\r\nQuestion 4\r\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n                                           ANSWER\r\nWe start by loading the data and reviewing the matrix\r\n##data(Rateprof)\r\n##pairs(Rateprof)\r\nAs the matrix stands we need to further scrape the data to reproduce the scatterplot matrix in Figure 1.13 in the ALR book. To do this we need to further identify the data frame that we want to work with as the there too much data currently within the matrix. The data frame itself holds 17 columns, we only need the 8th – 12th. So we use the code below to give us the reproduced scatterplot matrix.\r\nCode: pairs(Rateprof[,8:12])\r\n\r\n\r\n\r\nQuestion 5\r\nFor the student.survey data file in the smss package, conduct regression analyses relating\r\n(i)\r\ny = political ideology and x = religiosity,\r\nStep 1 • Install the package & the data file\r\n\r\nlibrary(smss)\r\n\r\n\r\ndata(“student.survey”)\r\n\r\nBoth variables contain categorical data.\r\n• Graph the variables and their relationship.\r\n                                      Can't Grpah within chunk\r\nCode: >ggplot(data = student.survey, aes(x = re, y = pi)) + geom_point() + xlab(“Religiosity”) + ylab(“Political Ideology”)\r\n(ii)\r\ny = high school GPA and x = hours of TV watching.\r\n                                         Can't Grpah within chunk\r\n                                         \r\nCode: >ggplot(data = student.survey, aes(x = tv, y = hi)) + geom_point() + xlab(“Hours of TV watching”) + ylab(“High school GPA”)\r\nAs we can clearly see from the graph above there’s a big correlation between the number of hours a high school student watches tv and their GPA in high school. The graph above shows that the fewer hours a student in high school watches TV the higher his/her GPA becomes.\r\n(a)\r\nUse graphical ways to portray the individual variables and their relationship.\r\nANSWER\r\nIn terms of the relationship between political ideology and religiosity, it seems as though graphically there’s a positive relationship between the two in that the more a student practices religion the more conservative he/she becomes over the course of a week.\r\n(b)\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\nCODE: >summary(student.survey)\r\n                                   Can't Grpah within chunk\r\nANSWER\r\nAfter running a summary of the student survey data, I was able to find many descriptive statistics to summarize the individual variable. First being in political ideology where I found that 63% of all 60 observations showed to be that the students were at some degree of liberal. Additionally, I also found that 73% of students either never went to church or occasionally went to church. With only 27% of the all the students that go to church most weeks and every week we can start to get a much better sense of the relationship between the two. Should more students go to church most to every week then we would probably see a much bigger favor of students being conservative.\r\nIn terms of the relationship between High School GPA and Hours of watching TV, the descriptive statistics shown from a summary show that on overage students with a 3.3GPA watch an average a little more than 7 hours a week.\r\n(c)\r\nSummarize and interpret results of inferential analyses.\r\nANSWER\r\nTo further explore this data (number of hours a high school student watches tv and their GPA) in terms of an inferential analysis, I turn to a summary of two variable at a time. Doing so would give me insight to the T – Value and P Score. Calculated from the sample data in a hypothesis test we calculate the test statistic under the null hypothesis. We find that there’s a very low T- Value (-2.11). It shows that it has no significance for the difference between the two variables. The smaller the t – score the more the groups hold a similarity which was confirmed and articulated from the visualization.\r\nThe p-value being .04 rounded indicates that the result is statistically significant. In this case we would reject the null hypothesis and lean in favor of the alternative hypothesis. Being in favor of the alternative hypothesis we can conclude that there is some statistical significance between the two measured phenomenon.\r\nCODE: >cor.test(student.survey\\(tv, student.survey\\)hi)\r\nIn terms of the relationship between political ideology and religiosity there has to be a numerical vector in order to conduct an inferential analysis.\r\n\r\nlibrary(smss)\r\n\r\n\r\ndata(“student.survey”)\r\n\r\n\r\ncor.test(student.survey\\(re, student.survey\\)pi)\r\n\r\nError in cor.test.default(student.survey\\(re, student.survey\\)pi) : ‘x’ must be a numeric vector\r\nQuestion 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60.\r\nUse the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\n                                            ANSWER\r\nThis is not sufficient evidence because what we’re doing is simply taking the average of the average. The overall class average is 70 for two tests but the specially tutored students increase from 50 to 60. How do we know if the specially tutored students would have had the same result not tutored? What if the tutoring had no effect on the students whatsoever? What we’re discussing is regression to the mean and regression to the mean shows that should you have a bad result in say a test, there’s a high likelihood that your results will be better the next time around. However, we shouldn’t expect special tutored students to be better or worse the second time around because of random chance variables that are unforeseen. As mentioned from the video, this is why it’s so important to say use control groups in clinical trials. This way we can see if drugs work better or worse by random chance.\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomtpaske878207/distill-preview.png",
    "last_modified": "2022-03-23T19:47:00-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomowenvespa874506/",
    "title": "Simple Linear Regression",
    "description": "DACSS 603 Homework 2",
    "author": [
      {
        "name": "Rhowena Vespa",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\r\n\r\n\r\nlibrary(distill)\r\nlibrary(dplyr)\r\nlibrary(tidyverse)\r\nlibrary(knitr)\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nlibrary(smss)\r\n\r\n\r\n\r\nQuestion 1\r\n(Problem 1.1 in ALR)\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n\r\n\r\ndata(\"UN11\") # load the UN11 data\r\nUN11 <- UN11 %>%\r\n  select(c(fertility,ppgdp)) # select the two variables to use \r\ndim(UN11)\r\n\r\n\r\n[1] 199   2\r\n\r\nkable(head(UN11), format = \"markdown\", digits = 10, caption = \"**Dependence of Fertility on ppgdp**\")\r\n\r\n\r\nTable 1: Dependence of Fertility on ppgdp\r\n\r\nfertility\r\nppgdp\r\nAfghanistan\r\n5.968\r\n499.0\r\nAlbania\r\n1.525\r\n3677.2\r\nAlgeria\r\n2.142\r\n4473.0\r\nAngola\r\n5.135\r\n4321.9\r\nAnguilla\r\n2.000\r\n13750.1\r\nArgentina\r\n2.172\r\n9162.1\r\n\r\n1.1.1. Identify the predictor and the response. \r\nPredictor is ppgdp, response is fertility.\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\nSee Figure 1.1 below. From this graph, a straight line mean function DOES NOT SEEM PLAUSIBLE for a summary of this graph. It looks like there is a negative correlation between ppgdp and fertility but we will need the lm function to visualize it.\r\n\r\n\r\nggplot(UN11, aes(x = ppgdp, y = fertility)) +\r\n    geom_point(color=2) + \r\n    labs(x=\"ppgdp-Gross National Product Per Person in U.S. dollars\", y=\"fertility-birth rate per 1000 females\", title = \"FIGURE 1.1 UN ppgdp vs fertility data in 2009\")\r\n\r\n\r\n\r\n\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nSee Figure 1.2 below. Yes, a simple linear regression model looks more plausible on this graph. Using the log function produced a graph with a more linear relationship between ppgdp and fertility.\r\n\r\n\r\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\r\n    geom_point(color=2) + \r\n    geom_smooth(method = \"lm\") +\r\n    labs(x=\"ppgdp-Gross National Product Per Person in U.S. dollars\", y=\"fertility-birth rate per 1000 females\", title = \"FIGURE 1.2 UN ppgdp and fertility data in 2009\")\r\n\r\n\r\n\r\n\r\nQuestion 2\r\n(Problem 9.47 in SMSS)\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n(a) How, if at all, does the slope of the prediction equation change?\r\nThe slope will change when responses are converted to British pounds. The new slope of the prediction equation when explanatory variable is in British pounds will be LESS than original slope (in US dollars).\r\n(b) How, if at all, does the correlation change?\r\nNo, a change in units on the explanatory variables from US Dollar to British pounds will not result in a correlation change. Correlation DOES NOT depend on the variable’s units.\r\nQuestion 3\r\n(Problem 1.5 in ALR)\r\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\ndata(\"water\") # load the water data\r\ndim(water)\r\n\r\n\r\n[1] 43  8\r\n\r\nkable(head(water), format = \"markdown\", digits = 10, caption = \"**Water runoff in the Sierras**\")\r\n\r\n\r\nTable 2: Water runoff in the Sierras\r\nYear\r\nAPMAM\r\nAPSAB\r\nAPSLAKE\r\nOPBPC\r\nOPRC\r\nOPSLAKE\r\nBSAAM\r\n1948\r\n9.13\r\n3.58\r\n3.91\r\n4.10\r\n7.43\r\n6.47\r\n54235\r\n1949\r\n5.28\r\n4.82\r\n5.20\r\n7.55\r\n11.11\r\n10.26\r\n67567\r\n1950\r\n4.20\r\n3.77\r\n3.67\r\n9.52\r\n12.20\r\n11.35\r\n66161\r\n1951\r\n4.60\r\n4.46\r\n3.93\r\n11.14\r\n15.15\r\n11.13\r\n68094\r\n1952\r\n7.15\r\n4.99\r\n4.88\r\n16.34\r\n20.05\r\n22.81\r\n107080\r\n1953\r\n9.70\r\n5.65\r\n4.91\r\n8.88\r\n8.15\r\n7.41\r\n67594\r\n\r\npairs(water,col = 2, main = \"Water Runoff in Sierras Scatterplot Matrix\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nwater1 <-lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water)\r\nsummary(water1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \r\n    OPSLAKE, data = water)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-12690  -4936  -1424   4173  18542 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\r\nAPMAM         -12.77     708.89  -0.018 0.985725    \r\nAPSAB        -664.41    1522.89  -0.436 0.665237    \r\nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \r\nOPBPC          69.70     461.69   0.151 0.880839    \r\nOPRC         1916.45     641.36   2.988 0.005031 ** \r\nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 7557 on 36 degrees of freedom\r\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \r\nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\r\n\r\nANALYSIS:\r\nOn residuals, although there is a big disparity between the min and max (-12690 and 18542), which can be possible outliers, I think the data is relatively balanced because the 1Q and 3Q (-4936 and 4173) are close in values.\r\nOn coefficients, sites OPRC and OPSLAKE has statistically significant values of Pr(>|t|) < 0.05, indicating that these two locations’ precipitation measurements are significant to BASAAM stream runoff volume.\r\nMultiple R-squared 0.9248 and adjusted R-squared 0.9123 are relatively close, suggesting model in not over-fitted. An adjusted R-squared of 0.9123 indicates a good fit for the model.\r\nLastly, with a p-value: < 2.2e-16, we can conclude that this model is statistically significant. This model can be used to predict runoff so engineers, planners, and policy makers could do their jobs more efficiently.\r\nQuestion 4\r\n(Problem 1.6 in ALR - slightly modified)\r\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20).\r\n\r\n\r\ndata(Rateprof)\r\nRateprof <- Rateprof %>%\r\n  select(c(quality,helpfulness,clarity,easiness,raterInterest)) # select the five variables to use\r\ndim(Rateprof)\r\n\r\n\r\n[1] 366   5\r\n\r\nkable(head(Rateprof), format = \"markdown\", digits = 10, col.names = c('Quality','Helpfulness','Clarity', 'Easiness', 'Rater Interest'), caption = \"**Professor Ratings**\")\r\n\r\n\r\nTable 3: Professor Ratings\r\nQuality\r\nHelpfulness\r\nClarity\r\nEasiness\r\nRater Interest\r\n4.636364\r\n4.636364\r\n4.636364\r\n4.818182\r\n3.545455\r\n4.318182\r\n4.545455\r\n4.090909\r\n4.363636\r\n4.000000\r\n4.790698\r\n4.720930\r\n4.860465\r\n4.604651\r\n3.432432\r\n4.250000\r\n4.458333\r\n4.041667\r\n2.791667\r\n3.181818\r\n4.684211\r\n4.684211\r\n4.684211\r\n4.473684\r\n4.214286\r\n4.233333\r\n4.266667\r\n4.200000\r\n4.533333\r\n3.916667\r\n\r\npairs(Rateprof,col = 2, main = \"Professor Ratings Scatterplot Matrix\")\r\n\r\n\r\n\r\n\r\nProvide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\nBased on this scatterplot matrix, we can observe a very strong linear positive correlation between: quality and helpfulness, quality and clarity, helpfulness and clarity.\r\nThere is moderate positive linear correlation between easiness and quality, clarity or helpfulness.\r\nThere is moderate positive linear correlation between raterinterest and clarity, helpfulness or quality\r\nEasiness and raterinterest have a weak positive linear association.\r\n\r\n\r\nRP <-cor(Rateprof, use = \"all.obs\",method = c(\"pearson\", \"kendall\", \"spearman\"))\r\nkable ((RP), format = \"markdown\", digits = 10, col.names = c('Quality','Helpfulness','Clarity', 'Easiness', 'Rater Interest'), caption = \"**Correlation Matrix**\")\r\n\r\n\r\nTable 4: Correlation Matrix\r\n\r\nQuality\r\nHelpfulness\r\nClarity\r\nEasiness\r\nRater Interest\r\nquality\r\n1.0000000\r\n0.9810314\r\n0.9759608\r\n0.5651154\r\n0.4706688\r\nhelpfulness\r\n0.9810314\r\n1.0000000\r\n0.9208070\r\n0.5635184\r\n0.4630321\r\nclarity\r\n0.9759608\r\n0.9208070\r\n1.0000000\r\n0.5358884\r\n0.4611408\r\neasiness\r\n0.5651154\r\n0.5635184\r\n0.5358884\r\n1.0000000\r\n0.2052237\r\nraterInterest\r\n0.4706688\r\n0.4630321\r\n0.4611408\r\n0.2052237\r\n1.0000000\r\n\r\nQuestion 5\r\n(Problem 9.34 in SMSS)\r\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\n\r\n\r\ndata(\"student.survey\") # load the student.survey data\r\nstudent.survey <- student.survey %>%\r\n  select(c(re,pi,hi,tv)) # select the four variables to use \r\ndim(student.survey)\r\n\r\n\r\n[1] 60  4\r\n\r\nkable(head(student.survey), format = \"markdown\", digits = 2, col.names = c('Religiosity','Political Ideology','HS GPA', 'Hrs of watching TV'), caption = \"**Student Survey Data**\")\r\n\r\n\r\nTable 5: Student Survey Data\r\nReligiosity\r\nPolitical Ideology\r\nHS GPA\r\nHrs of watching TV\r\nmost weeks\r\nconservative\r\n2.2\r\n3\r\noccasionally\r\nliberal\r\n2.1\r\n15\r\nmost weeks\r\nliberal\r\n3.3\r\n0\r\noccasionally\r\nmoderate\r\n3.5\r\n5\r\nnever\r\nvery liberal\r\n3.1\r\n6\r\noccasionally\r\nliberal\r\n3.5\r\n4\r\n\r\nA. Graph of Individual Variables and their Relationship\r\n(i) y = political ideology and x = religiosity\r\n\r\n\r\nggplot(student.survey, aes(x=re,ymin = 0, ymax = 30, fill=pi)) +\r\n  geom_bar() +\r\n  labs(x=\"Religiosity\", y=\"Political Ideology\", \r\n  title = \"FIGURE 2. Political Ideology and Religiosity\") +\r\n  facet_wrap(vars(re,pi),strip.position = \"left\") +\r\n  theme(axis.text.x = element_text(size = 8, angle = 90)) \r\n\r\n\r\n\r\n\r\n\r\n\r\nstudent.survey %>%\r\n    count(pi,re, sort = TRUE) %>%\r\n  kable(head(10), format = \"markdown\", digits = 10, col.names = c('Political Ideology','Religiosity','Number of Students'), caption = \"**Political Ideology and Religiosity Matrix Count**\")\r\n\r\n\r\nTable 6: Political Ideology and Religiosity Matrix Count\r\n\r\nPolitical Ideology\r\nReligiosity\r\nNumber of Students\r\n1\r\nliberal\r\noccasionally\r\n14\r\n2\r\nliberal\r\nnever\r\n8\r\n3\r\nmoderate\r\noccasionally\r\n8\r\n4\r\nvery liberal\r\noccasionally\r\n5\r\n5\r\nvery liberal\r\nnever\r\n3\r\n6\r\nslightly liberal\r\nnever\r\n2\r\n7\r\nslightly liberal\r\nevery week\r\n2\r\n8\r\nslightly conservative\r\nmost weeks\r\n2\r\n9\r\nslightly conservative\r\nevery week\r\n2\r\n10\r\nconservative\r\nmost weeks\r\n2\r\n11\r\nconservative\r\nevery week\r\n2\r\n12\r\nvery conservative\r\nevery week\r\n2\r\n13\r\nliberal\r\nmost weeks\r\n1\r\n14\r\nliberal\r\nevery week\r\n1\r\n15\r\nslightly liberal\r\noccasionally\r\n1\r\n16\r\nslightly liberal\r\nmost weeks\r\n1\r\n17\r\nmoderate\r\nnever\r\n1\r\n18\r\nmoderate\r\nmost weeks\r\n1\r\n19\r\nslightly conservative\r\nnever\r\n1\r\n20\r\nslightly conservative\r\noccasionally\r\n1\r\n\r\n(ii) y = high school GPA and x = hours of TV watching.\r\n\r\n\r\nggplot(student.survey, aes(x = tv, y = hi)) +\r\n    geom_point(color=2) + \r\n    geom_smooth(method = \"lm\") +\r\n    labs(x=\"Hrs of TV\", y=\"HS GPA\", title = \"FIGURE 3. HS GPA and Hrs of TV\")\r\n\r\n\r\n\r\n\r\nB. Interpret Descriptive Statistics of Variables and their Relationship.\r\n\r\n\r\nsummary(student.survey)\r\n\r\n\r\n            re                         pi           hi       \r\n never       :15   very liberal         : 8   Min.   :2.000  \r\n occasionally:29   liberal              :24   1st Qu.:3.000  \r\n most weeks  : 7   slightly liberal     : 6   Median :3.350  \r\n every week  : 9   moderate             :10   Mean   :3.308  \r\n                   slightly conservative: 6   3rd Qu.:3.625  \r\n                   conservative         : 4   Max.   :4.000  \r\n                   very conservative    : 2                  \r\n       tv        \r\n Min.   : 0.000  \r\n 1st Qu.: 3.000  \r\n Median : 6.000  \r\n Mean   : 7.267  \r\n 3rd Qu.:10.000  \r\n Max.   :37.000  \r\n                 \r\n\r\nINTERPRETATION:\r\nReligiosity: Mode is Occasionally (count: 29) Political Ideology:Mode is liberal (count:24). Based on this, a visualization of re vs pi (see figure 2) showed that this population had mostly occasional religiosity and liberal ideology as the mode.\r\nHS GPA: Mean GPA was 3.3 and very close to median of 3.35. The min GPA was 2.0 while max of 4.0. Hrs of Watching TV: On average, HS students watch TV for 7.267 hrs but most students watch for 6 hours. The min was 0 hrs while the maximum was 37 hrs. Based on Figure 3, there is an association between HS GPA and Hrs of TV watched.\r\nC. Summarize and Interpret results of Inferential Analyses\r\n(i) LOGISTIC REGRESSION: y = political ideology and x = religiosity\r\n\r\n\r\nSS.glm.fit <- glm(re ~ pi, data = student.survey, family = binomial)\r\nsummary(SS.glm.fit)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = re ~ pi, family = binomial, data = student.survey)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-2.1460  -0.3500   0.5314   0.9005   0.9695  \r\n\r\nCoefficients:\r\n             Estimate Std. Error z value Pr(>|z|)\r\n(Intercept)    5.8337   489.4505   0.012    0.990\r\npi.L          16.2199  1753.3896   0.009    0.993\r\npi.Q           8.1491  1526.1299   0.005    0.996\r\npi.C          -0.2996  1398.7211   0.000    1.000\r\npi^4          -4.6817  1304.7376  -0.004    0.997\r\npi^5          -5.0032   915.6782  -0.005    0.996\r\npi^6          -3.3188   401.1467  -0.008    0.993\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 67.480  on 59  degrees of freedom\r\nResidual deviance: 60.684  on 53  degrees of freedom\r\nAIC: 74.684\r\n\r\nNumber of Fisher Scoring iterations: 16\r\n\r\nINTERPRETATION\r\nBased on all the p-values, we fail to reject the null hypothesis and conclude that there is no association between religiosity and political ideology.\r\n(ii) LINEAR REGRESSION: y = high school GPA and x = hours of TV watching\r\n\r\n\r\nstudent.surveyii <-lm(hi ~ tv, data = student.survey)\r\nsummary(student.surveyii)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = hi ~ tv, data = student.survey)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.2583 -0.2456  0.0417  0.3368  0.7051 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\r\ntv          -0.018305   0.008658  -2.114   0.0388 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4467 on 58 degrees of freedom\r\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \r\nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\r\n\r\n\r\n\r\nstudent.survey1 <- student.survey %>%\r\n  select(c(hi,tv))\r\ncor(student.survey1)\r\n\r\n\r\n           hi         tv\r\nhi  1.0000000 -0.2675115\r\ntv -0.2675115  1.0000000\r\n\r\nINTERPRETATION\r\nBased on the p-value 0.0388, we reject the null hypothesis. Therefore, we can conclude that there is statistically significant association between HS GPA and hours of watching TV. However, with very low R-squared (0.07156), this model is not a good fit to explain variations in the data.\r\nBased on the correlation values, we can conclude a weak negative association between HS GPA and hrs of watching TV.\r\nQuestion 6\r\n(Problem 9.50 in SMSS)\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\r\nEXPLANATION:\r\nChoose 10 lowest scoring students (mean=50) and their scores will regress upward AFTER a tutoring program, NOT BECAUSE OF. These students who did poorly were unlucky. It is highly probable that these students would have increased their final exam scores (mean =60), with or without the tutoring program. The effect of the tutoring program on the test scores is INCONCLUSIVE. The same rationale can be used regarding highest scoring students during midterms. Chances are the same high performing students who scored higher during midterms will then score closer to the mean (mean=70) during their final exams. Both the lowest scoring students and the highest scoring students had their scores REGRESS TOWARDS THE MEAN of 70 in the final exam.\r\nTo determine if the tutoring program is effective in improving test scores, the design of the study should choose the sample student population RANDOMLY, and conduct the test in a CONTROLLED environment. Otherwise, we must consider regression to the mean as the rationale for the increase in test scores of the lowest performing group.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomowenvespa874506/distill-preview.png",
    "last_modified": "2022-03-12T19:36:09-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomemersonflemi875398/",
    "title": "EFleming DACSS 603 HW II",
    "description": "The following document contains my work for DACSS 603 HW II.",
    "author": [
      {
        "name": "EFleming",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nQuestion 1\n(Problem 1.1 in ALR)\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\nAnswer\nThe predictor is ppgdp and the response is fertility. Fertility depends on ppgdp. This question is a bit tricky and had me confused at first as it is easy to make the mistake of making fertility the predictor influencing “y.” However, based on my logic, I would argue it is the inverse as my answer states.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\nAnswer\n\n\nattach(UN11)\nplot(fertility~ppgdp, data=UN11)\n\n\n\n\n*The relationship does not seem linear. There is an inverse and therefore negative relationship. A straight line function would not be plausible in this case. We cannot draw a solid straight line without making changes to the variables themselves as we can with logs (which we will use next).\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nAnswer\n\n\nattach(UN11)\nplot(log(fertility)~log(ppgdp), data=UN11)\n\n\n\n\n*Yes, the log transformation created two variables that are linearly related. The log helps create a better fit for our data. With the logs, we transform the variables. The line generated using the lines is much better and more smooth when we implement the logs. The log transformation creates two variables that are linearly related. We can use simple linear regression to estimate the effect of log(ppgdp on log(fertility).\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\nHow, if at all, does the slope of the prediction equation change?\nAnswer\nWe want to take the income in dollars and make it pounds. In other words $1330 = 1.33 x 1000 for example. I\ny= B0 + B1 Inc($) + E y= B0 + B1(Inc(pounds x 1.33)) + E\nTherefore it can be written as: y= B0 + 1.33B1Inc(pounds) + E ^^^We get a new slope which is 1.33 times the old slope\nHow, if at all, does the correlation change?\nAnswer\nThe correlation does not change at all.\nQuestion 3\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\nWe want to see how correlated each of the sites are to one another in order to see if this can help us predict Steam Runoff.\nAnswer\n\n\nplot(BSAAM~APMAM, data = water) \n\n\n\n\n\n\nplot(BSAAM~APSAB,data = water)\n\n\n\n\n\n\nplot(BSAAM~APSLAKE,data = water)\n\n\n\n\n\n\nplot(BSAAM~OPBPC, data = water)\n\n\n\n\n\n\nplot(BSAAM~OPRC, data = water)\n\n\n\n\n\n\nplot(BSAAM~OPSLAKE, data = water)\n\n\n\n\nThere is a relatively strong and positive linear relationship between stream run off and precipitation measures taken at “OPBPC,” “OPRC,” and “OPSLake.” There is no relationship between the others. Here we are running a bi-variate comparison for BSAAM–as this particular site helps us determine runoff when used in conjunction with other individual sites.\nQuestion 4\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\nAnswer\n\n\ndata(\"Rateprof\")\npairs(Rateprof[,8:12], pch=19)\n\n\n\n\n“Quality~Helpfulness,” “Quality~Clarity” and “Helpfulness~Clarity” paris have strong positive correlations and all other pairs have a small positive correlation except (“raterInterest” ~ “Easiness”) which seems to have almost zero correlation. Here, we use the pairs() function to compare the correlation between variables in a bi-variate manner without having to incorporate multiple linear regression (which can get a bit messy very quickly with too many variables). We can recreate the nice table given to us in the text book very simply by using the code above.\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n(a) Use graphical ways to portray the individual variables and their relationship.\nAnswer (i)\nFor the correlation between political ideology and religiosity, we use the xtabs() function which allows us to compare the relationship between 2 categorical variables.\nWe also use a cor.test and run both as.numeric as they are ordinal. We can interpret this as people that are more religious tend to be more conservative in their political views. We have a statistically significant correlation with a small\n\n\ndata(\"student.survey\")\nxtabs(~re + pi, student.survey)\n\n\n              pi\nre             very liberal liberal slightly liberal moderate\n  never                   3       8                2        1\n  occasionally            5      14                1        8\n  most weeks              0       1                1        1\n  every week              0       1                2        0\n              pi\nre             slightly conservative conservative very conservative\n  never                            1            0                 0\n  occasionally                     1            0                 0\n  most weeks                       2            2                 0\n  every week                       2            2                 2\n\ncor.test(as.numeric(student.survey$re), as.numeric(student.survey$pi))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  as.numeric(student.survey$re) and as.numeric(student.survey$pi)\nt = 5.4163, df = 58, p-value = 1.221e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3818345 0.7265650\nsample estimates:\n      cor \n0.5795661 \n\n\n\nrmean = aggregate(as.numeric(student.survey$re), by = list(student.survey$pi), mean)\n\n\n\n^^The code above gives average values for religiosity for each political view on the liberal to conservative scale.\n\n\nplot(rmean)\n\n\n\n\n^^The graphical representation a basic representation of how things look with regards to political ideology and religiosity. If we want to take it a step further, we can use the “error.bar.by” function included in the “psych” package. This plot will be provided below.\n\n\nerror.bars.by(as.numeric(student.survey$pi),student.survey$re, ylab = \"Conservativeness\", v.labels=c(\"never\", \"occasionally\", \"most weeks\", \"every week\"))\n\n\n\n\nThe lines connect the means using this plot. This graphically explains those that attend church more often have higher levels of conservativeness. At the very least, there is a moderate to high correlation.\nAnswer (ii)\n\n\ndata(\"student.survey\")\nplot(~tv + hi, student.survey, col = \"red\")\n\n\n\n\nFor the correlation between high school GPA and hours spent watching TV, we use the plot function as both “tv” & “hi” are numerical values making it possible to plot their relationship.\n\n\ndata(\"student.survey\")\nplot(~tv + hi, student.survey, col = \"red\")\n\n\n\n\n(b) Interpret descriptive statistics for summarizing the individual variables and their relationship.\nAnswer (i)\nHere, we are comparing the political ideology and how often people attend religious services. As we can see, a much larger percentage of people to the left of moderate attend church occasionally or do not attend church at all. Moderates have a very similar relationship with regards to more liberal individuals and their church attending activity. As for conservatives, there is a larger percentage of those that attend church “most week” or “every week.” However, one issue is the fact that at first glance, there are more entries for people that are moderates or liberals than for those that are conservative. Therefore, this somewhat skews the interpretation of the data.\nAnswer (ii)\nThere does appear to be at least somewhat of a correlation between GPA and hours spent watching TV. However, because we cannot draw a smooth line without most likely implementing logs to help smooth things out. Therefore, we cannot say a considerable correlation exists. Instead, we have a weaker negative correlation that exists.\n\n\ndata(\"student.survey\")\nlm(tv~hi, data = student.survey)\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nCoefficients:\n(Intercept)           hi  \n     20.200       -3.909  \n\n(c) Summarize and interpret results of inferential analyses.\nAnswer (i)\n\n\ndata(\"student.survey\")\nxtab = table(student.survey$re, student.survey$pi)\nchisq.test(xtab)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  xtab\nX-squared = 42.288, df = 18, p-value = 0.001008\n\nHere, we are able to accept our alternative hypothesis and reject our null hypothesis because the p-value is very small. This means that events are dependent on one another. For two ordinal variables, it is somewhat tricky with regards to testing how correlated to one another they really are. We could technically simply relabel the ordinal data as numerical data (as we did) to compare this data more readily.\nAnswer (ii)\n\n\ndata(\"student.survey\")\ncor.test(student.survey$tv, student.survey$hi)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  student.survey$tv and student.survey$hi\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nBecause these two variables are numerical, we can run a correlation test where Pearson’s is being used by default. Here, we can reject the null hypothesis and accept our alternative hypothesis because the p-value is below .05. Here we have a very weak negative correlation for “r.” This was indicated graphically above. We also have a t-value slightly greater than -2 which gives us more evidence of the correlation being exhibited.\nQuestion 6\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video:\nAnswer\nConceptually, correlation does not prove causation! There are a myriad of other factors that could act as confounding variables which could have affected higher scores in the tutored class. This conclusion can quickly fall into the category of omitted variable bias. In order to avoid confirmation bias, we should be looking for any other variables that could have possibly had an impact on the class that was tutored. The problem here is that we are not controlling for any of the other factors.\n\n\n\n",
    "preview": "posts/httprpubscomemersonflemi875398/distill-preview.png",
    "last_modified": "2022-03-12T19:36:34-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomektracy875966/",
    "title": "HOMEWORK 2",
    "description": "DACSS 603, Spring 2022",
    "author": [
      {
        "name": "Erin Tracy",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\r\nQUESTION 1\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n1.1.1. Identify the predictor and the response.\r\nPredictor (x): PPGDP\r\nResponse (y): Fertility\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph.\r\n\r\n\r\nlibrary(alr4)\r\n?UN11\r\n\r\nplot(x=UN11$ppgdp, y=UN11$fertility)\r\n\r\n\r\n\r\n\r\nThe scatterplot seems to suggest that the higher the PPGDP, the lower the number of children per woman.\r\nDoes a straight-line mean function seem to be plausible for a summary of this graph?\r\n\r\n\r\nggplot(data=UN11, aes(x=ppgdp, y=fertility))+\r\n  geom_point()+\r\n  geom_smooth(method=\"lm\", se=FALSE)\r\n\r\n\r\n\r\n\r\nYes, straight line does suggest there is function.\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\n\r\n\r\nUN11_log_ppgdp<- log(UN11$ppgdp)\r\nUN11_log_fertility<- log(UN11$fertility)\r\n\r\nplot(x=UN11_log_ppgdp, y=UN11_log_fertility)\r\n\r\n\r\n\r\n\r\nThe scatterplot using logs still suggests that number of children per woman decreases as PPGDP increases,but this new model represents the data more cleanly.\r\nQUESTION 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n(a) How, if at all, does the slope of the prediction equation change?\r\n\r\n\r\nusdollar<- (1:10)\r\npound<- seq(1.33,13.3, length.out = 10)\r\n\r\nslope<-(usdollar/pound)\r\nslope\r\n\r\n\r\n [1] 0.7518797 0.7518797 0.7518797 0.7518797 0.7518797 0.7518797\r\n [7] 0.7518797 0.7518797 0.7518797 0.7518797\r\n\r\nI didn’t initially know how to solve this, so I put together sample of values for dollars and pounds. I calculated the slope and can see that it will not change.\r\n(b) How, if at all, does the correlation change?\r\n\r\n\r\ncor.test(usdollar,pound)\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  usdollar and pound\r\nt = 189812531, df = 8, p-value < 2.2e-16\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 1 1\r\nsample estimates:\r\ncor \r\n  1 \r\n\r\nThe correlation test of the sample data shows that the correlation is 1- perfect correlation. It makes sense that the correlation will never change.\r\nQUESTION 3\r\nWaterrunoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\n?water\r\n\r\nsummary(water)\r\n\r\n\r\n      Year          APMAM            APSAB           APSLAKE     \r\n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \r\n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \r\n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \r\n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \r\n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \r\n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \r\n     OPBPC             OPRC           OPSLAKE           BSAAM       \r\n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \r\n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \r\n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \r\n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \r\n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \r\n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \r\n\r\nhead(water)\r\n\r\n\r\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\r\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\r\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\r\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\r\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\r\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\r\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\r\n\r\npairs(~APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data=water)\r\n\r\n\r\n\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$APMAM)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$APSAB)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$APSLAKE)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$OPBPC)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$OPRC)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$OPSLAKE)\r\n\r\n\r\n\r\n\r\nI successfully created a scatterplot matrix above, but I had trouble reading the data so I also ran each of the scatterplots separately. There seems to be very strong correlation for OPBPC, OPRC and OPSLAKE between snowfall in inches and stream runoff. For the other sites, more snowfall does seem to indicate more stream runoff, but the correlation is not as great.\r\nQUESTION 4\r\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n\r\n\r\nsummary(Rateprof)\r\n\r\n\r\n    gender       numYears        numRaters       numCourses    \r\n female:159   Min.   : 1.000   Min.   :10.00   Min.   : 1.000  \r\n male  :207   1st Qu.: 6.000   1st Qu.:15.00   1st Qu.: 3.000  \r\n              Median :10.000   Median :24.00   Median : 4.000  \r\n              Mean   : 8.347   Mean   :28.58   Mean   : 4.251  \r\n              3rd Qu.:11.000   3rd Qu.:37.00   3rd Qu.: 5.000  \r\n              Max.   :11.000   Max.   :86.00   Max.   :12.000  \r\n                                                               \r\n pepper       discipline          dept        quality     \r\n no :320   Hum     :134   English   : 49   Min.   :1.409  \r\n yes: 46   SocSci  : 66   Math      : 34   1st Qu.:2.936  \r\n           STEM    :103   Biology   : 20   Median :3.612  \r\n           Pre-prof: 63   Chemistry : 20   Mean   :3.575  \r\n                          Psychology: 20   3rd Qu.:4.250  \r\n                          Spanish   : 20   Max.   :4.981  \r\n                          (Other)   :203                  \r\n  helpfulness       clarity         easiness     raterInterest  \r\n Min.   :1.364   Min.   :1.333   Min.   :1.391   Min.   :1.098  \r\n 1st Qu.:3.069   1st Qu.:2.871   1st Qu.:2.548   1st Qu.:2.934  \r\n Median :3.662   Median :3.600   Median :3.148   Median :3.305  \r\n Mean   :3.631   Mean   :3.525   Mean   :3.135   Mean   :3.310  \r\n 3rd Qu.:4.351   3rd Qu.:4.214   3rd Qu.:3.692   3rd Qu.:3.692  \r\n Max.   :5.000   Max.   :5.000   Max.   :4.900   Max.   :4.909  \r\n                                                                \r\n   sdQuality       sdHelpfulness      sdClarity        sdEasiness    \r\n Min.   :0.09623   Min.   :0.0000   Min.   :0.0000   Min.   :0.3162  \r\n 1st Qu.:0.87508   1st Qu.:0.9902   1st Qu.:0.9085   1st Qu.:0.9045  \r\n Median :1.15037   Median :1.2860   Median :1.1712   Median :1.0247  \r\n Mean   :1.05610   Mean   :1.1719   Mean   :1.0970   Mean   :1.0196  \r\n 3rd Qu.:1.28730   3rd Qu.:1.4365   3rd Qu.:1.3328   3rd Qu.:1.1485  \r\n Max.   :1.67739   Max.   :1.8091   Max.   :1.8091   Max.   :1.6293  \r\n                                                                     \r\n sdRaterInterest \r\n Min.   :0.3015  \r\n 1st Qu.:1.0848  \r\n Median :1.2167  \r\n Mean   :1.1965  \r\n 3rd Qu.:1.3326  \r\n Max.   :1.7246  \r\n                 \r\n\r\nhead(Rateprof)\r\n\r\n\r\n  gender numYears numRaters numCourses pepper discipline\r\n1   male        7        11          5     no        Hum\r\n2   male        6        11          5     no        Hum\r\n3   male       10        43          2     no        Hum\r\n4   male       11        24          5     no        Hum\r\n5   male       11        19          7     no        Hum\r\n6   male       10        15          9     no        Hum\r\n               dept  quality helpfulness  clarity easiness\r\n1           English 4.636364    4.636364 4.636364 4.818182\r\n2 Religious Studies 4.318182    4.545455 4.090909 4.363636\r\n3               Art 4.790698    4.720930 4.860465 4.604651\r\n4           English 4.250000    4.458333 4.041667 2.791667\r\n5           Spanish 4.684211    4.684211 4.684211 4.473684\r\n6           Spanish 4.233333    4.266667 4.200000 4.533333\r\n  raterInterest sdQuality sdHelpfulness sdClarity sdEasiness\r\n1      3.545455 0.5518564     0.6741999 0.5045250  0.4045199\r\n2      4.000000 0.9020179     0.9341987 0.9438798  0.5045250\r\n3      3.432432 0.4529343     0.6663898 0.4129681  0.5407021\r\n4      3.181818 0.9325048     0.9315329 0.9990938  0.5882300\r\n5      4.214286 0.6500112     0.8200699 0.5823927  0.6117753\r\n6      3.916667 0.8632717     1.0327956 0.7745967  0.6399405\r\n  sdRaterInterest\r\n1       1.1281521\r\n2       1.0744356\r\n3       1.2369438\r\n4       1.3322506\r\n5       0.9749613\r\n6       0.6685579\r\n\r\n?Rateprof\r\n\r\npairs(~quality + clarity + helpfulness + easiness + raterInterest, data=Rateprof)\r\n\r\n\r\n\r\n\r\nPlease note I used watched this video helped me create this scatterplot matrix https://www.youtube.com/watch?v=AY9PYzJtCNA\r\nIt looks like quality, clarity and helpfulness are related. It looks like professors who excel at one of these things, excel at all three. Perhaps the quality of the professor is very good when the professor exercises great clarity and helpfulness. It does not look like there is any relationship between a professor being easy and quality, clarity or helpfulness. It also looks like there is not a relationship between rate of interest and quality, clarity of helpfulness of a professor.\r\nQUESTION 5\r\nFor the student.survey data file in the smss package\r\n#install.packages(“smss”) #install.packages(“alr4”) #install.packages(“car”) #install.packages(“effects”) #install.packages(“carData”) #install.packages(“r package”, repos = “http://cran.us.r-project.org”)\r\n\r\n\r\ninstall.packages(\"r package\", repos = \"http://cran.us.r-project.org\")\r\n\r\n\r\nlibrary(smss)\r\n\r\ndata(student.survey)\r\n\r\nsummary(student.survey)\r\n\r\n\r\n      subj       ge           ag              hi       \r\n Min.   : 1.00   f:31   Min.   :22.00   Min.   :2.000  \r\n 1st Qu.:15.75   m:29   1st Qu.:24.00   1st Qu.:3.000  \r\n Median :30.50          Median :26.50   Median :3.350  \r\n Mean   :30.50          Mean   :29.17   Mean   :3.308  \r\n 3rd Qu.:45.25          3rd Qu.:31.00   3rd Qu.:3.625  \r\n Max.   :60.00          Max.   :71.00   Max.   :4.000  \r\n                                                       \r\n       co              dh             dr               tv        \r\n Min.   :2.600   Min.   :   0   Min.   : 0.200   Min.   : 0.000  \r\n 1st Qu.:3.175   1st Qu.: 205   1st Qu.: 1.450   1st Qu.: 3.000  \r\n Median :3.500   Median : 640   Median : 2.000   Median : 6.000  \r\n Mean   :3.453   Mean   :1232   Mean   : 3.818   Mean   : 7.267  \r\n 3rd Qu.:3.725   3rd Qu.:1350   3rd Qu.: 5.000   3rd Qu.:10.000  \r\n Max.   :4.000   Max.   :8000   Max.   :20.000   Max.   :37.000  \r\n                                                                 \r\n       sp               ne               ah             ve         \r\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Mode :logical  \r\n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 0.000   FALSE:60       \r\n Median : 5.000   Median : 3.000   Median : 0.500                  \r\n Mean   : 5.483   Mean   : 4.083   Mean   : 1.433                  \r\n 3rd Qu.: 7.000   3rd Qu.: 5.250   3rd Qu.: 2.000                  \r\n Max.   :16.000   Max.   :14.000   Max.   :11.000                  \r\n                                                                   \r\n pa                         pi                re         ab         \r\n d:21   very liberal         : 8   never       :15   Mode :logical  \r\n i:24   liberal              :24   occasionally:29   FALSE:60       \r\n r:15   slightly liberal     : 6   most weeks  : 7                  \r\n        moderate             :10   every week  : 9                  \r\n        slightly conservative: 6                                    \r\n        conservative         : 4                                    \r\n        very conservative    : 2                                    \r\n     aa              ld         \r\n Mode :logical   Mode :logical  \r\n FALSE:59        FALSE:44       \r\n NA's :1         NA's :16       \r\n                                \r\n                                \r\n                                \r\n                                \r\n\r\nhead(student.survey)\r\n\r\n\r\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\r\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\r\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\r\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\r\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\r\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\r\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal\r\n            re    ab    aa    ld\r\n1   most weeks FALSE FALSE FALSE\r\n2 occasionally FALSE FALSE    NA\r\n3   most weeks FALSE FALSE    NA\r\n4 occasionally FALSE FALSE FALSE\r\n5        never FALSE FALSE FALSE\r\n6 occasionally FALSE FALSE    NA\r\n\r\n?student.survey\r\n\r\n\r\n\r\nconduct regression analyses relating (i) y = political ideology and x = religiosity,\r\nInitially I received a lot of errors when I tried to plot this data. So I used various ways to remove na data, opting for na.omit Then I realized that the data was not numerical, so I recoded the data. “Very Conservative” to “Very Liberal” became 1 through 7 for “Political Ideology”. “Never” to “Every Week” became 0-3 for “how often you attend religious services.”\r\nThen I could plot the data.\r\n#install.packages(“dplyr”)\r\n\r\n\r\ninstall.packages(\"r package\", repos = \"http://cran.us.r-project.org\")\r\n\r\ndata(\"student.survey\")\r\n\r\nis.na(student.survey)\r\n\r\n\r\n       subj    ge    ag    hi    co    dh    dr    tv    sp    ne\r\n [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [6,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [7,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [8,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [9,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[10,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[11,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[12,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[13,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[14,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[15,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[16,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[17,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[18,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[19,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[20,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[21,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[22,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[23,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[24,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[25,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[26,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[27,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[28,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[29,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[30,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[31,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[32,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[33,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[34,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[35,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[36,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[37,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[38,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[39,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[40,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[41,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[42,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[43,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[44,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[45,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[46,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[47,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[48,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[49,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[50,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[51,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[52,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[53,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[54,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[55,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[56,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[57,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[58,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[59,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[60,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n         ah    ve    pa    pi    re    ab    aa    ld\r\n [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [6,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n [7,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [8,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [9,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[10,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[11,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[12,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[13,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[14,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[15,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[16,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[17,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\r\n[18,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[19,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[20,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[21,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[22,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[23,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[24,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[25,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[26,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[27,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[28,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[29,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[30,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[31,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[32,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[33,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[34,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[35,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[36,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[37,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[38,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[39,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[40,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[41,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[42,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[43,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[44,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[45,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[46,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[47,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[48,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[49,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[50,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[51,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[52,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[53,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[54,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[55,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[56,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[57,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[58,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[59,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[60,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n\r\nstudent.survey<- na.omit(student.survey)\r\nsummary(student.survey)\r\n\r\n\r\n      subj       ge           ag              hi       \r\n Min.   : 1.00   f:22   Min.   :22.00   Min.   :2.200  \r\n 1st Qu.:17.00   m:21   1st Qu.:24.00   1st Qu.:3.000  \r\n Median :31.00          Median :27.00   Median :3.300  \r\n Mean   :31.19          Mean   :29.23   Mean   :3.305  \r\n 3rd Qu.:46.50          3rd Qu.:31.00   3rd Qu.:3.650  \r\n Max.   :60.00          Max.   :71.00   Max.   :4.000  \r\n                                                       \r\n       co              dh             dr               tv        \r\n Min.   :2.600   Min.   :   0   Min.   : 0.200   Min.   : 0.000  \r\n 1st Qu.:3.200   1st Qu.: 180   1st Qu.: 1.500   1st Qu.: 2.000  \r\n Median :3.500   Median : 630   Median : 2.000   Median : 5.000  \r\n Mean   :3.493   Mean   :1333   Mean   : 4.186   Mean   : 6.756  \r\n 3rd Qu.:3.800   3rd Qu.:1650   3rd Qu.: 5.000   3rd Qu.: 8.000  \r\n Max.   :4.000   Max.   :8000   Max.   :20.000   Max.   :37.000  \r\n                                                                 \r\n       sp               ne               ah             ve         \r\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Mode :logical  \r\n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 0.000   FALSE:43       \r\n Median : 5.000   Median : 3.000   Median : 1.000                  \r\n Mean   : 6.023   Mean   : 3.953   Mean   : 1.465                  \r\n 3rd Qu.: 7.500   3rd Qu.: 5.000   3rd Qu.: 2.000                  \r\n Max.   :16.000   Max.   :14.000   Max.   :11.000                  \r\n                                                                   \r\n pa                         pi                re         ab         \r\n d:11   very liberal         : 6   never       :12   Mode :logical  \r\n i:19   liberal              :14   occasionally:18   FALSE:43       \r\n r:13   slightly liberal     : 4   most weeks  : 5                  \r\n        moderate             : 8   every week  : 8                  \r\n        slightly conservative: 6                                    \r\n        conservative         : 3                                    \r\n        very conservative    : 2                                    \r\n     aa              ld         \r\n Mode :logical   Mode :logical  \r\n FALSE:43        FALSE:43       \r\n                                \r\n                                \r\n                                \r\n                                \r\n                                \r\n\r\nhead(student.survey)\r\n\r\n\r\n   subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa               pi\r\n1     1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r     conservative\r\n4     4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i         moderate\r\n5     5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i     very liberal\r\n7     7  m 24 3.6 3.7    0  0.2  5 12  4  2 FALSE  i          liberal\r\n8     8  f 31 3.0 3.0 5000  1.5  5  3  3  1 FALSE  i          liberal\r\n10   10  m 28 4.0 3.1  900  2.0  1  1  2  1 FALSE  i slightly liberal\r\n             re    ab    aa    ld\r\n1    most weeks FALSE FALSE FALSE\r\n4  occasionally FALSE FALSE FALSE\r\n5         never FALSE FALSE FALSE\r\n7  occasionally FALSE FALSE FALSE\r\n8  occasionally FALSE FALSE FALSE\r\n10        never FALSE FALSE FALSE\r\n\r\nlibrary(dplyr)\r\n\r\n\r\nstudent.survey$pi<- recode(student.survey$pi,\r\n                  \"1\" = \"very conservative\",\r\n                  \"2\" = \"conservative\",\r\n                  \"3\" = \"slightly conservative\",\r\n                  \"4\" = \"moderate\",\r\n                  \"5\" = \"slightly liberal\",\r\n                  \"6\" = \"liberal\",\r\n                  \"7\" = \"very liberal\")\r\n\r\nstudent.survey$re<- recode(student.survey$re,\r\n                  \"0\" = \"never\",\r\n                  \"1\" = \"occasionally\",\r\n                  \"2\" = \"most weeks\",\r\n                  \"3\" = \"every week\")\r\n\r\nplot(x = student.survey$re, y = student.survey$pi)\r\n\r\n\r\n\r\n\r\nIt was not clear exactly what this model meant, so I tried another way.\r\n(a) Use graphical ways to portray the individual variables and their relationship.\r\n\r\n\r\nggplot(data=student.survey, aes(x=re, y=pi))+\r\n  geom_point()\r\n\r\n\r\n\r\n\r\n(b) Interpret descriptive statistics for summarizing the individual variables and their relationship.\r\nThis model is unusual, but it does seem to indicate that at as the frequency of attending religious services grew, so did the number associated with political ideology.\r\n(c) Summarize and interpret results of inferential analyses.\r\nPeople who attend church often are more likely to leave conservative, than people who attend church never or only occasionally.\r\nconduct regression analyses relating (ii) y = high school GPA and x = hours of TV watching.\r\n\r\n\r\nplot(x = student.survey$tv, y = student.survey$hi)\r\n\r\n\r\n\r\n\r\n(a) Use graphical ways to portray the individual variables and their relationship.\r\n\r\n\r\nggplot(data=student.survey, aes(x=tv, y=hi))+\r\n  geom_point()+\r\n  geom_smooth(method=\"lm\", se=FALSE)\r\n\r\n\r\n\r\n\r\n(b) Interpret descriptive statistics for summarizing the individual variables and their relationship.\r\nI do not see a significant relationship here.\r\n(c) Summarize and interpret results of inferential analyses.\r\nWhile a few outliers- who watch a lot of TV seem to have below average High School GPAs, most people in this study fall between a 3.0 and a 4.0 GPA and average number of hours of tv does not seem to affect GPA.\r\nQUESTION 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\r\nMaybe the 10 students who performed the poorest on the midterm would have had an increase from 50 to 60 for the final, regardless of the tutoring. Perhaps they would have been so aware and concerned that they had performed poorly on the midterm, that they would have made an extra effort to do better on the next (and final) exam. While the students that did better or even just average for the class- would not have felt the same drive to try harder.\r\nhttps://www.youtube.com/watch?v=1tSqSMOyNFE\r\nThere is always a level of chance. “You should not expect them to be as unlucky when you test them a second time…their scores should improve just based on random chance.”\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomektracy875966/distill-preview.png",
    "last_modified": "2022-03-12T19:36:39-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomclairebattagliahomework-2-603/",
    "title": "Homework 2",
    "description": "Simple Linear Regression",
    "author": [
      {
        "name": "Claire Battaglia",
        "url": "https://rpubs.com/clairebattaglia"
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\n\nContents\nQuestion 1\nAnswer\nSolution\n\nQuestion 2\nAnswer\nSolution\n\nQuestion 3\nAnswer\nSolution\n\nQuestion 4\nSolution\nDiscussion\n\nQuestion 5\nSolution\n\nQuestion 6\nSolution\n\n\nQuestion 1\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\nIdentify the predictor and the response.\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nAnswer\nThe predictor, or explanatory, variable is gross national product per person (ppgdp) and the response variable is fertility.\nNo. The relationship between the two variables appears to be exponential instead of linear so a simple linear regression model is not a good fit.\nYes. A simple linear regression model is plausible when we log-transform the data.\nSolution\nLet’s start by inspecting the data. I’ll load the data set and create a new dataframe with just the variables I’m interested in. I’ll then take a quick look to make sure that there are no missing values.\n\n\nShow code\n\n# load data\ndata(\"UN11\")\n\n# create new dataframe\nun11 <- UN11 %>%\n  select(c(fertility, ppgdp))\n\n# get summary\nsummary(un11)\n\n\n   fertility         ppgdp         \n Min.   :1.134   Min.   :   114.8  \n 1st Qu.:1.754   1st Qu.:  1283.0  \n Median :2.262   Median :  4684.5  \n Mean   :2.761   Mean   : 13012.0  \n 3rd Qu.:3.545   3rd Qu.: 15520.5  \n Max.   :6.925   Max.   :105095.4  \n\nNow I’ll create a scatterplot.\n\n\nShow code\n\n# create scatterplot\nggplot(un11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  labs(title = \"Fertility v. Gross National Product Per Person\", x = \"Gross National Product Per Person\", y = \"Fertility\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\n\n\n\nWe can see that a linear model is not a good fit. This makes intuitive sense, as a linear model would predict a negative birth rate that continues to grow in absolute value beyond a certain value for ppgdp, which isn’t logical. It instead seems that fertility varies pretty wildly among countries with a ppgdp less than 125,000 or so but that for countries with a ppgdp above that threshold, fertility varies much less and eventually seems to settle around 2.\nBecause the relationship isn’t linear, a log transformation might be useful.\n\n\nShow code\n\nggplot(un11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  labs(title = \"Log Transformation of Fertility v. Gross National Product Per Person\", x = \"Log of Gross National Product Per Person\", y = \"Log of Fertility\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\n\n\n\nA linear model of the log transformation appears to be a much better fit.\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\nHow, if at all, does the slope of the prediction equation change?\nHow, if at all, does the correlation change?\nAnswer\nThe slope does change. Converting the units of the response variable at the exchange rate from 2016 (1 GBP = 1.33 USD, 1 USD = .75 GBP), \\[new\\;slope=\\frac{old\\;slope}{1.33}\\] It’s worth noting that if we were to convert the units of the explanatory variable instead of the response variable, we would calculate the new slope by performing the inverse mathematical operation of the unit change.\nThe correlation does not change.\nSolution\nWhile the slope (\\(b\\)) of a prediction equation and the correlation (\\(r\\)) between the two variables are linked conceptually and mathematically, they are distinct from one another and convey different information.\nThe slope is a measure of the direction of the association between the two variables. For every unit increase in \\(x\\), \\(y\\) will either increase or decrease (if the two variables are actually associated with one another). The sign of the slope will tell us which and the value itself will tell us by how much.\nThe slope cannot tell us the strength of that association, however. This is because the slope is dependent on the specific units of the variables. If the data are converted to a different unit of measurement, the slope will change, while logically we know that the strength of the association between the two variables hasn’t changed.\nAs a standardized form of the slope, the correlation is not dependent on the specific units of the variables and is thus able to tell us the strength of the association between the two variables.\nLet’s look at a quick example (all data are entirely made up and completely meaningless).\n\n\nShow code\n\nx <- 1:20 # set values for x\na <- 1 # set y intercept\nb <- 2 # set slope\nyUSD = a + b*x # prediction equation with y in USD\n\n# create dataframe\ndataQ2 <- data.frame(x, yUSD)\n\n# convert y from USD to GBP\ndataQ2 <- dataQ2 %>%\n  mutate(yGBP = yUSD/1.33)\n\n# view\ndataQ2\n\n\n    x yUSD      yGBP\n1   1    3  2.255639\n2   2    5  3.759398\n3   3    7  5.263158\n4   4    9  6.766917\n5   5   11  8.270677\n6   6   13  9.774436\n7   7   15 11.278195\n8   8   17 12.781955\n9   9   19 14.285714\n10 10   21 15.789474\n11 11   23 17.293233\n12 12   25 18.796992\n13 13   27 20.300752\n14 14   29 21.804511\n15 15   31 23.308271\n16 16   33 24.812030\n17 17   35 26.315789\n18 18   37 27.819549\n19 19   39 29.323308\n20 20   41 30.827068\n\nI now have 20 values for \\(x\\) in USD, 20 corresponding \\(y\\) values in USD, and a second set of \\(y\\) values that are the original \\(y\\) values converted into GBP at the given exchange rate. I can now regress each set of \\(y\\) values onto \\(x\\) and compare the slopes and correlation coefficients.\n\n\nShow code\n\n# regress yUSD onto x\nfitUSD <- lm(yUSD ~ x, data = dataQ2)\n\n# regress yGBP onto x\nfitGBP <- lm(yGBP ~ x, data = dataQ2)\n\n# show both regression outputs\nstargazer(fitUSD, fitGBP,\n          type = \"text\",\n          header = FALSE,\n          single.row = TRUE,\n          no.space = TRUE,\n          column.sep.width = \"3pt\",\n          font.size = \"small\")\n\n\n\n=================================================================================================================================\n                                                                      Dependent variable:                                        \n                              ---------------------------------------------------------------------------------------------------\n                                                    yUSD                                              yGBP                       \n                                                     (1)                                               (2)                       \n---------------------------------------------------------------------------------------------------------------------------------\nx                                             2.000*** (0.000)                                  1.504*** (0.000)                 \nConstant                                      1.000*** (0.000)                                  0.752*** (0.000)                 \n---------------------------------------------------------------------------------------------------------------------------------\nObservations                                         20                                                20                        \nR2                                                  1.000                                             1.000                      \nAdjusted R2                                         1.000                                             1.000                      \nResidual Std. Error (df = 18)                       0.000                                             0.000                      \nF Statistic (df = 1; 18)      79,494,261,481,456,398,357,598,065,131,520.000*** 71,025,695,591,639,092,023,323,881,635,840.000***\n=================================================================================================================================\nNote:                                                                                                 *p<0.1; **p<0.05; ***p<0.01\n\nWhen \\(y\\sim\\sf{x}\\) and both variables are in USD, \\(b=2\\) and \\(r=1\\)\nThis means that when \\(x\\) increases by a single unit, \\(y\\) increases by 2 units.\nWhen I convert \\(y\\) from USD into GBP and once again run the regression \\(y\\sim\\sf{x}\\), \\(b=1.504\\) and \\(r=1\\)\nThis means that when \\(x\\) increases by a single unit, \\(y\\) increases by 1.504 units.\n\\[1.504=\\frac{2}{1.33}\\]\n\nNote that while the output doesn’t include the correlation coefficient, we know that \\(r=1\\) because \\(R^2=1\\).\nBecause I created the data myself, I know that the relationship between \\(x\\) and \\(y\\) is the same when \\(y\\) is in USD as when \\(y\\) is in GBP, yet the slopes are different. This demonstrates that the slope is dependent on the specific unit of measurements of the variables while the correlation coefficient is not. The two are used in concert to understand the relationship between two variables.\nQuestion 3\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\nAnswer\nYear does not appear to be correlated with any variable.\nAPMAM, APSAB, and APSLAKE appear to be moderately positively correlated with one another.\nOPBPC, OPRC, and OPSLAKE appear to be strongly positively correlated with one another.\nThere appears to be a moderate positive correlation between BSAAM and APMAM, APSAB, and APSLAKE, and a strong positive correlation between BSAAM and OPBPC, OPRC, and OPSLAKE.\nSolution\nAs always, I’ll start by inspecting the data.\n\n\nShow code\n\n# load data\ndata(\"water\")\n\n# create object\nwater <- water\n\n# preview\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\nThe predictor variables are the precipitation measurements taken at each of the six sites: APMAM, APSAB, APSLAKE, OPBPC, OPRC, OPSLAKE, measured in inches.\n\nMeasurements are included in the package documentation, available here.\nThe response variable is the stream runoff at the site BSAAM, measured in acre-feet.\n\n\nShow code\n\n# create scatterplot matrix\npairs(water)\n\n\n\n\nYikes. Let’s first orient ourselves so that we can make sense of this.\nThe \\(x\\)-axis of any given plot is the variable named in that column. In the above matrix, each plot in the first column has the \\(x\\)-axis year.\nThe \\(y\\)-axis of any given plot is the variable named in that row. In the above matrix, each plot in the first row has the \\(y\\)-axis year.\nBy looking at the first column and row, then, we can see the variable year plotted against every other variable, on the \\(x\\) and \\(y\\)-axis, respectively.\nWith that in mind, let’s assess.\nYear does not appear to be correlated with any variable, which is to be expected. While collecting these data over time could reveal larger trends (e.g. climate change), the year itself isn’t predictive of runoff.\nAPMAM, APSAB, and APSLAKE appear to be moderately positively correlated with one another.\nOPBPC, OPRC, and OPSLAKE appear to be strongly positively correlated with one another.\nBSAAM appears to be moderately positively correlated with APMAM, APSAB, and APSLAKE.\nBSAAM appears to be strongly positively correlated with OPBPC, OPRC, and OPSLAKE.\nAnother way to visualize the data is with the ggcorr function. Instead of showing us a scatterplot matrix, it calculates the correlation between each set of variables and then displays the strength of the association as color-coded categories. This is an easy to confirm what we gleaned from the scatterplot matrix.\n\n\nShow code\n\nggcorr(water)\n\n\n\n\nWe can see that no variables are negatively correlated with one another (blue), that year is either not correlated or very weakly positively correlated with every variable (white to pale pink), that APMAM, APSAB, and APSLAKE are moderately positively correlated with one another (light red to red), that OPBPC, OPRC, and OPSLAKE are strongly positively correlated with one another (red), and, finally, that BSAAM is strongly positively correlated with OPBPC, OPRC, and OPSLAKE (red) and slightly less so with APMAM, APSAB, and APSLAKE (light red).\nQuestion 4\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\nSolution\nAs always, I’ll start by inspecting the data.\n\n\nShow code\n\n# load data\ndata(\"Rateprof\")\n\n# create object\nrateProf <- Rateprof\n\n# preview\nhead(rateProf)\n\n\n  gender numYears numRaters numCourses pepper discipline\n1   male        7        11          5     no        Hum\n2   male        6        11          5     no        Hum\n3   male       10        43          2     no        Hum\n4   male       11        24          5     no        Hum\n5   male       11        19          7     no        Hum\n6   male       10        15          9     no        Hum\n               dept  quality helpfulness  clarity easiness\n1           English 4.636364    4.636364 4.636364 4.818182\n2 Religious Studies 4.318182    4.545455 4.090909 4.363636\n3               Art 4.790698    4.720930 4.860465 4.604651\n4           English 4.250000    4.458333 4.041667 2.791667\n5           Spanish 4.684211    4.684211 4.684211 4.473684\n6           Spanish 4.233333    4.266667 4.200000 4.533333\n  raterInterest sdQuality sdHelpfulness sdClarity sdEasiness\n1      3.545455 0.5518564     0.6741999 0.5045250  0.4045199\n2      4.000000 0.9020179     0.9341987 0.9438798  0.5045250\n3      3.432432 0.4529343     0.6663898 0.4129681  0.5407021\n4      3.181818 0.9325048     0.9315329 0.9990938  0.5882300\n5      4.214286 0.6500112     0.8200699 0.5823927  0.6117753\n6      3.916667 0.8632717     1.0327956 0.7745967  0.6399405\n  sdRaterInterest\n1       1.1281521\n2       1.0744356\n3       1.2369438\n4       1.3322506\n5       0.9749613\n6       0.6685579\n\nNow I’ll create a basic scatterplot matrix.\n\n\nShow code\n\n# create scatterplot matrix\npairs(~ quality + helpfulness + clarity + easiness + raterInterest, data = rateProf)\n\n\n\n\nDiscussion\nQuality appears to have:\na strong positive correlation with helpfulness and clarity,\na moderate positive correlation with easiness,\na weak positive correlation with raterInterest.\nHelpfulness appears to have:\na strong positive correlation with quality and clarity,\na moderate positive correlation with easiness,\na weak positive correlation with raterInterest.\nClarity appears to have:\na strong positive correlation with quality and helpfulness,\na moderate positive correlation with easiness,\na weak positive correlation with raterInterest.\nEasiness appears to have:\na moderate positive correlation with quality, helpfulness, and clarity,\na weak positive correlation with raterInterest.\nRaterInterest appears to have a weak positive correlation with every other variable.\nBeyond a simple statement of the association between each of these variables, however, it’s impossible to say what substantive, or contextual, significance these data have. In an attempt to understand the data more clearly, I created an account with Rate My Professors and took a look around. Some things I noticed are:\nThe questions that yield the data represented in the above matrix all utilize a Likert scale and there is no guidance as to what constitutes a “1” or a “5” on any given question. For example, the question that yields the quality rating is simply “Rate your professor” and the scale is from “1 - Awful” to “5 - Awesome.” Without knowing what makes a professor awful or awesome to particular respondent, the rating itself is arguably meaningless.\nInformation about the respondent (the student) is paramount to placing the ratings in context. For example, the question that yields the easiness rating is “How difficult was this professor?” If the rating were for the professor of this class (DACSS 603), for example, a rating of “5 - Very Difficult” could mean something completely different if it’s coming from an undergraduate student with a humanities background versus a graduate student with a statistics background. While the question is technically about the professor, not the subject material, those two are too easily conflated. Without knowing this information about the respondent, this rating is of limited value.\nThere are some challenges to measurement validity. For example, is a high rating in clarity due to the professor actually being clear when conveying information or is it due to the student having a strong enough background in the subject matter to understand the material well, regardless of how clearly it’s presented? Similarly, we know that people ask better questions when they’re better versed in a subject so it’s conceivable that a high helpfulness rating is actually reflective of the respondent being able to ask good enough questions to get good answers.\nThe whole point of Rate My Professors is for students to be able to read reviews of a professor and/or class before taking a class. Thus if it’s being utilized as intended, students are, by design, going into a course with others’ evaluation of the professor and/or class in mind, which could conceivably influence their attitude/behavior in the class, which could affect their experience, which would ultimately affect their own rating.\nUltimately, I don’t think these data can tell us much about a given professor. The real potential of these data is in understanding how people rate their professors, which is not the same thing as how the professors actually are.\nIt could be interesting to subset the data in different ways: by the gender of the professor (do we rate professors of different gender differently?), the gender of the respondent (do students of different gender rate professors differently?), the discipline (do we rate humanities professors differently than STEM professors?), the age/year of the respondent (do undergraduate freshmen tend to rate differently than graduate students?), etc. As the questions are currently designed, however, they don’t consistently capture valid information about the professor or the underlying factors that influence how people might rate their professors.\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating\ny = political ideology and x = religiosity,\ny = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\nUse graphical ways to portray the individual variables and their relationship.\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\nSummarize and interpret results of inferential analyses.\nSolution\nAs always, I’ll start by inspecting the data.\n\n\nShow code\n\n# load data\ndata(\"student.survey\")\n\n# create dataframe\nstudS <- student.survey %>%\n  select(c(\"pi\", \"re\", \"hi\", \"tv\"))\n\n# get summary\nsummary(studS)\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nWhile I am using the summary function here primarily to determine which types of variables we’re working with and to make sure there are no missing values, it actually also returns some of the descriptive statistics I’m interested in. I’ll look at those more closely in each section.\nPolitical Ideology and Religiosity\nPolitical ideology and religiosity are both categorical variables, meaning that respondents can select their response option from a specified set of values that are categorical in nature. Because there is an underlying order to each range of response options, both are ordinal.\nThis means that I’ll primarily be looking at the frequency and relative frequency of each response option. Given that the distance between response options is not quantifiable and almost certainly unequal, measures like the mean and standard deviation are not appropriate here.\nI’ll start by looking at political ideology. A simple bar chart is a great way to visualize frequency of each response.\n\n\nShow code\n\n# create bar chart\nggplot(studS, aes(x = pi)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Political Ideology of STA 6126 Students\", x = \"Political ideology\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nWe can clearly see that the mode is “liberal,” as well as the relative frequency of all of the response options (although not the exact values).\nWe could also view the frequency distribution in a simple table. In the table below, the first value in each cell is the number of students who responded with that option and the second value is the proportion. Looking at the first cell, we can see that 8 students, or 13.3%, identify as “very liberal.”\n\n\nShow code\n\n# create freq table\nCrossTable(studS$pi,  max.width = 3)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  60 \n\n \n                      |          very liberal |               liberal |      slightly liberal | \n                      |-----------------------|-----------------------|-----------------------|\n                      |                     8 |                    24 |                     6 | \n                      |                 0.133 |                 0.400 |                 0.100 | \n                      |-----------------------|-----------------------|-----------------------|\n\n                      |              moderate | slightly conservative |          conservative | \n                      |-----------------------|-----------------------|-----------------------|\n                      |                    10 |                     6 |                     4 | \n                      |                 0.167 |                 0.100 |                 0.067 | \n                      |-----------------------|-----------------------|-----------------------|\n\n                      |     very conservative | \n                      |-----------------------|\n                      |                     2 | \n                      |                 0.033 | \n                      |-----------------------|\n\n\n\n \n\nIf we were willing to sacrifice some precision to simplify the data we could collapse the categories into “conservative,” “moderate,” and “liberal.”\n\n\nShow code\n\n# collapse categories\npiCollapse <- studS %>%\n  mutate(pi = fct_collapse(pi,\n                         liberal = c(\"very liberal\", \"liberal\", \"slightly liberal\"),\n                         conservative = c(\"very conservative\", \"conservative\", \"slightly conservative\"),\n                         moderate = c(\"moderate\")))\n\n# create bar plot\nggplot(piCollapse, aes(x = pi)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Political Ideology of STA 6126 Students\", x = \"Political ideology\", y = NULL) +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\nAnother way to approach these data is by looking at the students’ distance from the ideological center. That is, how are the students distributed around the center of the ideological spectrum?\n\n\nShow code\n\n# collapse categories\npiDistance <- studS %>%\n  mutate(pi = fct_collapse(pi,\n                           \"furthest left or right of center\" = c(\"very liberal\", \"very conservative\"),\n                           \"a bit further left or right of center\" = c(\"liberal\", \"conservative\"),\n                           \"slightly left or right of center\" = c(\"slightly liberal\", \"slightly conservative\"),\n                           \"center (neither left nor right)\" = c(\"moderate\")))\n\n# create bar plot\nggplot(piDistance, aes(x = pi)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Distance from Ideological Center\", x = \"Distance from center\", y = NULL) +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\nIt appears that the greatest number of students consider themselves to be more than just slightly left or right of center but not very far left or right of center.\nI’m sure this is an entire area of study in itself so I’d want to explore the literature before standing by this interpretation but for now I’ll say we could potentially look at the data this way.\nNext I’ll look at religiosity. Again, a simple bar chart is a great place to start.\n\n\nShow code\n\n# create bar chart\nggplot(studS, aes(x = re)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Religiosity of STA 6126 Students\", x = \"How often do you attend religious services?\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nOnce again this allows us to easily identify the mode and relative frequency of each response option. “Occasionally” is clearly the most common response; “most weeks” the least common.\nWe can again look at the frequency distribution in a simple table.\n\n\nShow code\n\n# create freq table\nCrossTable(studS$re,  max.width = 4)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  60 \n\n \n             |        never | occasionally |   most weeks |   every week | \n             |--------------|--------------|--------------|--------------|\n             |           15 |           29 |            7 |            9 | \n             |        0.250 |        0.483 |        0.117 |        0.150 | \n             |--------------|--------------|--------------|--------------|\n\n\n\n \n\nIn order to understand the relationship between political ideology and religiosity, we can construct a contingency table and/or a scatterplot.\n\n\nShow code\n\n# create table pi and re\ntablePiRe <- table(studS$pi, studS$re)\n\n# view\ntablePiRe\n\n\n                       \n                        never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nShow code\n\n# create scatterplot\nggplot(studS, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_jitter() +\n  labs(title = \"Political Ideology v. Religiosity\", x = \"Religiosity\", y = \"Political ideology\") +\n  theme_minimal()\n\n\n\n\nThe two do seem to be associated with one another. Less religious students are more likely to be more liberal and more religious students are more likely to be more conservative.\nBecause both variables are ordinal, we can also calculate gamma (\\(\\gamma\\)) to understand the association between them. Gamma is the standardized difference between pairs that are concordant (both measures are either high or low) and discordant (one measure is high and the other is low).\n\n\nShow code\n\n# calculate gamma\nGoodmanKruskalGamma(tablePiRe,\n                    conf.level = 0.95)\n\n\n    gamma    lwr.ci    upr.ci \n0.5747711 0.3646211 0.7849211 \n\nThis output gives us the 95% confidence interval for gamma. Here, \\(\\gamma=.575\\). The positive sign indicates a positive correlation and the confidence interval tells us gamma will most likely lie between .365 and .785—that is, there is a moderate to fairly strong correlation between the two variables.\nBecause of the way the categories are numbered (1 = very liberal, 7 = very conservative), gamma has been calculated such that we articulate the association:\nReligiosity is positively associated with conservatism. As religiosity increases, conservatism increases.\nWe could also say that religiosity is negatively associated with liberalism but that is not what’s being reflected in our gamma calculation.\nThere are a few ways to approach understanding association between ordinal variables. I’ve chosen to look at gamma because the sample size seems too small to justify using \\(\\chi^2\\). Another option is to treat the data numerically and then run a simple linear regression. Given the potentially extreme variability in distance between the categories (for religiosity in particular), I’m not sure we could justify doing that here.\nThat being said, here’s what it would look like.\n\n\nShow code\n\n# convert categories to numeric values\npireNum <- studS %>%\n  select(c(\"pi\", \"re\")) %>%\n  sapply(unclass)\n\n# convert to dataframe\npireNum <- data.frame(pireNum)\n\n# regress ideology onto religiosity\nfitPiRe <- lm(pi ~ re, data = pireNum)\n\n# get summary\nsummary(fitPiRe)\n\n\n\nCall:\nlm(formula = pi ~ re, data = pireNum)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre            0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\nHere again we do see a positive correlation, which confirms our above conclusion. The \\(P\\)-value meets the criteria for statistical significance at the .01 level, indicating that we can be confident that the slope (\\(b\\)) is not zero.\nPut another way, we can reject the null hypothesis that the two variables are independent of one another.\nHigh School GPA and TV Watching\nBoth high school GPA and hours of TV watched per week are quantitative variables. This means that in addition to looking at the frequency and relative frequency of each value, we’ll also be able to look at the mean and standard deviation of each data set.\nI’ll first look at high school GPA. Because this is a small data set, a stem-and-leaf plot might be a useful way to take a quick look.\n\nWe could also use a histogram.\n\n\nShow code\n\n# create stem and leaf plot\nstem(studS$hi, scale = 3)\n\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  20 | 0\n  21 | 0\n  22 | 0\n  23 | 0\n  24 | \n  25 | \n  26 | \n  27 | 0\n  28 | 0\n  29 | \n  30 | 00000000000000\n  31 | 0\n  32 | 000\n  33 | 000000\n  34 | 00000\n  35 | 0000000\n  36 | 000\n  37 | 0000\n  38 | 000000\n  39 | \n  40 | 00000\n\nWe can see that the lowest grade earned is a 2.0 (1 student) and that the highest grade earned is a 4.0 (5 students), making the range 2.\n3.0 appears to be the most frequently earned grade (the mode) and the largest proportion of grades are clustered between 3.0 and 3.8 so I expect both the mean and median to fall somewhere between those values.\nBeyond the mode, range, and some sense of the distribution of the grades, there are a couple of other descriptive statistics we can look at.\n\n\nShow code\n\n# get summary stats\nselect(studS, hi) %>%\n  describe.by()\n\n\n   vars  n mean   sd median trimmed  mad min max range  skew kurtosis\nX1    1 60 3.31 0.46   3.35    3.35 0.52   2   4     2 -0.76     0.57\n     se\nX1 0.06\n\nThe median is 3.35, the mean is 3.31, and the standard deviation is .46. We can see that the data are slightly skewed because the median and mean are different values. The mean is pulled down by those few very low GPAs.\nNow I’ll look at the average number of hours of TV watched per week. We can use a simple histogram to take a look at the frequency distribution.\n\n\nShow code\n\n# create hist\nggplot(studS, aes(x = tv)) +\n  geom_histogram(fill = \"#830042\") +\n  labs(title = \"TV-Watching Habits of STA 6126 Students\", x = \"Average hours watched per week\", y = NULL) +\n  theme_minimal()\n\n\n\n\nThis gives us a sense of the frequency distribution and we can again get some summary statistics.\n\n\nShow code\n\n# get summary stats\nselect(studS, tv) %>%\n  describe.by()\n\n\n   vars  n mean   sd median trimmed  mad min max range skew kurtosis\nX1    1 60 7.27 6.72      6    6.24 5.93   0  37    37 2.14     6.06\n     se\nX1 0.87\n\nWe can see from the histogram that most of the students watch less than 15 hours per week but that a few watch more than 25 hours per week. The median is 6 and the mean is 7.27, reflecting the skew created by those few very high values. The range (37) is surprisingly large and it’s hard to even imagine watching 37 hours of TV per week. It’s conceivable that there are two types of TV-watching: active watching (you sit down to watch something) and passive watching (it’s on in the background while you do something else). The large range makes me wonder if some respondents were reporting only active watching while perhaps others were reporting both active and passive watching.\nBefore looking at these two variables together, it’s worth noting that we’re looking at each student’s GPA in high school and TV-watching habits in graduate school. It’s worth noting because I think it’s tempting to conceptualize TV-watching habits as the explanatory variable and GPA as the outcome variable (something like “watching more/less TV is associated with a higher/lower GPA”) and subsequently explore causal inference but given the temporal order in this case (GPA precedes TV-watching) and long time between the two (a minimum of 4 years), it seems highly unlikely that any association between these two variables could have any substantive meaning.\nThat being said, a scatterplot is a great way to visualize association.\n\n\nShow code\n\n# create scatterplot\nggplot(studS, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_jitter() +\n  labs(title = \"High School GPA v. Hours of TV Watched per Week in Graduate School\", x = \"Hours of TV watched per week\", y = \"High School GPA\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\n\n\n\nThere appears to be a weak negative correlation between high school GPA and hours of TV watched per week.\n\n\nShow code\n\n# cor test\ncor.test(studS$tv, studS$hi)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  studS$tv and studS$hi\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nA correlation coefficient (\\(r\\)) of -.268 confirms that there is a weak negative correlation. Those students with higher GPAs in high school tend to watch less TV in graduate school.\n\n\nShow code\n\n# fit model\nfitHiTv <- lm(hi ~ tv, data = studS)\n\n# get summary\nsummary(fitHiTv)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = studS)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\nOur regression analysis indicates that for every unit increase in GPA, there is a .018 decrease in hours of TV watched in graduate school.\nAgain, though, given the above discussion of these two variables, I don’t believe there is any substantive or contextual meaning to the observed association.\nQuestion 6\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\nSolution\nRegression towards the mean is a statistical phenomenon in which extreme values (those furthest away from the mean) tend to be less extreme (closer to the mean) when measured again.\nIt occurs when two variables are correlated but there is also some element of chance that explains the outcome variable. Observing a non-random sample makes it more likely that we will observe this phenomenon.\nIn the context of this quasi-experiment, each student’s test scores vary around some true level of knowledge, which we don’t know. The difference between the student’s true level of knowledge and the student’s exam score is the error.\n\nTo call this a quasi-experiment feels generous but for lack of a more precise term, that’s what I’m going to call it.\nThis error can occur in either direction and be caused by many things. That is, a student could score higher or lower than her true level of knowledge because of any number of conditions—conditions that occur to some degree by chance. Because those conditions occur to some degree by chance, it’s improbable that the same students will experience the same conditions during both the first observation (the midterm exam) and the second observation (the final exam). If those students do not experience those same conditions, we would expect to see the students who scored higher than their true level of knowledge score a little lower and the students who scored lower than their true level of knowledge score a little higher. This will occur regardless of treatment.\nIt’s important to note that regression towards the mean does not imply that every student who scored extremely high or low did so purely by chance and that their scores will be different when observed a second time. We’re looking at the mean score of the ten lowest-scorers. If even just a few of those students scored lower than their true level of knowledge due to some chance condition—perhaps one student was sick on exam day, one missed the bus and had to run to school, and a third’s parents had just announced their divorce—that wasn’t present at the time of the second observation (the final exam), those students would score higher, which would increase the mean of the entire group of lowest-scorers. Thus we cannot say whether there was any change in the students’ true level of knowledge.\nIt would be helpful to also look at the mean score of the ten highest-scorers on the midterm. Regression towards the mean tells us that we would likely see the mean score of those ten students decrease from the midterm to the final.\nFrom a research design perspective, this quasi-experiment is problematic in other ways that prevent us from drawing any causal inference about the effectiveness of the treatment.\nThe sample is small.\nThere is no control group. There is a group of students who did not receive the treatment but because the treatment and no-treatment groups were not randomly assigned, we know that they differed from one another in at least one way (their midterm exam score). Whether they differed in other ways was not explored so at this time we have no way of knowing whether they differed in systematic ways that could also explain their scores.\nIt’s likely that one observation (the midterm exam) is insufficient for determining a student’s true level of knowledge. In order to overcome the regression challenge, we would need more observations of the same students over time. This would help us understand how much of each student’s score can be attributed to some true level of knowledge and how much is due to some chance condition.\nWe can finally conclude that the phenomenon of regression towards the mean could easily explain the increase in the mean score of the ten lowest-scorers. In addition, there are numerous other challenges to the validity of this quasi-experiment. As such, we cannot claim that the treatment was effective. Indeed, we can’t even claim that there was any actual change in the students’ level of knowledge.\n\n\n\n",
    "preview": "posts/httpsrpubscomclairebattagliahomework-2-603/distill-preview.png",
    "last_modified": "2022-03-12T19:36:42-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomalexanderhong86876317/",
    "title": "DACSS 603 HW#2",
    "description": "Second homework for DACSS 603.",
    "author": [
      {
        "name": "Alexander Hong",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n\r\n\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nhw1 <- UN11\r\n\r\nhw1_plot <- ggplot(hw1, aes(ppgdp, fertility)) + geom_point()\r\nhw1_plot_log <- ggplot(hw1, aes(log(ppgdp), log(fertility)) ) + geom_point()\r\n\r\n\r\n\r\n1.1.1. Identify the predictor and the response. - The response is ppgdp (on ppgdp) and the predictor is fertility (dependence of).\r\n\r\n\r\n\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n\r\n\r\n\r\nNo. If anything, the points seem to be distributed in a negative exponential fashion.\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nThe natural logarithmic transformation has enabled a simple linear regression model to be plausible.\r\nQuestion 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n\r\n\r\nsome_pred <- c(7, 9, 11, 12, 13, 14)\r\nsome_inc_us <- c(45000, 58000, 65400, 80000, 94600, 103580)\r\nsome_inc_uk <- some_inc_us * (1 / 1.33)\r\n\r\nhw2 <- data.frame(some_pred, some_inc_us, some_inc_uk)\r\nhw2_table <- knitr::kable(hw2)\r\n\r\nhw2a <- summary(lm(some_pred ~ some_inc_us, data = hw2))\r\nhw2b <- summary(lm(some_pred ~ some_inc_uk, data = hw2))\r\n\r\n\r\n\r\nFirst, I made a hypothetical dataset that has some response variable (some_pred), along with hypothetical salaries (some_inc_us), as well as converting the salaries to pounds (some_inc_uk)\r\nsome_pred\r\nsome_inc_us\r\nsome_inc_uk\r\n7\r\n45000\r\n33834.59\r\n9\r\n58000\r\n43609.02\r\n11\r\n65400\r\n49172.93\r\n12\r\n80000\r\n60150.38\r\n13\r\n94600\r\n71127.82\r\n14\r\n103580\r\n77879.70\r\nHow, if at all, does the slope of the prediction equation change\r\nThe slope (coefficient) for using dollars as a predictor 1.1334084^{-4}\r\nis different from\r\nusing pounds as a predictor 1.5074332^{-4}\r\nHow, if at all, does the correlation change? It seems as if the correlation does not change\r\nR Squared value, using dollar salaries to predict some_pred : 0.9465294\r\nR Squared value, using UK pound salaries to predict some_pred : 0.9465294\r\nQuestion 3\r\nWater runoff in the Sierras (ALR Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\nlibrary(alr4)\r\n\r\nhw3 <- pairs(water[,2:7])\r\n\r\n\r\n\r\n\r\nIt seems that all the AP___ predictors are heavily linearly related with other AP___ predictors. The same can be said with OP___ predictors being heavily linearly related with other OP___ predictors.\r\nOtherwise, the linear relationship from an AP___ predictor to an OP___ predictor is not as prevalent.\r\nQuestion 4\r\nProfessor ratings (ALR Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n\r\n\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\nhw4 <- Rateprof\r\nhw4_plot_var <- select(hw4, quality, clarity, helpfulness, easiness, raterInterest)\r\nhw4_pairs <- pairs(hw4_plot_var)\r\n\r\n\r\n\r\n\r\nThe quality, clarity, and helpfulness ratings seem to be highly, positively correlated with each other and demonstrate clear linear relationships with each other.\r\nThe easiness ratings also seem to be positively correlated with helpfulness, clarity, and quality. However the linear relationship is not as as strong compared to the relationships that quality, clarity, and helpfulness share with each other.\r\nRater interest\r\nQuestion 5\r\nFor the student.survey SMSS data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\nUse graphical ways to portray the individual variables and their relationship.\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\nSummarize and interpret results of inferential analyses.\r\n\r\n\r\nlibrary(smss)\r\nlibrary(ggplot2)\r\n\r\n## To convert the responses to numerical values, based on scaling\r\ndata(student.survey)\r\nhw5 <- student.survey\r\n\r\nre_mod <- unclass(hw5$re)\r\npi_mod <- unclass(hw5$pi)\r\n\r\nre_scale <- c(1:4)\r\npi_scale <- c(1:7)\r\nre_levels <- levels(hw5$re)\r\npi_levels <- levels(hw5$pi)\r\n\r\n# Outputs the scales and what it is associated with\r\nre_levels_df <- data.frame(re_scale, re_levels)\r\npi_levels_df <- data.frame(pi_scale, pi_levels)\r\n\r\n# Model fitting\r\ni5 <- lm(pi_mod ~ re_mod, hw5)\r\ni5_summ <- summary(i5)\r\ni5_plot <- ggplot(hw5, aes(re_mod, pi_mod)) + geom_point()\r\ni5_resumm <- summary(re_mod)\r\ni5_pisumm <- summary(pi_mod)\r\n\r\na5 <- data.frame(as.factor(i5_pisumm), as.factor(i5_resumm))\r\nnames(a5) <- c('PI', 'RE')\r\n\r\n# Correlation Test\r\ni5_cortest <- cor.test(pi_mod, re_mod)\r\n\r\n# Information fot TV Watching vs High School GPA\r\nii5 <- lm(hi ~ tv, hw5)\r\nii5_plot <- ggplot(hw5, aes(tv, hi)) + geom_point()\r\n\r\naa5 <- data.frame(as.factor(summary(hw5$tv)), as.factor(summary(hw5$hi)) )\r\nnames(aa5) <- c('TV', 'HI')\r\n\r\nii5_cortest <- cor.test(hw5$hi, hw5$tv)\r\n\r\n\r\n\r\nOutline of the response choices for Religious Services and Political Ideology\r\nre_scale\r\nre_levels\r\n1\r\nnever\r\n2\r\noccasionally\r\n3\r\nmost weeks\r\n4\r\nevery week\r\npi_scale\r\npi_levels\r\n1\r\nvery liberal\r\n2\r\nliberal\r\n3\r\nslightly liberal\r\n4\r\nmoderate\r\n5\r\nslightly conservative\r\n6\r\nconservative\r\n7\r\nvery conservative\r\n\r\na. Ideology vs Religious Services Responses Plot\r\n\r\n\r\n\r\nb. Summary Statistics for the Religious Services and Political Ideology responses\r\n\r\nPI\r\nRE\r\nMin.\r\n1\r\n1\r\n1st Qu.\r\n2\r\n1.75\r\nMedian\r\n2\r\n2\r\nMean\r\n3.03333333333333\r\n2.16666666666667\r\n3rd Qu.\r\n4\r\n3\r\nMax.\r\n7\r\n4\r\nThe median for the responses are that students go to religious services occasionally, and that the political ideology is liberal. The mean however, suggests that students still occasionally go to religious services occasionally, but that their political ideology is slightly liveral.\r\nc. Inferential Analyses\r\nCorrelation Test\r\nTest Statistic : 5.4162562\r\nP Value : 1.22113^{-6}\r\nAs far as the correlation test goes, it can be deemed that the correlation between the Political Ideology and Religious Services responses is statistically significant.\r\nHigh School GPA vs TV Watching Analysis\r\na. Plot outlining the relationship between GPA and the amount of TV watched\r\n\r\n\r\n\r\nb. Summary Statistics for High School GPA and Amount of TV Hours Watched\r\n\r\nTV\r\nHI\r\nMin.\r\n0\r\n2\r\n1st Qu.\r\n3\r\n3\r\nMedian\r\n6\r\n3.35\r\nMean\r\n7.26666666666667\r\n3.30833333333333\r\n3rd Qu.\r\n10\r\n3.625\r\nMax.\r\n37\r\n4\r\nThe lowest amount of TV watched is 0 hours, while the most amount of TV watched is 37 hours! On average, the amount of TV watched amongst this group is 7.3 hours, with the median being 6 hours.\r\nThe lowest GPA reported by the respondents is 2, with the highest being a 4. On average, the respondents have a 3.31 GPA with a median of a 3.35 GPA.\r\nc. Inferential Analyses\r\nCorrelation Test\r\nTest Statistic : -2.1143654\r\nP Value : 0.0387935\r\nThe correlation test shows that the correlation between hours of TV watched and GPA is statistically significant. Furthermore, the amount of TV watched has a negative relationship with GPA, reinforcing the cliche idea that student’s grades will suffer as more time spent watching on TV.\r\nQuestion 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\n\r\n\r\nhw6 <- read.csv(\"~/DACSS/DACSS603/hw6.csv\")\r\nhw6_ttest <- t.test(hw6$final, hw6$final_h1)\r\n\r\n\r\n\r\nI constructed a fake dataset, with one column displaying test scores that had the 10 people scoring 50, and another column scoring 60 on the next test, and not adjusting other test scores to average out to 70. After conducting a two sided T test with this scenario with a test statistic of -1 and a p-value of 0.319 , this test in itself suggests that there is not enough evidence to prove that the improvement was statistically significant.\r\nFurthermore, regression to the mean suggests that the most extreme of occurrences is not going to happen again and go back to the mean. In this testing case, the students that scored 50 will most likely score better the 2nd or 3rd time around.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomalexanderhong86876317/distill-preview.png",
    "last_modified": "2022-03-12T19:36:47-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomegeeslindacss-603-hw2/",
    "title": "HW2_DACSS603",
    "description": "DACSS 603 Homework 2: Linear Regression",
    "author": [
      {
        "name": "Eliza Geeslin",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nQuestion 1\n(Problem 1.1 in ALR)\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\nThe predictor is ppgdp and the response is fertility. So, in the scatterplot we will see that fertility is on the y-axis while ppgdp is on the x-axis.\n1.1.2. Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point(color = \"#4FB06D\") +\n  labs(y = \"Fertility (birth rate per 1000 females)\", x = \"PPGDP ($)\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nWe see that fertility decreases as ppgdp increases to a certain point, and then fertility appears to flatten out (around 1-2 births per female). Based on this graph it seems like a straight-line mean function is not the best summary of this graph.\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point(color = \"#4FB06D\") +\n  labs(y = \"Fertility (birth rate per 1000 females)\", x = \"PPGDP ($)\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nA simple linear regression does seem like a more plausible model for this graph. The mean function appears to be linear, and it is believable that there could be consistent varience across the points (although this is not completely certain). As we would expect, we see that there are a few outliers with either very high or very low fertility for their ppgdp.\nQuestion 2\n(Problem 9.47 in SMSS)\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n(a) How, if at all, does the slope of the prediction equation change?\nAssuming that the data itself is the same and the study was not re-done with based on income in Britain (just conversion), the new slope will be equal to the old slope divided by 1.33 (each British pound represents a smaller interval).\n(b) How, if at all, does the correlation change?\nThe correlation will not change (i.e. the relationship is the same regardless of the unit of income used to represent it).\nQuestion 3\n(Problem 1.5 in ALR)\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\n\n\n# use pairs() function\n\npairs(~ Year + APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE + BSAAM, \n      data = water,\n      col = \"#0068B1\",\n      pch = 20)\n\n\n\n\nSummary of relationship between variables based on this matrix:\nYear does not appear to be related to the the other variables (runoff or water levels).\nThe OPBPC, OPRC, and OPSLAKE sites seem to be correlated with each other. The plots that include two of these variables seem to show a dependence that is stronger than any dependence they have with the APSLAKE, APSAB, and APMAM sites.\nThe APSLAKE, APSAB, and APMAM sites also seem to be correlated with each other.\nBSAAM (the stream runoff) is more correlated with the OPBPC, OPRC, and OPSLAKE group. Perhaps this stream runoff drains to the OPBPC, OPRC, and OPSLAKE sites more than the others.\nIt seems like there is probably another stream that runsoff into the APSLAKE, APSAB, and APMAM sites that we are not accounting for here since they are all closely related to each other but not as much to BSAAM.\nQuestion 4\n(Problem 1.6 in ALR - slightly modified)\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\n\n\n# find the names of the variables\n# names(Rateprof)\n\n#use pairs() to graph matrix\npairs(~ quality + clarity + helpfulness + easiness + raterInterest, \n      data = Rateprof,\n      xlim = c(1,5),\n      ylim = c(1,5),\n      xaxs = \"i\", \n      yaxs = \"i\",\n      col = \"#FF5C35\",\n      pch = 20)\n\n\n\n\nSummary of relationship between variables based on this matrix:\nThere is a strong correlation between the quality, clarity, and helpfulness variables. This indicates that professors who rate highly in one of these categories may also rate highly in the other two.\nThere are weaker relationships between easiness and raterInterest and the rest of the variables.\nQuestion 5\n(Problem 9.34 in SMSS)\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\nUse graphical ways to portray the individual variables and their relationship.\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\nSummarize and interpret results of inferential analyses.\n\n\n# downloading the data\ndata(\"student.survey\")\n\n# taking a look at each variable.\n# ?student.survey\n\n#select the variables we need. \nstudent.survey <- student.survey %>%\n  select(c(pi, re, hi, tv))\n\nstr(student.survey)\n\n\n'data.frame':   60 obs. of  4 variables:\n $ pi: Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re: Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ hi: num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ tv: num  3 15 0 5 6 4 5 5 7 1 ...\n\nhead(student.survey)\n\n\n            pi           re  hi tv\n1 conservative   most weeks 2.2  3\n2      liberal occasionally 2.1 15\n3      liberal   most weeks 3.3  0\n4     moderate occasionally 3.5  5\n5 very liberal        never 3.1  6\n6      liberal occasionally 3.5  4\n\n\n\n# graph to portray individual variables and their relationship\n\nplot(pi ~ re, data = student.survey)\n\n\n\n\nUsing the datatable() and descriptive graph above, we see that for political ideology we see that the scale consists of very conservative, conservative, slightly conservative, moderate, slightly liberal, liberal, very liberal. The religiosity variable is how often the respondent attends religious services, and the options are: never, occasionally, most weeks, and every week.\n\n\n# graph to portray individual variables and their relationship\n\nplot(hi ~ tv, data = student.survey)\n\n\n\n\nHigh School GPA is on a 4 point scale (from 1 to 4) and TV is hours of watching TV.\nNext, in order to summarize these variables using descriptive statistics, I am going to assign numerical values to the political ideology and religiosity values as follows:\nvery conservative = 7, conservative = 6, slightly conservative = 5, moderate = 4, slightly liberal = 3, liberal = 2, very liberal = 1\nnever = 1, occasionally = 2, most weeks = 3, and every week = 4\n\n\n# re-assign values as integers and recreate table so I can double check\nstudent.survey <- student.survey %>%\n  add_column(pi_num = (as.integer(as.factor(student.survey$pi))), .after = 1)\n\nstudent.survey <- student.survey %>% \n  add_column(re_num = (as.integer(as.factor(student.survey$re))), .after = 3)\n             \nhead(student.survey)\n\n\n            pi pi_num           re re_num  hi tv\n1 conservative      6   most weeks      3 2.2  3\n2      liberal      2 occasionally      2 2.1 15\n3      liberal      2   most weeks      3 3.3  0\n4     moderate      4 occasionally      2 3.5  5\n5 very liberal      1        never      1 3.1  6\n6      liberal      2 occasionally      2 3.5  4\n\n\n\n#select only numerical values\n\nstudent.survey <- student.survey %>%\n  select(c(pi_num, re_num, hi, tv))\n\n#summarize data\nsummary(student.survey)\n\n\n     pi_num          re_num            hi              tv        \n Min.   :1.000   Min.   :1.000   Min.   :2.000   Min.   : 0.000  \n 1st Qu.:2.000   1st Qu.:1.750   1st Qu.:3.000   1st Qu.: 3.000  \n Median :2.000   Median :2.000   Median :3.350   Median : 6.000  \n Mean   :3.033   Mean   :2.167   Mean   :3.308   Mean   : 7.267  \n 3rd Qu.:4.000   3rd Qu.:3.000   3rd Qu.:3.625   3rd Qu.:10.000  \n Max.   :7.000   Max.   :4.000   Max.   :4.000   Max.   :37.000  \n\n\n\n# use stat.desc() to summarize data\nstat.desc(student.survey)\n\n\n                  pi_num      re_num           hi          tv\nnbr.val       60.0000000  60.0000000  60.00000000  60.0000000\nnbr.null       0.0000000   0.0000000   0.00000000   5.0000000\nnbr.na         0.0000000   0.0000000   0.00000000   0.0000000\nmin            1.0000000   1.0000000   2.00000000   0.0000000\nmax            7.0000000   4.0000000   4.00000000  37.0000000\nrange          6.0000000   3.0000000   2.00000000  37.0000000\nsum          182.0000000 130.0000000 198.50000000 436.0000000\nmedian         2.0000000   2.0000000   3.35000000   6.0000000\nmean           3.0333333   2.1666667   3.30833333   7.2666667\nSE.mean        0.2112201   0.1261482   0.05934157   0.8672043\nCI.mean.0.95   0.4226505   0.2524220   0.11874221   1.7352718\nvar            2.6768362   0.9548023   0.21128531  45.1225989\nstd.dev        1.6361040   0.9771398   0.45965782   6.7173357\ncoef.var       0.5393749   0.4509876   0.13893939   0.9244040\n\nNow we have a lot of information:\npolitical ideology - the medium ideology was a 2 which is liberal while the mean ideology was a ~3 which is slightly liberal. Using the stat.desc() means we also know the standard deviation is 1.64 and the 95% confidence interval of the mean is 3.03 ± .24.\nreligiosity - the median is 2 while the mean is ~2 which is occasionally attending religious services. The standard deviation is .98 and the 95% confidence interval of the mean is 2 ± .25.\nhigh school GPA - the median GPA was a 3.35 while the mean was 3.31; the maximum GPA was a 4.0 and the minimum was a 2.0.\nhours watching TV - the median hours watched was 6 hours and the mean was 7.27 hours. The minimum hours watched was 0 while the maximum was 37 hours.\n\n\n# plot the relationship between religiosity and political ideology - scatterplot\nggplot(data = student.survey, aes(x = re_num, y = pi_num)) +\n  geom_point(color = \"#5C62D6\") +\n  labs(x = \"Religiosity (frequency of attending services)\", y = \"Political Ideology\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nBased on this it looks like there is weak evidence of a correlation between religiosity (as measured by attendence) and political ideology from this graph. Slightly conservative, moderate, slightly liberal and liberal respondents vary from never, occasionally, most weeks, and every week service attendance. However, we do see that on either end (very liberal and conservative/very conservative) there is some relationship between ideology and religiosity.\n\n\n# plot the relationship between HS GPA and hours of TV watched - scatterplot\nggplot(data = student.survey, aes(y = hi, x = tv)) +\n  geom_point(color = \"#ED2D40\") +\n  labs(x = \"TV watched per week (Avg Hrs.)\", y = \"High School GPA\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nThis is a repeat of the summary graph above (plot()) but we do glean some interesting information. In general, most students are watching less than 15 hours of TV on average per week. Most respondents to this survey had a GPA of 3.0 or greater. It seems like there is not a strong relationship between the average number of hours of TV watched per week and the student’s GPA.\nFinally, I used cor.test() to test the correlation between each of these pairs. HO is that the correlation coefficient = 0, while HA is that the correlation coefficient =/= 0.\n\n\n# correlation test between political ideology and religiosity using cor.test()\n\ncor.test(student.survey$pi_num, student.survey$re_num)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  student.survey$pi_num and student.survey$re_num\nt = 5.4163, df = 58, p-value = 1.221e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3818345 0.7265650\nsample estimates:\n      cor \n0.5795661 \n\nFor political ideology and religiosity (how often you attend religious services) there is a p-value of less than the 0.05 significance level. Since the p-value is smaller than 0.05, we can reject the null hypothesis (that there is no relationship). So, we can assume that these two do have a significant relationship to each other. The estimated correlation/correlation coefficient is 0.560 so the correlation shows a modest positive correlation.\n\n\n# correlation test between GPA and TV using cor.test()\n\ncor.test(student.survey$hi, student.survey$tv)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  student.survey$hi and student.survey$tv\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nFor the relationship between high school GPA and the average hours of TV watched per week there is a p-value of 0.039 which is less than the 0.05 significance level. Therefore, we can assume that these two do have a significant relationship to each other. Since the p-value is .039 we can reject the null hypothesis. However, the relationship is not as significant as the one we looked at above. Additionally, the estimated correlation is -0.268, so there is a small negative correlation.\nQuestion 6\n(Problem 9.50 in SMSS)\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\nWe don’t have enough evidence (enough data) to imply that the tutoring program was responsible for the increased scores. Regression towards the mean is the idea that if one sample of a random variable is extreme, the next sampling of the same random variable is likely to be closer to its mean and extreme events are likely to be followed by more typical ones. In the example in the question, the group that was selected to receive tutoring was the “extreme” group (performing the lowest on the midterm exam). On the next exam (the final), the average for the class was 70 and the 10 students in the extreme group’s average regressed toward the mean. Therefore, we do not know if the increase was due to the tutoring.\n\n\n\n",
    "preview": "posts/httpsrpubscomegeeslindacss-603-hw2/distill-preview.png",
    "last_modified": "2022-03-12T19:36:51-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomjflatteryhomework2/",
    "title": "Homework 2",
    "description": "Homework 2",
    "author": [
      {
        "name": "Justin Flattery",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\n\nlibrary('smss')\nlibrary('alr4')\n\n\n\n1\n1.1.1\n\n\n\nPredictor variable is ppdg and response variable is fertility\n1.1.2\n\n\nplot(x = UN11$ppgdp, y = UN11$fertility)\n\n\n\n\nA straight line mean function does not seem to be plausible for a summary of this graph. It appears to be a non linear relationship between the x and y axis.\n1.1.3\n\n\nplot(x = log(UN11$ppgdp), y = log(UN11$fertility))\n\n\n\n\nYes! There seems to be a linear relationship when using the log of these values. Therefore It appears a simple linear regression would be plausible for this graph\n2\na\nThe slope of the prediction will change by 1/1.6 (=0.625) since previously it was predicting slope based on every 1 unit changein the explanatory variable. Now the rate of that one unit is decreasing to equal the same change in the response variable\nb\nThe overall correlation will not change in the regression analysis since the units of the explanatory variable are all moving at an equal rate.\n3\n\n\ndata('water')\n\n\n\n\n\npairs(~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE + BSAAM, data = water)\n\n\n\n\nUsing the information from these plots, we can see a linear relationship appears plausible to explain the OP sites and Bishop runoff, however the relationship for AP sites and runoff seems to be more randomly related.\n4\n\n\ndata('Rateprof')\n\npairs(~quality+helpfulness+clarity+easiness+raterInterest,data=Rateprof,\n      main=\"Professor Rating Scatterplot Matrix\")\n\n\n\n\nAccording to this scatterplot there is a positive linear relationship between the variables of quality, helpfulness and clarity. However the variables of easiness and raterInterest do not appear correlated or have a linear relationship with any of the other variables analyzed\n5\n\n\ndata('student.survey')\n\n\n\n(a)\n\n\nplot(x = student.survey$re, y = student.survey$pi)\n\n\n\n\n\n\nplot(x = student.survey$tv, y = student.survey$hi)\n\n\n\n\nb)\ni)\nFor political ideology, there is a scale from very conservative to moderate to liberal Religiosity is measured by how often you attend a religious service It appears according to the plot in a that the more religious service attended, the more conservative individuals were.\nii)\nFor high school GPA, ranked on scale from 1-4 hours of watching TV is measured by average number of hours per week that individuals watched TV It appears according to the plot in a that the more tv watched, the lower individuals GPA were.\nc)\ni)\n\n\nsummary(plot(x = student.survey$re, y = student.survey$pi)) \n\n\n\nNumber of cases in table: 60 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 42.29, df = 18, p-value = 0.001008\n    Chi-squared approximation may be incorrect\n\nSince this is a categorical comparison, regression can not necessarily be run (without substituing dummy variables) However, we can see the p value for this relationship is <0.01 and therefore it appears there is a significant relationship\nii)\nFirst, running a regression analysis\n\n\nlm(re ~ pi, data = student.survey)\n\n\n\nCall:\nlm(formula = re ~ pi, data = student.survey)\n\nCoefficients:\n(Intercept)         pi.L         pi.Q         pi.C         pi^4  \n     2.6071       2.0552       0.4501       0.1361      -0.2283  \n       pi^5         pi^6  \n    -0.3046       0.4565  \n\n\n\nsummary(lm(tv ~ hi, data = student.survey))\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.600 -3.790 -1.167  2.408 27.746 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   20.200      6.175   3.271   0.0018 **\nhi            -3.909      1.849  -2.114   0.0388 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.528 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\nI see there is a significant relationship with a low p value, however a low r2 value. However, viewing our plot, it appears a logarithmic relationship may be a better fit for this data\n\n\nsummary(lm((tv) ~ log(hi), data = student.survey))\n\n\n\nCall:\nlm(formula = (tv) ~ log(hi), data = student.survey)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.846 -4.035 -1.172  2.508 27.933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   20.933      6.717   3.116  0.00285 **\nlog(hi)      -11.525      5.619  -2.051  0.04481 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.542 on 58 degrees of freedom\nMultiple R-squared:  0.06761,   Adjusted R-squared:  0.05154 \nF-statistic: 4.206 on 1 and 58 DF,  p-value: 0.04481\n\nNow we see a higher r^2 value an inverse relationship between the hours of tv watched and log of an individuals GPA.The equation relating these two is Log(GPA) = -11.525 * hours of tv watched + 21\n6\nSince these students were the lowest performers in the previous test, they are more likely to have improved test scores closer to the mean in the next test. The findings are in line with the concept of regression toward the mean. For example, if we took the top 10 highest perfoming students and provided them tutoring, their test scores are actually expected to get closer to the mean in the next sample (i.e go down!). We could not conclude therefore that tutoring had a negative affect, similarly we do not have enough evidence with the information provided to state the tutoring program is successful.\n\n\n\n",
    "preview": "posts/httpsrpubscomjflatteryhomework2/distill-preview.png",
    "last_modified": "2022-03-12T19:36:55-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomofyalcinhw2-sol/",
    "title": "Homework 2 | Solutions",
    "description": "Solutions to Homework 2",
    "author": [
      {
        "name": "Omer Yalcin",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nQuestion 1\n\n\ndata(UN11)\n\n\n\n1.1.1\nThe predictor is ppgdp, i.e. GDP per capita. The response is fertility, the birth rate per 1000 women.\n1.1.2.\n\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\nA straight line is not appropriate, because the relationship has an L-shaped structure.\n1.1.2\n\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\nYes, now a simple linear regression model is more plausible. We can imagine a negative-sloped straight line going through those points.\nQuestion 2\n(a)\nThe conversion from USD to British pound will mean the numerical value of the response will be divided by 1.33. To offset that, the slope will also become divided by 1.33.\n(b)\nCorrelation will not change because it is a standardized measure that is not influenced by the unit of measurement.\nBoth outcomes can easily be shown via simulation.\nQuestion 3\n\n\ndata(water)\npairs(water)\n\n\n\n\nYear appears to be largely unrelated to each of the other variables\nthe three variables starting with “O” seem to be correlated with each other, meaning that all the plot including two of these variables exhibit a dependence between the variables that is stronger than the dependence between the “O” variables and other variables. The three variables starting with “A” also seem to be another correlated group\nBSAAM is more closely related to the “O” variables than the “A” variables\nQuestion 4\n\n\ndata(Rateprof)\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\n\n\n\nThe very strong pair-wise correlation among quality, clarity, and helpfulness is very striking. easiness is also correlated fairly highly with the other three. raterInterest is also moderately correlated, but raters almost always say they are at least moderately interested in the subject. Overall, the results might show that people don’t necessarily distinguish all these dimensions very well in their minds—or that professors that do one in one dimension tend to do well on the others too.\nQuestion 5\na\nOne way of visually representing the relationship between religiosity and political ideology is as follows (and there are other ways). As we go towards bars to the right (more religiousity), we see lighter colors pop up (more conservatism)\n\n\ndata(student.survey)\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\n\n\n\nThe relationship between high school GPA and hours of watching TV can be shown with a good old scatter plot.\n\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\n\n\n\nb\nWe can get some descriptive statistics with:\n\n\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nFor the categorical variables, we have somewhat of a skewed sample: not all categories have a similar number of people. While the high school GPA distribution looks like a normal-looking unimodal distribution, the hours of TV variable has quite a few outliers, apparent from the difference between 3rd quartile and the maximum value While the high school GPA distribution looks like a normal-looking unimodal distribution, the TV variable has some outliers, apparent from the difference between 3rd quartile and the maximum value.\nc\nDealing with ordinal variables in linear regression is a difficult problem. We’ll just go ahead and assume that we can just convert them to numeric and use them. This would be done for political ideology and religiosity. High school GPA and hours of TV are already continuous.\n\n\nm1 <- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\nm2 <- lm(hi ~ tv, data = student.survey)\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\n\n\n==========================================================\n                                  Dependent variable:     \n                              ----------------------------\n                               Pol. Ideology     HS GPA   \n                                    (1)            (2)    \n----------------------------------------------------------\nReligiosity                       0.970***                \n                                  (0.179)                 \n                                                          \nHours of TV                                     -0.018**  \n                                                 (0.009)  \n                                                          \nConstant                          0.931**       3.441***  \n                                  (0.425)        (0.085)  \n                                                          \n----------------------------------------------------------\nObservations                         60            60     \nR2                                 0.336          0.072   \nAdjusted R2                        0.324          0.056   \nResidual Std. Error (df = 58)      1.345          0.447   \nF Statistic (df = 1; 58)         29.336***       4.471**  \n==========================================================\nNote:                          *p<0.1; **p<0.05; ***p<0.01\n\nReligiosity is positively and statistically significantly (at the 0.01 significance level) associated with conservatism.\nHours of TV is negatively and statistically significantly (at the 0.05 significance level) associated with High School GPA. Watching an average of 1 more hour of TV per week is associated with a 0.018 decline in High School GPA.\nQuestion 6\nRegression toward the mean implies that extreme values will always occur by chance and in a next iteration of the test / experiment, those observations’ expected value will still be whatever the previous mean was. In this sense, they’re expected to regress toward the mean.\nIn this example, the tutored students might have been chosen from those who performed very poorly by chance, and the improvement might just be an artifact of regression toward the mean.\n\n\n\n",
    "preview": "posts/httpsrpubscomofyalcinhw2-sol/distill-preview.png",
    "last_modified": "2022-03-12T19:37:00-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomchester876831/",
    "title": "Homework Two",
    "description": "DACSS 603",
    "author": [
      {
        "name": "Cynthia Hester",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\n\nContents\nQuestion 1\nSolution\n\nQuestion 2\nSolution\n\nQuestion 3\nSolution\n\nQuestion 4\nSolution\n\nQuestion 5\nSolution\n\nQuestion 6\nSolution\n\n\nQuestion 1\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nSolution\n1.1.1. Identify the predictor and the response.\nThe predictor is ppgdp. This is because the problem is studying the dependence of fertility on ppgdp (gross national product per person) which is independent/explanatory.\nThe response variable is fertility. This is because fertility is the dependent variable in relation to ppgdp.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph? \nFirst, we import the data from the United Nations (Datafile:UN11) contained in data accompanying the ALR text book data set.\n\n\nhide\n\ndata(\"UN11\")                      #load UN11 data, which contains\nUnited_Nations_11<-UN11           #\"UN11\" renamed for easier readability\nhead(United_Nations_11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\nI now separate the two variables, fertility and ppgdp from the UN11 dataset for inspection\n\n\nhide\n\n#separates fertility and ppgdp from dataset\n\nUnited_Nations_11 <- United_Nations_11 %>%\n  select(c(fertility,ppgdp))\nhead(United_Nations_11,5)               # First five rows of variables fertility and ppgdp verified\n\n\n            fertility   ppgdp\nAfghanistan     5.968   499.0\nAlbania         1.525  3677.2\nAlgeria         2.142  4473.0\nAngola          5.135  4321.9\nAnguilla        2.000 13750.1\n\nHere I use a table to represent the variables extracted from the UN11 data\n\n\nhide\n\nkable(head(United_Nations_11),format = \"markdown\",digits = 3,colnames = c('fertility','ppgdp'),\n      caption = \"United Nations Fertility and Gross National Product Per Person in USD\")\n\n\nTable 1: United Nations Fertility and Gross National Product Per Person in USD\n\nfertility\nppgdp\nAfghanistan\n5.968\n499.0\nAlbania\n1.525\n3677.2\nAlgeria\n2.142\n4473.0\nAngola\n5.135\n4321.9\nAnguilla\n2.000\n13750.1\nArgentina\n2.172\n9162.1\n\nHere I look at a summary of the two variables as well as remove any missing data\n\n\nhide\n\nis.na(United_Nations_11) %>% #removes missing values from variables we\n  str()                      #provides a concise overview of the data set\n\n\n logi [1:199, 1:2] FALSE FALSE FALSE FALSE FALSE FALSE ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:199] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ : chr [1:2] \"fertility\" \"ppgdp\"\n\nI now look at a numerical summary of the two variables to get a more granular overview of the data.\n\n\nhide\n\n  summary(United_Nations_11)                 #numerical summary\n\n\n   fertility         ppgdp         \n Min.   :1.134   Min.   :   114.8  \n 1st Qu.:1.754   1st Qu.:  1283.0  \n Median :2.262   Median :  4684.5  \n Mean   :2.761   Mean   : 13012.0  \n 3rd Qu.:3.545   3rd Qu.: 15520.5  \n Max.   :6.925   Max.   :105095.4  \n\nVariables fertility and ppgdp renamed for better understandability\n\n\nhide\n\nUnited_Nations_rename<-UN11 %>%\n  rename(Gross_National_Product_Per_Person_USD=ppgdp)%>%\n  rename(Fertility_Birthrate_per_1000_females_from_2009=fertility)\n\n\n\nWe now look at the scatterplot of the UN11 (renamed\nUnited_Nations_11) data using fertility on the vertical axis versus ppgdp on the horizontal axis.\n\n\nhide\n\nUnited_Nations_11<-UN11\nggplot(data = UN11, aes(x = ppgdp ,y = fertility))+\n  geom_point(color=5)+\n  labs(title = \"Fertility vs United Nations Gross National Product Per Person USD\")\n\n\n\n\nAnalysis\nThis linear regression scatter plot does not appear to be an effective summary of the data. The mean is not linear and the variance is not constant. This could be partly attributed to the crowding of the x-axis and y-axis. Therefore, using natural logarithms for the x-axis and y-axis would be warranted to determine if there is any indication of the plausibility of a straight-line mean function.\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nScatterplot reflecting Natural Log of variables fertility vs. ppgdp\n\n\nhide\n\nUnited_Nations_11<-UN11\nggplot(data = UN11,aes(x = log(ppgdp),y = log(fertility)))+\n  geom_point(color=5)+\n  labs(title = \"Natural_Log of Fertility vs UN Gross National Product Per Person USD\")\n\n\n\n\nScatterplot reflecting Natural Log of variables fertility vs. ppgdp with linear regression\n\n\nhide\n\nUnited_Nations_11<-UN11\nggplot(data = UN11,aes(x = log(ppgdp),y = log(fertility)))+\n  geom_point(color=5)+\n  geom_smooth(method =\"lm\")+\n  labs(title = \"Natural_Log of Fertility vs UN Gross National Product Per Person USD\")\n\n\n\n\nAnalysis\nImplementation of the natural logarithmic scale for the UN11 scatterplot appears to indicate an effective representation of a linear regression. Compared to the previous scattorplot, this plot appears to be linear, and the variance seems to be plausible. Therefore, the relationship between fertility and gross domestic product(ppgdp )is linear.\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\nSolution\nPart A\nHow, if at all, does the slope of the prediction equation change?\nWhen responses are converted to British pounds sterling the slope changes,thus the slope increases by 1.33 times the original. This is because there is an inverse relationship between the slope,and the explanatory variables.\nPart B\nHow, if at all, does the correlation change?\nThere will be no change in the correlation. This is because the strength and pattern (correlation) cannot be affected by change in units.\nQuestion 3\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM.\nDraw the scatterplot matrix for these data and summarize the information available from these plots.\nSolution\nFirst, we load the (Data files: water) and check the structure of the data\n\n\nhide\n\ndata(\"water\")                       #importing water data set\nhead(water,5)                       #looking at first 5 rows of data set\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n\n\n\nhide\n\nstr(water)                          #concise look at data frame\n\n\n'data.frame':   43 obs. of  8 variables:\n $ Year   : int  1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 ...\n $ APMAM  : num  9.13 5.28 4.2 4.6 7.15 9.7 5.02 6.7 10.5 9.1 ...\n $ APSAB  : num  3.58 4.82 3.77 4.46 4.99 5.65 1.45 7.44 5.85 6.13 ...\n $ APSLAKE: num  3.91 5.2 3.67 3.93 4.88 4.91 1.77 6.51 3.38 4.08 ...\n $ OPBPC  : num  4.1 7.55 9.52 11.14 16.34 ...\n $ OPRC   : num  7.43 11.11 12.2 15.15 20.05 ...\n $ OPSLAKE: num  6.47 10.26 11.35 11.13 22.81 ...\n $ BSAAM  : int  54235 67567 66161 68094 107080 67594 65356 67909 92715 70024 ...\n\nhide\n\nsummary(water)                      #provides numerical overview of data\n\n\n      Year          APMAM            APSAB           APSLAKE     \n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \n     OPBPC             OPRC           OPSLAKE           BSAAM       \n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \n\nFor better readability and understandability of the data, a table is created\n\n\nhide\n\nlibrary(DT)\ndatatable(head(water,25), options = list(\n  columnDefs = list(list(className = 'dt-center',targets = 7)),\n  rownames = T,\n  pageLength = 5,\n  autowidth = T,\n  lengthMenu =c(5,10,15,20)\n))\n\n\n\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"],[1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972],[9.13,5.28,4.2,4.6,7.15,9.7,5.02,6.7,10.5,9.1,8.75,8.1,3.75,10.15,6.15,12.75,7.35,11.25,4.05,12.65,4.65,5.35,4.05,5.9,9.45],[3.58,4.82,3.77,4.46,4.99,5.65,1.45,7.44,5.85,6.13,5.23,3.77,1.47,5.09,3.52,8.17,4.33,6.56,1.9,6.62,3.84,3.62,1.98,5.72,4.82],[3.91,5.2,3.67,3.93,4.88,4.91,1.77,6.51,3.38,4.08,5.9,4.56,1.78,4.86,3.3,10.16,4.85,7.6,2,7.14,3.34,4.62,2.94,5.42,6.79],[4.1,7.55,9.52,11.14,16.34,8.88,13.57,9.28,21.2,9.55,15.25,9.05,4.57,8.9,16.9,16.75,5.25,8.4,10.85,23.25,7.1,43.37,8.95,8.45,7.9],[7.43,11.11,12.2,15.15,20.05,8.15,12.45,9.65,18.55,9.2,14.8,6.85,6.1,7.15,14.75,11.55,7.45,13.2,8.25,17,6.8,24.85,11.25,10.9,7.6],[6.47,10.26,11.35,11.13,22.81,7.41,13.32,9.8,17.42,8.25,17.48,9.56,7.65,9,17.68,15.53,8.2,13.29,12.56,23.66,8.28,33.07,11,10.82,8.06],[54235,67567,66161,68094,107080,67594,65356,67909,92715,70024,99216,55786,46153,47947,76877,88443,54634,78806,56542,116244,60857,146345,73726,65530,60772]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Year<\\/th>\\n      <th>APMAM<\\/th>\\n      <th>APSAB<\\/th>\\n      <th>APSLAKE<\\/th>\\n      <th>OPBPC<\\/th>\\n      <th>OPRC<\\/th>\\n      <th>OPSLAKE<\\/th>\\n      <th>BSAAM<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-center\",\"targets\":7},{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,8]},{\"orderable\":false,\"targets\":0}],\"rownames\":true,\"pageLength\":5,\"autowidth\":true,\"lengthMenu\":[5,10,15,20],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nRemoving any missing data\n\n\nhide\n\nis.na(water) %>%     #removes missing values from variables we are working with\n  summary()          #summary of data set, provides numerical insight\n\n\n    Year           APMAM           APSAB          APSLAKE       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:43        FALSE:43        FALSE:43        FALSE:43       \n   OPBPC            OPRC          OPSLAKE          BSAAM        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:43        FALSE:43        FALSE:43        FALSE:43       \n\nDraw the scatterplot matrix for these data and summarize the information available from these plots.\nWe now draw a scatterplot matrix for the water data for better understandably\n\n\nhide\n\n#Since data has already been imported we can rename the water variable\nwater_supply<-water\nhead(water_supply,5)            #first five rows of dataset\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n\nScatterplot matrix for water data\n\n\nhide\n\npairs(water_supply,main = \"Sierra Southern California Water Supply Runoff\",\n      pch = 21, bg = \"green\")\n\n\n\n\nFor more granular insight, we run a simple linear regression on the water data set\n\n\nhide\n\n#Simple linear regression of water data set\n\nlm_water_supply<-lm(BSAAM~APMAM+APSAB+APSLAKE+OPBPC+OPRC+OPSLAKE,data = water)\nsummary(lm_water_supply)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\nAnalysis:\nStarting with variable Year we see that it does not seem to be appear to be particularly related to any of the other variables.APSLAKE,APSAB and APMAM appear to be correlated, and it is observed in relation to the runoff variable BSAAM they do not seem to appear to have a strong correlation.\nIt appears that OPSLAKE and OPRC are strongly correlated as well being positively linear. This suggests statistical significance of the p-value in the linear regression Pr(>|t|)< 0.05.The BSAAM variable appears to be strongly correlated with them as well.Since these variables appear to be correlated with each other, it may cause a multicollinearity problem. Furthermore, with a p-value: < 2.2e-16 the model appears to be statistically significant. The residuals display of the regression a wide disparity from a Minimum of -12690 to a Maximum of 18542. This may suggest outliers in the model.\nFinally, the multiple R-squared: value of 0.9248 and the Adjusted R-squared: 0.9123  values are close to 1 which suggests there is minimal overfitting.\nQuestion 4\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\nSolution\nFirst, the data (Data file: Rateprof) is imported from the ALR4 dataset\n\n\nhide\n\ndata(\"Rateprof\")\nhead(Rateprof,5)                           #first 5 rows of data\n\n\n  gender numYears numRaters numCourses pepper discipline\n1   male        7        11          5     no        Hum\n2   male        6        11          5     no        Hum\n3   male       10        43          2     no        Hum\n4   male       11        24          5     no        Hum\n5   male       11        19          7     no        Hum\n               dept  quality helpfulness  clarity easiness\n1           English 4.636364    4.636364 4.636364 4.818182\n2 Religious Studies 4.318182    4.545455 4.090909 4.363636\n3               Art 4.790698    4.720930 4.860465 4.604651\n4           English 4.250000    4.458333 4.041667 2.791667\n5           Spanish 4.684211    4.684211 4.684211 4.473684\n  raterInterest sdQuality sdHelpfulness sdClarity sdEasiness\n1      3.545455 0.5518564     0.6741999 0.5045250  0.4045199\n2      4.000000 0.9020179     0.9341987 0.9438798  0.5045250\n3      3.432432 0.4529343     0.6663898 0.4129681  0.5407021\n4      3.181818 0.9325048     0.9315329 0.9990938  0.5882300\n5      4.214286 0.6500112     0.8200699 0.5823927  0.6117753\n  sdRaterInterest\n1       1.1281521\n2       1.0744356\n3       1.2369438\n4       1.3322506\n5       0.9749613\n\nMissing values are removed and then a summary of the data is run,to gain insight into the structure of the data.\n\n\nhide\n\nis.na(Rateprof) %>%        #removes missing values from variables we\nsummary()                  #insight into numerical data\n\n\n   gender         numYears       numRaters       numCourses     \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n   pepper        discipline         dept          quality       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n helpfulness      clarity         easiness       raterInterest  \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n sdQuality       sdHelpfulness   sdClarity       sdEasiness     \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n sdRaterInterest\n Mode :logical  \n FALSE:366      \n\nhide\n\nstr(Rateprof)\n\n\n'data.frame':   366 obs. of  17 variables:\n $ gender         : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ numYears       : int  7 6 10 11 11 10 7 11 11 7 ...\n $ numRaters      : int  11 11 43 24 19 15 17 16 12 18 ...\n $ numCourses     : int  5 5 2 5 7 9 3 3 4 4 ...\n $ pepper         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ discipline     : Factor w/ 4 levels \"Hum\",\"SocSci\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dept           : Factor w/ 48 levels \"Accounting\",\"Anthropology\",..: 17 42 3 17 45 45 45 17 34 17 ...\n $ quality        : num  4.64 4.32 4.79 4.25 4.68 ...\n $ helpfulness    : num  4.64 4.55 4.72 4.46 4.68 ...\n $ clarity        : num  4.64 4.09 4.86 4.04 4.68 ...\n $ easiness       : num  4.82 4.36 4.6 2.79 4.47 ...\n $ raterInterest  : num  3.55 4 3.43 3.18 4.21 ...\n $ sdQuality      : num  0.552 0.902 0.453 0.933 0.65 ...\n $ sdHelpfulness  : num  0.674 0.934 0.666 0.932 0.82 ...\n $ sdClarity      : num  0.505 0.944 0.413 0.999 0.582 ...\n $ sdEasiness     : num  0.405 0.505 0.541 0.588 0.612 ...\n $ sdRaterInterest: num  1.128 1.074 1.237 1.332 0.975 ...\n\nFive variables we are focused on extracted from Rateprof dataset.\n\n\nhide\n\n#data set renamed for subset\nrate_my_prof<-Rateprof\ncolnames(rate_my_prof)            #column names in dataset\n\n\n [1] \"gender\"          \"numYears\"        \"numRaters\"      \n [4] \"numCourses\"      \"pepper\"          \"discipline\"     \n [7] \"dept\"            \"quality\"         \"helpfulness\"    \n[10] \"clarity\"         \"easiness\"        \"raterInterest\"  \n[13] \"sdQuality\"       \"sdHelpfulness\"   \"sdClarity\"      \n[16] \"sdEasiness\"      \"sdRaterInterest\"\n\nFive variable subset of RateProf dataset\n\n\nhide\n\n#subset of RateProf data\n\nrate_my_prof<-select(Rateprof,c('quality','helpfulness','clarity','easiness','raterInterest'))\nhead(rate_my_prof)     #first 5 rows of data set\n\n\n   quality helpfulness  clarity easiness raterInterest\n1 4.636364    4.636364 4.636364 4.818182      3.545455\n2 4.318182    4.545455 4.090909 4.363636      4.000000\n3 4.790698    4.720930 4.860465 4.604651      3.432432\n4 4.250000    4.458333 4.041667 2.791667      3.181818\n5 4.684211    4.684211 4.684211 4.473684      4.214286\n6 4.233333    4.266667 4.200000 4.533333      3.916667\n\nTable of subset of data for better understandability\n\n\nhide\n\nkable(head(rate_my_prof),format = \"markdown\",digits = 5,\ncolnames = c('Quality','Helpfulness','Clarity','Easiness','RaterInterest'),caption\n= \"Rate My Professor\")\n\n\nTable 2: Rate My Professor\nquality\nhelpfulness\nclarity\neasiness\nraterInterest\n4.63636\n4.63636\n4.63636\n4.81818\n3.54545\n4.31818\n4.54545\n4.09091\n4.36364\n4.00000\n4.79070\n4.72093\n4.86047\n4.60465\n3.43243\n4.25000\n4.45833\n4.04167\n2.79167\n3.18182\n4.68421\n4.68421\n4.68421\n4.47368\n4.21429\n4.23333\n4.26667\n4.20000\n4.53333\n3.91667\n\nScatterplot Matrix of five RateProf variables\n\n\nhide\n\npairs(rate_my_prof,\n      col = \"green3\",\n      pch = 20,\n      main = \"Rate my Professor Matrix ScatterPlot \")\n\n\n\n\nProvide a brief description of the relationships between the five ratings\nInterpretation:\nWe see that if there is an intersection of any two variables then there is linear correlation of varying degrees of strength. Furthermore,if the correlation is not as linear then the correlation is weak.\nWe see that the relationship between some pairs of variables indicate better positive linear correlations than others.\nQuality-Clarity indicates a very strong positive linear correlation\nQuality-Happiness indicates a very strong positive linear correlation\nQuality-Easiness indicates a weak positive linear correlation\nQuality-RaterInterest indicates a weak positive linear correlation\nHelpfulness-Easiness indicates a weak linear correlation\nHelpfulness-RaterInterest indicates a weak linear correlation\nClarity-Helpfulness indicates a positive correlation\nClarity-RaterInterest indicates a weak positive correlation\nClarity-Easiness indicates a weak positive correlation\nEasiness and RaterInterest indicates a very weak positive correlation\nQuestion 5\n(Problem 9.34 in SMSS)\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\nUse graphical ways to portray the individual variables and their relationship.\n\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\n\nSummarize and interpret results of inferential analyses.\n\nSolution\nI first, import and inspect the student.survey dataset from SMSS text.\n\n\nhide\n\ndata(\"student.survey\")                   #import dataset\nstudent_survey_data<-student.survey      #renamed for better understandability\nhead(student_survey_data,5)              #inspects first five rows of data\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\n            re    ab    aa    ld\n1   most weeks FALSE FALSE FALSE\n2 occasionally FALSE FALSE    NA\n3   most weeks FALSE FALSE    NA\n4 occasionally FALSE FALSE FALSE\n5        never FALSE FALSE FALSE\n\nTo gain better understanding of the data I look at a summery, strings,and column names of the data set\n\n\nhide\n\ncolnames(student_survey_data)                 #column names of dataset\n\n\n [1] \"subj\" \"ge\"   \"ag\"   \"hi\"   \"co\"   \"dh\"   \"dr\"   \"tv\"   \"sp\"  \n[10] \"ne\"   \"ah\"   \"ve\"   \"pa\"   \"pi\"   \"re\"   \"ab\"   \"aa\"   \"ld\"  \n\nhide\n\nsummary(student_survey_data)                  #numeric structure of data\n\n\n      subj       ge           ag              hi       \n Min.   : 1.00   f:31   Min.   :22.00   Min.   :2.000  \n 1st Qu.:15.75   m:29   1st Qu.:24.00   1st Qu.:3.000  \n Median :30.50          Median :26.50   Median :3.350  \n Mean   :30.50          Mean   :29.17   Mean   :3.308  \n 3rd Qu.:45.25          3rd Qu.:31.00   3rd Qu.:3.625  \n Max.   :60.00          Max.   :71.00   Max.   :4.000  \n                                                       \n       co              dh             dr               tv        \n Min.   :2.600   Min.   :   0   Min.   : 0.200   Min.   : 0.000  \n 1st Qu.:3.175   1st Qu.: 205   1st Qu.: 1.450   1st Qu.: 3.000  \n Median :3.500   Median : 640   Median : 2.000   Median : 6.000  \n Mean   :3.453   Mean   :1232   Mean   : 3.818   Mean   : 7.267  \n 3rd Qu.:3.725   3rd Qu.:1350   3rd Qu.: 5.000   3rd Qu.:10.000  \n Max.   :4.000   Max.   :8000   Max.   :20.000   Max.   :37.000  \n                                                                 \n       sp               ne               ah             ve         \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Mode :logical  \n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 0.000   FALSE:60       \n Median : 5.000   Median : 3.000   Median : 0.500                  \n Mean   : 5.483   Mean   : 4.083   Mean   : 1.433                  \n 3rd Qu.: 7.000   3rd Qu.: 5.250   3rd Qu.: 2.000                  \n Max.   :16.000   Max.   :14.000   Max.   :11.000                  \n                                                                   \n pa                         pi                re         ab         \n d:21   very liberal         : 8   never       :15   Mode :logical  \n i:24   liberal              :24   occasionally:29   FALSE:60       \n r:15   slightly liberal     : 6   most weeks  : 7                  \n        moderate             :10   every week  : 9                  \n        slightly conservative: 6                                    \n        conservative         : 4                                    \n        very conservative    : 2                                    \n     aa              ld         \n Mode :logical   Mode :logical  \n FALSE:59        FALSE:44       \n NA's :1         NA's :16       \n                                \n                                \n                                \n                                \n\nhide\n\nstr(student_survey_data)\n\n\n'data.frame':   60 obs. of  18 variables:\n $ subj: int  1 2 3 4 5 6 7 8 9 10 ...\n $ ge  : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 2 2 1 2 2 ...\n $ ag  : int  32 23 27 35 23 39 24 31 34 28 ...\n $ hi  : num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ co  : num  3.5 3.5 3 3.2 3.5 3.5 3.7 3 3 3.1 ...\n $ dh  : int  0 1200 1300 1500 1600 350 0 5000 5000 900 ...\n $ dr  : num  5 0.3 1.5 8 10 3 0.2 1.5 2 2 ...\n $ tv  : num  3 15 0 5 6 4 5 5 7 1 ...\n $ sp  : int  5 7 4 5 6 5 12 3 5 1 ...\n $ ne  : int  0 5 3 6 3 7 4 3 3 2 ...\n $ ah  : int  0 6 0 3 0 0 2 1 0 1 ...\n $ ve  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ pa  : Factor w/ 3 levels \"d\",\"i\",\"r\": 3 1 1 2 2 1 2 2 2 2 ...\n $ pi  : Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re  : Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ ab  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ aa  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ ld  : logi  FALSE NA NA FALSE FALSE NA ...\n\nSubset of data extracted from student survey dataset we are inspecting\n\n\nhide\n\nstudent_survey_data<-student_survey_data %>% \n  select(c(pi,re,hi,tv))               #data subset for plots\n  head(student_survey_data,5)\n\n\n            pi           re  hi tv\n1 conservative   most weeks 2.2  3\n2      liberal occasionally 2.1 15\n3      liberal   most weeks 3.3  0\n4     moderate occasionally 3.5  5\n5 very liberal        never 3.1  6\n\nMissing values are removed and then a summary of the data is run to gain insight and inspect structure of the data.\n\n\nhide\n\nis.na(student_survey_data) %>%         #removes missing values from data \nsummary(student_survey_data)\n\n\n     pi              re              hi              tv         \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:60        FALSE:60        FALSE:60        FALSE:60       \n\nFor easier readability the data is placed in a table.\n\n\nhide\n\nkable(head(student_survey_data),format = \"markdown\",digits = 3,\ncol.names = c('Political_Ideology','Religiosity','HighSchoolGPA','HoursTVWatched'),caption\n= \"Student_Survey_Data\")\n\n\nTable 3: Student_Survey_Data\nPolitical_Ideology\nReligiosity\nHighSchoolGPA\nHoursTVWatched\nconservative\nmost weeks\n2.2\n3\nliberal\noccasionally\n2.1\n15\nliberal\nmost weeks\n3.3\n0\nmoderate\noccasionally\n3.5\n5\nvery liberal\nnever\n3.1\n6\nliberal\noccasionally\n3.5\n4\n\nPart A\nUse graphical ways to portray the individual variables and their relationship.\ni) Now that we know the structure of the data we can represent it visually with a plot. The first plot represents variables: y = political ideology(pi) vs x = religiosity(re).\n\n\nhide\n\nstudent_survey_plot<-plot(pi~re,data = student.survey,\n        #survey plot using plot function\n                          main = \"Political Ideology vs. Religiosity\")\n\n\n\n\nI use xtabs() function to gain further insight into the data since it is categorical.\n\n\nhide\n\ndata(\"student.survey\")\nxtabs(~pi+re,student.survey)\n\n\n                       re\npi                      never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nii)\nThe second plot is represented visually with the following variables: y = high school GPA(hi) and x = hours of TV watching(tv). Since the variables are numeric it is more straight forward for interpretation.\n\n\nhide\n\nstudent_survey_plot<-plot(hi~tv,data = student.survey,\n                          xlab=\"Hours of TV Watching\",\n                          ylab=\"High school GPA\",\n                          col = \"green\",\n                          main = \"High School GPA vs. Hours of TV \nWatching\")                #survey plot using plot function\n\n\n\n\nAnalysis:\nInspection of the Political Ideology(pi) vs. Religiosity(re) plot yielded very little meaningful information in its current form, as a categorical subset of the data. I used the xtabs() function to gain further insight in to the data since it is categorical. Whereas, the Hours of TV Watching(tv) and High school GPA(hi) data are numeric and did yield some insight. I will further explore any correlations in Part B.\nPart B\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\ni)\nI rename all 4 variables of the subset for better understandability\n\n\nhide\n\nstudent_survey_rename<-student.survey %>% \n  rename(Political_Ideology=pi) %>% \n  rename(Religiosity=re) %>% \n  rename(Hours_of_TV=tv) %>% \n  rename(HighSchool_GPA=hi)\n\n\n\nI then use my favorite tool, the table to gain further insight into the political ideology and religiosity variables.\n\n\nhide\n\ndata(\"student.survey\")\nxtabs(~Political_Ideology+Religiosity,student_survey_rename)\n\n\n                       Religiosity\nPolitical_Ideology      never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nSummary of 4 variables pi,re,hi,tv\n\n\nhide\n\nsummary(student_survey_data)\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nInterpretation:\nThe relationship between the two categorical variables Political Ideology and Religiosity , where Political Ideology is the dependent variable and Religiosity independent, yields the following insight based on the xtabs and summary analysis. A mode of 24 for liberal Political Ideology associated to a mode of 29 for the liberal sample Religiosity occasionally. This result coincides with the xtabs table indicating a mode of 14 of the liberal sample who attend a religious service occasionally.\nii)\nHere I use a linear model to determine if there is a correlation between Highschool GPA and Hours Watching TV\n\n\nhide\n\nggscatter(student.survey,x=\"tv\",y=\"hi\",\n          add = \"reg.line\",conf.int = TRUE,\n          xlab = \"Hours Watching TV\",ylab = \"HighSchool_GPA\",title = \"HighSchool_GPA vs Hours Watching TV\")\n\n\n\n\nI can now gain further insight into the variables high school gpa(hi) and hours watching tv(tv)\n\n\nhide\n\nskim(student_survey_data)      #provides concise,descriptive insight into the data\n\n\nTable 4: Data summary\nName\nstudent_survey_data\nNumber of rows\n60\nNumber of columns\n4\n_______________________\n\nColumn type frequency:\n\nfactor\n2\nnumeric\n2\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\npi\n0\n1\nTRUE\n7\nlib: 24, mod: 10, ver: 8, sli: 6\nre\n0\n1\nTRUE\n4\nocc: 29, nev: 15, eve: 9, mos: 7\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nhi\n0\n1\n3.31\n0.46\n2\n3\n3.35\n3.62\n4\n▂▁▇▇▆\ntv\n0\n1\n7.27\n6.72\n0\n3\n6.00\n10.00\n37\n▇▃▁▁▁\n\nhide\n\nstr(student_survey_data)\n\n\n'data.frame':   60 obs. of  4 variables:\n $ pi: Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re: Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ hi: num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ tv: num  3 15 0 5 6 4 5 5 7 1 ...\n\nhide\n\nsummary(student_survey_data)\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nInterpretation:\nStudents Highschool GPA has a mean of 3.31gpa and median of 3.35gpa. The GPA’s are within a range of 2.00 for minimum and 4.00 for maximum. It has a standard deviation of 0.46 which indicates the data are clustered around the mean. This is verified in the graph.\nStudents Hours Watching TV has a mean of 7.3 hours, median of 6 hours. The hours watched range from a minimum of 0 to a maximum value of 37 hours per week.\nPART C\nSummarize and interpret results of inferential analyses.\ni)\nTo gain better insight into the political ideology and religiosity variables, I use the function cor.test() (as discussed in class).The cor.test() function will provide an association or correlation between paired samples political ideology(pi) and religiosity variables(re).\n\n\nhide\n\n# correlation test\ncor.test(as.numeric(student_survey_data$pi),as.numeric(student_survey_data$re))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  as.numeric(student_survey_data$pi) and as.numeric(student_survey_data$re)\nt = 5.4163, df = 58, p-value = 1.221e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3818345 0.7265650\nsample estimates:\n      cor \n0.5795661 \n\nInterpretation:\nTest_Statistic = 5.4163 The p-value = 1.221e-06 is less p-value is <0.05 we see the correlation between  political ideology and religiosity variables** is statistically significant since the p-value is < 0.05 we therefore reject the null hypothesis.\nTo gain better insight into the “HighSchoolGPA vs Hours Watching TV” I use a cor.test as discussed in class\n\n\nhide\n\ncor.test(as.numeric(student_survey_data$hi),as.numeric(student_survey_data$tv))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  as.numeric(student_survey_data$hi) and as.numeric(student_survey_data$tv)\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nii)\nPearson correlation plot “HighSchoolGPA vs Hours Watching TV”\n\n\nhide\n\nggscatter(student.survey,x=\"tv\",y=\"hi\",\n          add = \"reg.line\",conf.int = TRUE,\n          cor.coef=TRUE,cor.method = \"pearson\",\n          xlab = \"Hours Watching TV\",ylab = \"HighSchool_GPA\",title = \"HighSchool_GPA vs Hours Watching TV\")\n\n\n\n\nInterpretation:\nTest statistic = -2.1144\nBased on the cor.test we see the correlation between gpa and television watching is statistically significant since the p-value is < 0.05 at p-value: 0.039, we therefore reject the null hypothesis. Graphically we also observe there is a moderately weak negative correlation between gpa and television watching.\nQuestion 6\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\nSolution\nRegression toward the mean is an idea that refers to the fact that if one sample of a random variable is extreme, the next sampling of the same random variable is likely to be closer to its mean.\nIn this case, the original sample mean of the specially tutored students was 50 which is very low compared to the overall class sample mean of 70; therefore the next sampling of the same set of the specially tutored students is likely to produce a value of a sample mean closer to the overall mean of 70; and this is purely due to the statistical phenomenon of regression towards mean and not due to any special effect of the special tutoring program ; even without the special tutoring program, this second sample mean would most likely be closer to 70 than the first sample mean.\n\n\n\n",
    "preview": "posts/httpsrpubscomchester876831/distill-preview.png",
    "last_modified": "2022-03-12T19:37:04-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomcdaviau872505/",
    "title": "HW #1",
    "description": "First homework assignment for DACSS 603",
    "author": [
      {
        "name": "Chris Daviau",
        "url": {}
      }
    ],
    "date": "2022-03-06",
    "categories": [],
    "contents": "\nProblem Set 1\n1.\nQuestion:\nThe time between the date a patient was recommended for heart surgery\nand the surgery date for cardiac patients in Ontario was collected by\nthe Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health\nand Long-Term Care, Ontario, Canada, 2006). The sample mean and sample\nstandard deviation for wait times (in days) of patients for two cardiac\nprocedures are given in the accompanying table. Assume that the sample\nis representative of the Ontario population. Construct the 90%\nconfidence interval to estimate the actual mean wait time for each of\nthe two procedures. Is the confidence interval narrower for angiography\nor bypass surgery?\n\n[1] 18.21042 19.78958\n[1] 17.42495 18.57505\n[1] \"The confidence interval for angiography is narrower compared to bypass surgeries\"\n\n2.\nQuestion:\nA survey of 1031 adult Americans was carried out by the National Center\nfor Public Policy. Assume that the sample is representative of adult\nAmericans. Among those surveyed, 567 believed that college education is\nessential for success. Find the point estimate, p, of the proportion of\nall adult Americans who believe that a college education is essential\nfor success. Construct and interpret a 95% confidence interval for\np.\n\n\np <- 567/1031\nn <- 1031\nx <- 567\n\n# We are 95% certain that p falls between 0.519 and 0.580\nprop.test(x,n,p)\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  x out of n, null probability p\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n3.\nQuestion:\nSuppose that the financial aid office of UMass Amherst seeks to estimate\nthe mean cost of textbooks per quarter for students. The estimate will\nbe useful if it is within 5 dollars of the true population mean\n(i.e. they want the confidence interval to have a length of 10 dollars\nor less). The financial aid office is pretty sure that the amount spent\non books varies widely, with most values between 30 and 200 dollars.\nThey think that the population standard deviation is about a quarter of\nthis range. Assuming the significance level to be 5%, what should be the\nsize of the sample?\n\n\n# Std. dev. \n  text_sd <- (200-30)/4\n\n# Z-score for .05 siginificance level\n  z <- 1.96\n  \n# Referring to the confidence interval formula, it can be broken down into point estimate +/- margin of error\n# In this case, UMass specifically wants our CI length to be 10 dollars or less, so accounting for both tails:\n  e <- 10/2\n\n# Since z(se) = e, and se = s/sqrt(n), and we know all other variables except n,\n# We can create a formula to solve for n:\n  n <- (text_sd^2 * z^2)/e^2\n\n# The size of the sample should be 278\nprint(n)\n\n\n[1] 277.5556\n\n4.\nQuestion:\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union\nagreement, the mean income for all senior-level workers in a large\nservice company equals 500 dollars per week. A representative of a\nwomen’s group decides to analyze whether the mean income μ for female\nemployees matches this norm. For a random sample of nine female\nemployees, ȳ = $410 and s = 90.\n4a.\nTest whether the mean income of female employees differs from $500 per\nweek. Include assumptions, hypotheses, test statistic, and P-value.\nInterpret the result.\n\n\n# T Statistic\n(410-500)/(90/sqrt(9))\n\n\n[1] -3\n\n# Determine p-value, multiply by 2 to account for both tails\n# The p-value obtained indicates the mean is significantly different from $500\npt(-3, 8)*2\n\n\n[1] 0.01707168\n\n4b. Report the\nP-value for Ha : μ < 500. Interpret.\n\n\n# This time we only account for one side since it's \"less than 500\"\n# The p-value indicates the mean is significantly less than $500\npt(-3, 8)\n\n\n[1] 0.008535841\n\n4c.\nReport and interpret the P-value for H a: μ > 500. (Hint: The\nP-values for the two possible one-sided tests must sum to 1.)\n\n\n# We only want the right side since it's \"greater than 500\", which is why lower.tail=FALSE\n# The p-value indicates that the mean is not significantly greater than $500\npt(-3, 8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n5.\nQuestion:\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith\nseparately conduct studies to test H0: μ = 500 against Ha : μ = 500,\neach with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ =\n519.7,with se = 10.0.\n5a.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and\nP-value = 0.049 for Smith.\n\n\nJ_t_score <- (519.5-500)/10\n\nJ_p_value <- pt(J_t_score, 999, lower.tail = FALSE)*2\n\nS_t_score <- (519.7-500)/10\n\nS_p_value <- pt(S_t_score, 999, lower.tail = FALSE)*2\n\n# Jones\nprint(J_t_score)\n\n\n[1] 1.95\n\nprint(J_p_value)\n\n\n[1] 0.05145555\n\n# Smith\nprint(S_t_score)\n\n\n[1] 1.97\n\nprint(S_p_value)\n\n\n[1] 0.04911426\n\n5b.\nUsing α = 0.05, for each study indicate whether the result is\n“statistically significant.”\nSmith’s results are statistically significant since the p-value is\nless than .05, but Jones’ results are not since the p-value is greater\nthan .05.\n5c.\nUsing this example, explain the misleading aspects of reporting the\nresult of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0”\nversus “Do not reject H0 ,” without reporting the actual P-value.\nIt’s misleading to report a p-value is significant without providing\nthe actual value because, as shown above, the p-value could be\nsignificant, but only by a small margin and doesn’t tell the whole\nstory\n6.\nQuestion:\nAre the taxes on gasoline very high in the United States? According to\nthe American Petroleum Institute, the per gallon federal tax that was\nlevied on gasoline was 18.4 cents per gallon. However, state and local\ntaxes vary over the same period. The sample data of gasoline taxes for\n18 large cities is given below in the variable called gas_taxes. Is\nthere enough evidence to conclude at a 95% confidence level that the\naverage tax per gallon of gas in the US in 2005 was less than 45 cents?\nExplain.\n\n\n\n\n\n# There is not enough evidence to conclude that the average tax per gallon of gas was less than 45 cents\n# The conf. interval contains the value 45, meaning the average very well could be 45\nt.test(gas_taxes, conf.level = .95)$conf.int\n\n\n[1] 36.23386 45.49169\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-06T19:21:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomkpopiela873483/",
    "title": "Homework 1",
    "description": "DACSS-603",
    "author": [
      {
        "name": "Katie Popiela",
        "url": {}
      }
    ],
    "date": "2022-03-06",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nTo calculate the confidence intervals of bypass and angiography procedures we must use the following formula: x +/- z(s/sqrt(n)). X represents the sample mean, z represents the z-value, s represents the sample standard deviation, and n represents sample size.\r\nBypass:\r\nIn this part of the problem, variables x, z, s, and n hold the following values:\r\nx = 19,\r\nz = 1.645 (for 90% confidence interval),\r\ns = 10,\r\nn = 539.\r\n\r\n\r\n19+1.645*(10/(sqrt(539)))\r\n\r\n\r\n[1] 19.70855\r\n\r\n\r\n\r\n19-1.645*(10/(sqrt(539))) \r\n\r\n\r\n[1] 18.29145\r\n\r\nConfidence Interval = 18.29145, 19,70855\r\nAngiography\r\nIn this part of the problem, variables x, z, s, and n hold the following values:\r\nx = 18,\r\nz = 1.645,\r\ns = 9,\r\nn = 847\r\n\r\n\r\n18+1.645*(9/(sqrt(847)))\r\n\r\n\r\n[1] 18.50871\r\n\r\n\r\n\r\n18-1.645*(9/(sqrt(847)))\r\n\r\n\r\n[1] 17.49129\r\n\r\nConfidence Interval = 17.49129, 18.50871\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\nTo find the point estimate p, with a 95% confidence interval we must use the following formula:\r\np +/- z*(sqrt(p(1 p)/n))\r\nin which p represents the sample proportion, z represents the chosen z-value, and n represents sample size. The values these variables take are listed in the code chunk below.\r\n\r\n\r\np <-0.54995  \r\nz <-1.96  \r\nn <-1031  \r\np+1.96*(sqrt(p*(1-p)/n)) \r\n\r\n\r\n[1] 0.5803182\r\n\r\n\r\n\r\np-1.96*(sqrt(p*(1-p)/n))\r\n\r\n\r\n[1] 0.5195818\r\n\r\nConfidence Interval = 0.5195818, 0.5803182\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nWe must first start with the confidence interval formula:\r\nx +/- z*(s/sqrt(n)) = 5 in which\r\nx = sample mean, z = chosen z-value, s = sample standard deviation, and n = sample size. 5 accounts for the $5 range the cost estimate must be within.\r\nBut because we are looking for variable n (sample size), we have to reorganize the equation;\r\nz*(s/5)^2 = n\r\n\r\n\r\nf <-function(n, z = 1.96, s = 42.5) {\r\n  res <- z*s/sqrt(n)\r\n  return(res)\r\n}\r\n\r\nvec <- vapply(1:300, FUN = f, FUN.VALUE = 5.0)\r\nwhich(vec < 5) [1]\r\n\r\n\r\n[1] 278\r\n\r\nOnce we transition the formula into code, we must then create a vector in order to see the lowest value the sample size can be to achieve a mean textbook cost within $5 of the true population mean.\r\nThe result indicates that the sample must contain at least 278 people to achieve an estimate within $5 of the true population mean.\r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\na. Test whether the mean income of female employees differs from $500/week. Include assumptions, hypotheses, test statistic, and p-value. Interpret the result.b. Report the p-value for \\(H_{a}\\) : μ < 500. Interpret.c. Report and interpret the P-value for \\(H_{a}\\) : μ > 500.(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nIn order to test whether or not the mean income for female employees differs from $500/week, we must first condect a one-sample, two-sided significance test.\r\nWe can also assume the following:\r\n1. The sample is random and the population has a normal distribution\r\n2. The mean income for all senior-level workers = $500/week\r\n3. From the random sample of 9 female employees, the mean income = $410/week\r\n4. Standard deviation = 90\r\n5. Null Hypothesis: \\(H_{0}\\): μ = 500\r\n6. Alternative Hypothesis: \\(H_{a}\\): μ ≠ 500\r\n\r\n\r\n(410 - 500)/(90/sqrt(9))\r\n\r\n\r\n[1] -3\r\n\r\nThe test statistic/t-test value is -3.\r\nNow onto the P-value\r\n\r\n\r\nrandom_sample_n <- 9  \r\ndf_n <- (random_sample_n - 1)  \r\nt_test <- (410 - 500)/(90/sqrt(9))  \r\np_value <- pt(t_test, df_n)*2  \r\nprint(p_value)\r\n\r\n\r\n[1] 0.01707168\r\n\r\nInterpretation: We know that the p-value is 0.01707. Assuming α = 0.05, we can see that 0.01707 < 0.05, meaning we can reject the null hypothesis. As a result, we have sufficient evidence to assert that the mean income for female employees differs from the general mean of $500/week.\r\nThe next part of the question asks us to report the p-value for Ha: μ < 500, and then interpret.\r\nHypotheses\\(H_{0}\\) : μ = $500/week\\(H_{a}\\) : μ = <$500/week\r\nP-Value = p(t < t_test)p(t<-3)\r\nP-value for \\(H_{a}\\): μ > 500 (left tail test) using the formula pt(q,df,lower.tail=TRUE,log.p=FALSE)\r\n\r\n\r\nq <- -3\r\nrandom_sample_n <- 9\r\ndf_n <- (random_sample_n-1)\r\nleft_p_value <- pt(q,df_n,lower.tail=TRUE,log.p=FALSE)\r\nprint(left_p_value)\r\n\r\n\r\n[1] 0.008535841\r\n\r\nP= 0.0085, which can be rounded to 0.01. This indicates that there is strong evidence against the mean weekly income being $500 or more.\r\nNext we must calculate the P-value for \\(H_{0}\\) : μ < 500 (right tail test)\r\n\r\n\r\nq <- -3\r\nrandom_sample_n <- 9\r\ndf_n <- (random_sample_n-1)\r\nright_p_value <- pt(q,df_n,lower.tail = FALSE,log.p=FALSE)\r\nprint(right_p_value)\r\n\r\n\r\n[1] 0.9914642\r\n\r\nThe P-value for \\(H_{0}\\) : μ < 500 is 0.99. This indicates strong evidence in favor of the null hypothesis, going against the claim that mean μ > 500. To ensure these findings are in fact correct, we have to confirm that the sum of left_p_value and right_p_value = 1.\r\n\r\n\r\nleft_p_value <- 0.01\r\nright_p_value <- 0.99\r\ntotal_sum_lr <- left_p_value + right_p_value\r\nprint(total_sum_lr)\r\n\r\n\r\n[1] 1\r\n\r\nAs is shown above, the sum of the left and right tails is 1.\r\nQuestion 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7,with se = 10.0.\r\na. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nJones\r\nWe will start with a t-test, using the formula: t = (\\(\\overline{y}\\) - μ)/10.0\r\n\\(\\overline{y}\\) = 519.5 μ = 500 se = 10.0\r\n\r\n\r\nt_test <- (519.5-500)/10.0\r\nprint((519.5-500)/10.0)\r\n\r\n\r\n[1] 1.95\r\n\r\nWe have thus shown that for Jones, t = 1.95. Now, onto the p-value.\r\n\r\n\r\nn <- 1000\r\ndf_j <- (n - 1)  \r\nt_test <- (519.5-500)/10.0  \r\np_value <- pt(t_test, df_j,lower.tail = FALSE,log.p = FALSE)*2  \r\nprint(p_value)\r\n\r\n\r\n[1] 0.05145555\r\n\r\nThe presented p-value of 0.051 is in fact correct, as the math shows. The next step is doing the same calculations for Smith, in which t = 1.97 and P-value = 0.049\r\nSmith\r\nT-test:\r\n\\(\\overline{y}\\) = 519.7 μ = 500 se = 10.0\r\n\r\n\r\nt_test <- (519.7 - 500)/10.0\r\nprint(t_test)\r\n\r\n\r\n[1] 1.97\r\n\r\nOnce again the presented value for t has been confirmed as correct (t=1.97). Now that we have this information, we can calculate and hopefully confirm the p-value as well.\r\nP-value:\r\n\r\n\r\nn <- 1000\r\ndf_s <- (n - 1)  \r\nt_test <- (519.7-500)/10.0  \r\np_value <- pt(t_test, df_s,lower.tail = FALSE,log.p = FALSE)*2  \r\nprint(p_value)\r\n\r\n\r\n[1] 0.04911426\r\n\r\nSmith’s p-value is correct at 0.049.\r\nb. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\r\njones_p_value = 0.051\r\nsmith_p_value = 0.049\r\n\\(\\alpha\\) = 0.05\r\nIn order for a p-value to be statistically significant, it must be greater than 0.05. Smith’s p-value is 0.049 which, while close, is still less than 0.05. Jones’s p-value, however, is statistically significant at 0.051.\r\nc. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nBoth studies yielded extremely similar results, but the difference is great enough that only Jones’s work was statistically significant.However, given the closeness in result values between the two, we can see that both have moderate evidence against \\(H_{0}\\).\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAssumptions:\r\ngas_taxes_sample <- 18 df_gt <- gas_taxes_sample-1 95% confidence interval Significance level: \\(\\alpha\\) = 0.05 (based on confidence interval)\r\nTo start, we must calculate the t-score to find the upper and lower intervals of gas_taxes_sample\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\ngas_taxes_sample <- 18\r\ndf_gt <- gas_taxes_sample - 1\r\nmean_gt <- mean(gas_taxes)\r\ntscore_gt <- qt(p=0.05,df=df_gt,lower.tail=FALSE)\r\ngas_sd <- sd(gas_taxes)\r\nme_gas_taxes <- qt(0.05,df = df_gt)*gas_sd/sqrt(18)\r\n\r\nlower_int_gt<-(mean_gt-me_gas_taxes)\r\nprint(lower_int_gt)\r\n\r\n\r\n[1] 44.67946\r\n\r\nThe lower bound of gas_taxes = 44.67946\r\nNow to find the upper bound:\r\n\r\n\r\nupper_int_gt <- (mean_gt + me_gas_taxes)\r\nprint(upper_int_gt)\r\n\r\n\r\n[1] 37.0461\r\n\r\nThe upper bound is 37.0461.Therefore, the confidence interval (at 95%) is [37.0461, 44.6794].\r\nThe average tax/gallon of gas is less than $0.45, so it is within the upper and lower bounds of the confidence interval. However, we will test an alternate outcome via a t-test\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nmean(gas_taxes)\r\n\r\n\r\n[1] 40.86278\r\n\r\n\r\n\r\nt.test(gas_taxes,conf.level=0.95)\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 9.555e-13\r\nalternative hypothesis: true mean is not equal to 0\r\n95 percent confidence interval:\r\n 36.23386 45.49169\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nIn this scenario, the confidence interval is [36.2338, 45.4916]. Therefore there isn’t enough evidence to conclude that at a 95% confidence level the average tax per gallon of gas was less than $0.45 in the US in 2005 since $0.45 is within the confidence interval (which contains tax rates greater than $0.45).\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-06T19:21:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomofyalcinhw1-sol/",
    "title": "Homework 1 - Solutions",
    "description": "Solutions for homework 1.",
    "author": [
      {
        "name": "Omer Yalcin",
        "url": {}
      }
    ],
    "date": "2022-03-06",
    "categories": [],
    "contents": "\nPlease check your answers against the solutions.\nQuestion 1\n\n\nbypass_n = 539\nangio_n = 847\n\nbypass_sample_mean = 19\nangio_sample_mean = 18\n\nbypass_sample_sd = 10\nangio_sample_sd = 9\n\nbypass_se = bypass_sample_sd/sqrt(bypass_n)\nangio_se = angio_sample_sd/sqrt(angio_n)\n\nbypass_me = qt(0.95, df = bypass_n - 1)*bypass_se\nangio_me = qt(0.95, df = angio_n - 1)*angio_se\n\n\n\nThe confidence intervals:\n\n\nprint(bypass_sample_mean + c(-bypass_me, bypass_me))\n\n\n[1] 18.29029 19.70971\n\nprint(angio_sample_mean + c(-angio_me, angio_me))\n\n\n[1] 17.49078 18.50922\n\nThe size of the confidence intervals, which is twice the margin of error:\n\n\n2 * bypass_me\n\n\n[1] 1.419421\n\n2 * angio_me\n\n\n[1] 1.018436\n\nThe confidence interval for angiography is narrower.\nQuestion 2\none-step solution:\n\n\nn = 1031\nk = 567\nprop.test(k, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  k out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nAlternatively:\n\n\np_hat <- k/n # point estimate\nse = sqrt((p_hat*(1-p_hat))/n) # standard error\ne = qnorm(0.975)*se # margin of error\np_hat + c(-e, e) # confidence interval \n\n\n[1] 0.5195839 0.5803191\n\nAlternatively, we can use the exact binomial test. In large samples like the one we have, the results should essentially be the same as prop.test().\n\n\nbinom.test(k, n)\n\n\n\n    Exact binomial test\n\ndata:  k and n\nnumber of successes = 567, number of trials = 1031, p-value =\n0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515 \n\nQuestion 3\n\n\nrange = 200-30\npopulation_sd = range/4\n\n\n\nRemember:\n\\[CI_{95} = \\bar x \\pm z \\frac{s}{\\sqrt n}\\] (We can use \\(z\\) because we assume population standard deviation is known.)\nWe want the number \\(n\\) that ensures:\n\\[ z \\frac{s}{\\sqrt n} = 5 \\] \\[ zs = 5 \\sqrt n\\] \\[ \\frac{zs}{5} = \\sqrt n\\] \\[  (\\frac{zs}{5})^2 = n\\]\nIn our case:\n\n\nz = qnorm(.975)\ns = population_sd\nn = ((z *s) / 5)^2\nprint(n)\n\n\n[1] 277.5454\n\nRounding up, we need a sample of 278.\nQuestion 4\nWe can write a function to find the t-statistic, and then do all the tests in a, b, and c using that.\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nwhere \\(\\bar x\\) is them sample mean, \\(\\mu\\) is the hypothesizes population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nWriting this in R:\n\n\nget_t_stat <- function(x_bar, mu, sd, n){\n  return((x_bar - mu) / (sd / sqrt(n)))\n}\n\n\n\nFind the t-statistic:\n\n\nt_stat <- get_t_stat(x_bar = 410, mu = 500, sd = 90, n = 9)\n\n\n\nA\nTwo-tailed test\n\n\nn = 9\npval_two_tail = 2*pt(t_stat, df = n-1)\npval_two_tail\n\n\n[1] 0.01707168\n\nWe can reject the hypothesis that population mean is 500.\nB\n\n\npval_lower_tail = pt(t_stat, df = n-1)\npval_lower_tail\n\n\n[1] 0.008535841\n\nWe can reject the hypothesis that population mean is greater than 500.\nC\n\n\npval_upper_tail = pt(t_stat, df = n-1, lower.tail=FALSE)\npval_upper_tail\n\n\n[1] 0.9914642\n\nWe fail to reject the hypothesis that population mean is less than 500.\nAlternatively for C, we could just subtract the answer in B from 1:\n\n\n1 - pval_lower_tail\n\n\n[1] 0.9914642\n\nQuestion 5\n\n\nt_jones = ((519.5 - 500)/ 10)\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\n\nt value for Jones: 1.95 \n\ncat(\"t value for Smith:\", t_smith, '\\n')\n\n\nt value for Smith: 1.97 \n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Jones: 0.0515 \n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Smith: 0.0491 \n\nAt 0.05 level Smith’s result is statistically significant but Jones’s is not. The result show the arbitrariness of the 0.05 demarcation line and the importance of reporting actual p-values to better make sense of results.\nQuestion 6:\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\nIn the one sided test, we are able to reject the null in favor of the alternative that the gas taxes are less than 45 cents.\nNote that a two-sided test at the same level would not have resulted in the rejection of the null.\nHowever, a two-sided 90% confidence interval gives the same upper bound, since now there is a 5% rejection are on two sides:\n\n\nt.test(gas_taxes, mu = 45, alternative = 'two.sided', conf.level = 0.9)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.07654\nalternative hypothesis: true mean is not equal to 45\n90 percent confidence interval:\n 37.04610 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-06T19:21:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/603-homework-1/",
    "title": "603, Homework 1",
    "description": "First homework assignment for DACSS 603",
    "author": [
      {
        "name": "Joe Davis",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nI wanted to do this one a bit longer-form than necessary, but I also appreciate the chance to practice organizing my R code neatly while solving. I need some good repetition to build those good habits.\nFor this question I assigned all of the values in the table to objects to use in the calculations of each treatment’s t-scores, margins of error, and confidence intervals. I did a quick check of the qnorm(.95) to see if the t-scores had converged to the normal distribution given the relatively high sample sizes of each treatment. Surprisingly, to me at least, rounding at the .000 level they still had very slightly different levels both to the normal distribution and to each others’ scores.\nI know I have the standard deviations, wait time would be a continuous variable, and these samples are larger than n = 30 so I can use qnorm and the 1.645 z critical value clearing those assumptions. But, it seems like I should use the distribution with fatter tails relative to the sample size values from qt regardless if I could assume normality from the other conditions, as to err on the side of caution for analyzing and communicating medical procedure data. I have a note in my code on that point, mostly for my own reference and will solve for the normal distribution values as well. After running the calculations the practical effect would be introducing slightly more uncertainty in the mean wait time for the angiography procedure while still keeping that procedures’ interval narrower, as we’d expect, than that of the bypass procedure. Especially if this was to be used to set patient expectations on wait times, the wider range would be the option a hospital or doctor would communicate, but we do appear to be in the range where t and z distributions are becoming quite similar.\n\n\n## Means, Total N,  and SDs from full question text. B = Bypass A = Angiography\n  #mean wait times for each procedure\n  mean_b <- 19 \n  mean_a <- 18 \n  \n  #standard deviations\n  sd_b <- 10\n  sd_a <- 9\n  \n  #total N\n  n_b <- 539\n  n_a <- 847\n\n## 90% CI score finding. .90 = 1 - a and I need  a/2 for the two tails since I have no theory on why the direction of the error would be important. Did a little side exploring of the t distribution and normal distribution scores below.\n  \n\n#Rounded to 1.645 but not used for calculations. I thought it was a sufficiently large sample size that that qt and qnorm should have returned the same values at .000 rounding, but after checking those, each T rounded up differently at the 3rd digits and for medical data I would rather err on the side of caution  \n\n    #normal z score\n  z_heart <- qnorm(.95) \n  round(z_heart, digits = 3)\n\n\n[1] 1.645\n\n  #check this for t scores\nb_t <- round(qt(.95, df = n_b -1), digits = 3)\na_t <-  round(qt(.95, df = n_a -1), digits = 3)\n  \n  #print them\n  b_t\n\n\n[1] 1.648\n\n  a_t\n\n\n[1] 1.647\n\n## 90% CI standard error of mean/ margin of error\n  #Take the score multiplied by standard deviations and sqrt of Ns, t value\n  mofe_bypass <- b_t*(sd_b/sqrt(n_b))\n  mofe_angiography <- a_t*(sd_a/sqrt(n_b))\n  \n  #doing this with z 1.645\n  mofe_bypass_z <- 1.645*(sd_b/sqrt(n_b))\n  mofe_angiography_z <- 1.645*(sd_a/sqrt(n_a))\n  \n  #print the normal distribution score MoE\n  mofe_bypass_z\n\n\n[1] 0.7085517\n\n  mofe_angiography_z\n\n\n[1] 0.5087058\n\n  # Print them using t value\n  mofe_bypass\n\n\n[1] 0.7098439\n\n  mofe_angiography\n\n\n[1] 0.6384718\n\n## +/- from the data set mean for the range\n  \n  #bypass upper and lower\n  bypass_lower <- mean_b - mofe_bypass\n  bypass_upper <- mean_b + mofe_bypass\n  \n  #combine the upper and lower to list interval\n  ci_bypass <- print(c(bypass_lower, bypass_upper))\n\n\n[1] 18.29016 19.70984\n\n  #angiography upper and lower\n  angio_lower <- mean_a - mofe_angiography\n  angio_upper <- mean_a + mofe_angiography\n  \n\n  \n  #combine the upper and lower to list interval\n  ci_angiography <- print(c(angio_lower, angio_upper))\n\n\n[1] 17.36153 18.63847\n\n#Normal distribution 90% CI\n  #bypass Z\n  bypass_lower_z <- mean_b - mofe_bypass_z\n  bypass_upper_z <- mean_b + mofe_bypass_z\n  \n  ci_bypass_z <- (print(c(bypass_lower_z, bypass_upper_z)))\n\n\n[1] 18.29145 19.70855\n\n  #angiography Z\n  angiography_lower_z <- mean_a - mofe_angiography_z\n  angiography_upper_z <- mean_a + mofe_angiography_z\n  \n  ci_angiography_z <- (print(c(angiography_lower_z, angiography_upper_z)))\n\n\n[1] 17.49129 18.50871\n\nThe confidence interval is narrower for angiography, as we have a larger sample size for that procedure’s wait time and a smaller standard deviation. We would expect this as in theory as the larger sample size mean should be closer to the true population mean, and the smaller standard deviation means we have less variation to begin with. That expectation is shown in both the smaller numerator and the larger denominator produced during the margin or error calculation. For comparison on the denominators, since the standard deviations were already listed in the table: 23.2163735 for bypass and 29.1032644 for angiography. For completeness, the normal distribution CI’s are 17.4912942, 18.5087058 for angiography and 18.2914483, 19.7085517 for the bypass procedure.\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nSince we are looking to construct this interval around a proportion, I used the prop.test function to construct the interval using the total sample size as the n input and the 567 raw respondents for college “being essential for success” as the success vector in the function. I decided to use this function versus hand calculating as in question 1.\n\n\nprop_coll_success <- prop.test(567, 1031, conf.level = .95)\nprop_coll_success\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nnames(prop_coll_success)\n\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"   \n[5] \"null.value\"  \"conf.int\"    \"alternative\" \"method\"     \n[9] \"data.name\"  \n\nThe point estimate from the survey data is 0.55 The 95% confidence The interval is 0.52, 0.58. This interval means that we could say we are 95% confident that the proportion of American adults who believe that “college education is essential for success” is somewhere between the lower and upper end of our confidence interval – assuming the initial survey was indeed random and representative. Representativeness (and randomness, but that’s already extremely difficult with surveying) would be especially important for this question depending on what variables were used to determine that the initial sample was representative, as beliefs around college education are increasingly subject to the impacts political polarization and thus we could be breaking some of our assumptions needed for our analysis to be accurate depending on which variables were used.\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nFor this problem, I started off assigning all of the elements of the problem to objects as I did for the prior two. The standard deviation problem is shown in sd_books below, as 1/4 of the difference between $200 and $30. The critical z for 5% significance level is shown in finding the qnorm result for half of the alpha level. The margin of error we need to aim for is 5 dollars and that is assigned to book_moe. I assumed it would take a decent sized sample, at least larger than 30, to get within 5 dollars with that large of a standard deviation and the we would be able to randomly collect this sample, so I used the large sample size for estimating mean equation to find the sample size n. It took too long to figure out the fancy letters in Rmarkdown, so I used abbreviations for standard deviation and such, unfortunately. \\[n =  sd ^ {2} (z /  M) ^ {2}\\]\n\n\n#Assign all of the elements of the problem to objects\nsd_books <- (200-30)*.25\nsd_books\n\n\n[1] 42.5\n\nbook_z <- qnorm(.975) \nbook_z\n\n\n[1] 1.959964\n\nbook_moe <- 5\n\n# Formula for n from margin of error calculation, large sample is n = sd^2 * z a/2 / M. Calculate by hand first.\nbook_n_by_hand <- sd_books ^ 2 * (book_z / book_moe) ^ 2\n\nbook_n_by_hand\n\n\n[1] 277.5454\n\n#Use the samplingbook package to confirm.\nbook_n_package <- sample.size.mean(e = book_moe, S = sd_books, level = .95)\n\nbook_n_package\n\n\n\nsample.size.mean object: Sample size for mean estimate\nWithout finite population correction: N=Inf, precision e=5 and standard deviation S=42.5\n\nSample size needed: 278\n\nAfter finding the answer of 278 –rounded up– students in the sample by hand calculating n, I wanted to check my work using the samplingbook package and putting the same elements from the problem into the function. That answer, shown above in book_n_package matched my “hand” calculation of 278 students in the sample needed to have a mean estimate within 5 dollars of the true population mean of textbook costs.\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s =90. \n-A)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n-B) Report the P-value for Ha : μ < 500. Interpret.\n-C) Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nSince this question has several follow up questions to report out, I’m going to dive right in to the code portion of the work, and I’ll have all of the narrative explanations and steps of work described there.\n\n\n#question elements assigned to objects, will use alpha = .05\n  \n  #population\n  all_union_mean <- 500\n  \n  #sample\n  women_mean <- 410\n  women_sd <- 90\n  women_n <- 9\n\n#get the test statistic t\n\n  #estimated standard error\n  women_se <- women_sd / sqrt(women_n)\n  women_se\n\n\n[1] 30\n\n  #test statistic\n  women_t <- (women_mean - all_union_mean) / women_se\n  women_t\n\n\n[1] -3\n\n#find two tail p-value, round to two digits\n  women_p <- round(2*pt(-abs(women_t), df = 8, lower.tail = FALSE ), digits = 2)\n  women_p\n\n\n[1] 1.98\n\n#p for < 500, round to two digits\n  women_lower_p <- round(pt(-abs(women_t), df = 8, lower.tail = TRUE), digits = 2)\n  women_lower_p\n\n\n[1] 0.01\n\n#p for > 500, round to two digits\n  women_greater_p <- round(pt(-abs(women_t), df = 8, lower.tail = FALSE), digits = 2)\n  women_greater_p\n\n\n[1] 0.99\n\nAssumptions: Because this is a small sample, I’m using the two-sided t-test as it is robust when the data may not clear the normality assumptions. Given the smaller sample size of the study, highly skewed data could impact one-tailed tests and the question didn’t explicitly state that they were looking higher or lower than the overall union average\nHypotheses: Null hypothesis is that the mean wage for women = 500 dollars, the same as the contract required mean for all senior workers at the union. The alternative hypothesis is that the mean wage for women =/= 500 dollars.\nTest statistic: The test statistic women_t has a value of -3. Since this is a negative value due to the sample mean being lower than the population mean, it’s important to remember that the absolute value should be used in calculating the p value.\nP value: The two-sided P value from women_t with 8 degrees of freedom equals 1.98.\nInterpreting the results: Since the two-tailed P value is lower than our pre-selected alpha level of .05 by some distance, we can reject the null hypothesis that the mean wage for women is equal to 500 dollars, and accept the alternative hypothesis that it is not equal to 500. The below data points make it seem as though it is very likely below the overall union mean, which lines up with the mean and sd data from their study. If only the very outer bounds of the deviation hits the overall mean, it seems like this all confirms that the female employee mean is < 500 dollars. I would suggest initiating the process of review, confirmation of the study, and the grievance process.\nQuestion B: The P value for the womens’ mean weekly wage being less than 500 is 0.01. This means that if our null hypothesis was true, we would have a 99% chance of getting a value below 500 for our sample mean. Being that the contract specifies an overall mean of 500 and it’s stated that this is a large company so we can assume normality through the Central Limit Theorem, we have a lot of evidence that the mean weekly wage for women is likely lower than what the contract specifies. Being that we have a small sample size it is possible for the one-tailed value to be off with highly skewed data, however. Given the rejection of the null and the one tail results here and below for greater than 500, I think the women’s group would have a strong case that their mean wage is not 500 and most likely lower than 500 dollars.\nQuestion C: The P value for the mean weekly wage for women being greater than 500 is 0.99. This means that if the null hypothesis was true, we would expect to get a mean weekly wage for women greater than the population mean wage 1% of the time. With the same small sample size caveat as above on one tailed results, this split in P values which would have us fail to reject and then reject the null hypothesis respectively, for above and below the population mean is quite extreme and would be another supporting point in the group filing a grievance.\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519. 7,with se = 10.0.\nA) Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. \nB) Using α = 0.05, for each study indicate whether the result is “statistically significant.” \nC)Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nSame as above, I’m going to assign out the objects and then go through the solution steps below the R code chunk. The answer for question A will be in the R code chunk, while I will answer the narrative aspects of the other questions in the text below.\n\n\n#Question Elements\n  #both studies with same n\n  study_n <- 1000\n\n  #null mean = 500\n  #alt mean =/= 500\n\n  #jones study elements\n  jones_mean <- 519.5\n  jones_se <- 10.0\n\n  #smith study elements\n  smith_mean <- 519.7\n  smith_se <- 10.0\n\n\n#Solve for Smith\n  \n  #t for Smith, use two digits and it must equal 1.97\n  smith_t <- round((smith_mean - 500) / smith_se,  digits = 2)\n  smith_t #It's 1.97 like it's supposed to be,  yay!\n\n\n[1] 1.97\n\n  #P for Smith, use three digits and two-tailed \n  smith_p <- round(2 * pt(-abs(smith_t), df = 999), digits = 3)\n  smith_p # It's .049 like it's supposed to be!\n\n\n[1] 0.049\n\n#Solve for Jones\n  \n  #t for Jones, use two digits and it must equal 1.95.\n  jones_t <- round((jones_mean - 500) / jones_se, digits = 2)\n  jones_t #It's 1.95 like it's suppost to be, yay!\n\n\n[1] 1.95\n\n  #P for Jones, use three digits and it must equal .051\n  jones_p <- round(2 * pt(-abs(jones_t), df = 999), digits = 3)\n  jones_p #It's .051 like it's supposed to be!\n\n\n[1] 0.051\n\nQuestion B and C: I thought it made sense to answer these questions in one response versus splitting them up by bullet point. Technically, both studies would be “statistically significant” at the .05 level as rounding .049 and .051 to two digits would take them both to less than or equal to .05. Rounding them without disclosing that would be no good. This is also quite misleading to attribute practical significance to a difference in P values of .002 to our arbitrarily set level of significance. Practically speaking, there is no real difference in the outcomes of these studies.\nWithout rounding, only Smith’s .049 would be below the alpha level and Jones’ .051 would be above. With these fine margins, reporting only whether the null was rejected or if we failed to reject it, or even just listing the “p less than or equal to .05 or p greater than .05” statement without the raw p values included would also ascribe practical significance in difference to these two studies even though they are very nearly identical. The differences could very well be random noise and chance, and reporting out the statement alone would make it harder to asses that.\nChoosing to publish Smith’s study just because it cleared the threshold and not Jones’ as well could make it harder to see the full picture of the parameter they studied or analyze the overall random variability in the experimental findings in a meta analysis setting.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period.\nThe sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAfter reading the question, I don’t think our initial sample was collected in a way that would allow us to be confident in a confidence interval constructed from it to look at the average price in the nation as a whole in 2005. If the federal tax is constant at 18.4, it looks like most of the variability from city to city in the sample data is driven by state and local tax levy decisions. The mean for the gas_tax data is 40.86, so over half of our mean amount comes from state and local variables that do not look to be accounted for in the sampling description as is. Most Americans in 2005 (and still today), did not live in very large cities, the amount depending of course on the exact definition of “large.”\nOur sample is only from 18 large cities in the country, and I think it’s reasonable to assume large cities have, or the very least could have, systemically different gas tax policies than other population densities. Some states could have caps or specific legislation that could impact the overall national average, and there’s not enough information about how the 18 large cities were selected or sampled to clear all of the assumptions, even if we didn’t think large cities varied from suburban, exurban or rural geographies in a way that would skew the small sample of data that we do have. Exploring bootstrapping or other approaches would also be impacted by this fact.\nNow, if I’m reading entirely too much into this set up and I should just show that I can evaluate if our mean is below a set level using a confidence interval, I’ll proceed to do that, too! To answer this question, I’ll use the psych package to get the descriptive statistics and look those over for fun and possible use to calculate by hand if the t.test result looks funny. Then I’ll use t.test(gas_tax) and look at the 95% CI range.\nIf the entire confidence interval is below 45 cents we would have enough evidence to say we think it’s 95% likely that the mean gas tax is below the 45 cent level, the equivalent of rejecting the null hypothesis and accepting the alternative hypothesis of gas taxes were likely lower than 45 cents. If the interval includes 45 and/or above that level we don’t have enough information, and would be doing the equivalent of failing to reject the null hypothesis.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n#Use describe from the psych package for overview of gas_tax data, could use the variables here to hand calculate the CI.\ngas_tax_summary <- describe(gas_taxes)\ngas_tax_summary\n\n\n   vars  n  mean   sd median trimmed  mad   min   max range  skew\nX1    1 18 40.86 9.31  41.47   41.41 9.72 18.49 54.41 35.92 -0.58\n   kurtosis   se\nX1    -0.32 2.19\n\n#Use t.test on gas_taxes to see the 95% CI\nt.test(gas_taxes)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 18.625, df = 17, p-value = 9.555e-13\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\nThe 95% interval does contain 45 cents, so we cannot say that we have enough evidence at this confidence level and interval to do the equivalent of rejecting the null hypothesis. Since the null value is at the very very top of the interval, and putting aside the other issues with the sample to begin with, it seems like the sort of result where we could say in practice it was likely lower than 45 cents. Since gas taxes and prices go to the third digit at the pump, our result would have a maximum 95% CI level “at the pump” reading of .455. It feels safe to describe in actual practice with that interval that we’re confident it was 45 cents or lower, even if we’d need a bit more data to say it in specific statistical terms.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-27T23:53:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httprpubscomemersonflemi870425/",
    "title": "DACSS 603 HW 1",
    "description": "The following document contains my first homework assignment.",
    "author": [
      {
        "name": "Emerson Fleming",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nAnswer:\nRemember that we are trying to create a confidence interval for the average weight time for each surgical procedure. We are trying to predict the mean in the population ideally. We find the upper and lower bonds in order to be able to find our confidence level between 2 bands if you will.\nFor here, we want to use qt() as it gives us the lower tail value. Remember that you are saying you want R to give you the area underneath of the left OR lower tail.\nt=-qt(0.05, 538)\nRemember, we start here as we are trying to find the area underneath 0.05 and the degrees of freedom would be n-1.\nUB=19+t*10/sqrt(539) 19.7091\nlB= 19-t*10/sqrt(539) 18.2902\nUltimately we get that we are 90% confident that true wait time for bypass patients is between 18.3 and 19.71 days. Remember that there is a chance we are wrong, we are only 90% confident. We are saying that we are 90% confident that you will be waiting between 18.3 and 19.71 days to get your procedure.\nThen we do the same thing for the other surgery\nt=qt(0.05, 846)\nUB=18+t*9/sqrt(847) 18.5092\nLB=18-t*9/sqrt(847) 17.49078\nNow we can say we are 90% confident that the true wait time for aniography patients is between 17.49 and 18.50 days.\nIn order to find which is narrower we simply do: 19.71-18.3 = 1.41 days\n18.51-17.5 = 1.01 days\nAngiography is a more narrow inverval.\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nAnswer\nRemember hear that p hat is the sample proportion. For here we want to say: phat= 567/1031=0.55 n=1031\nRemember that ultimately, we want to construct a confidence interval as this is much more accurate than a point estimate.\nLB < P < UB —->We want to follow this format essentially and plug what we know in\nUB=0.55=1.96+sqrt(0.550.45/1031) LB=0.55-1.96sqrt(0.55*0.45/1031)\n0.52 < P < 0.058\nTrue population proportion of those believing college education is essential for success is in between 0.52 and 0.58 with a 95% confidence level\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nAnswer\nThe important information here is that: Range= 170 Sigma= 170/4 = 42.5 Alpha=5 —->This means that the confidence level = 95% (1-(alpha))\nHere, we are trying to find the size of the sample but remember that we don’t actually know but so much about the data. At this point, we need qnorm() which will give us the z-score of both sides of a normal distribution. We get 0.025 using a z table as this is the number that corresponds to -1.96.\nqnorm(0.025) =-1.96 This tells us that -z = -1.96 and z = 1.96\n21.9642.5/(sqrrtn) = 10 —->We want to solve for n\nWhen we do, we get n=278 We can interpret this as 278 being the ideal sample size based on what we know\nQuestion 4\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha : μ < 500. Interpret. B. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nAnswer\nOur first step is to come up with a test statistic. Then you can take the test statistic and create a t graph where you -t and +t. You then compare your p-value with alpha. Or what you can do is calculate a critical p value or a z value (which corresponds to alpha). Then you would compare your test statistic with your critical t.\nIn this particular case, we will choose the former rather than the latter as we are able to use R to compute the p-value (which are difficult to find by hand).\nHo: M=500 Ha: M(can’t equal)500 n=9 s=90 y(bar)=410 t= y(bar)-M/(s/(root(n))= -3\ndf=8\npt(-3, 8) ^Quantile, remember here that 8 is our degrees of freedom (n-1)\npvalue <- (-3,8)*2 +This gives you area under the curve to the left of -3 pvalue= 0.017 +Here, the p-value is very small. This means that there is no way that you were unlucky and selected a bad sample. This means you can reject your null hypothesis (Ho) We are rejecting M=500 in favor of M(is not equal to)500\n(Alternative way) tc <- qt(0.025, 8) ^probability where degrees of freedom is 8 = -2.31 (critical value) t statistic = -3 Here, we can reject Ho, our t-statistic is in the critical region which means we can reject the null!\nB. Ho = M = 500 Ha = M < 500 P-value = 0.0085 Since this p-value is so small, we can reject the null hypothesis in favor of the alternative hypothesis.\nC. Ho = M < 500 Ha = M > 500 test statistic = -3 pt(-3,8) = 0.99 Here, we fail to reject the null hypothesis. This p-value is huge. This proves that M has to be lower that 500 because we are failing to reject the null hypothesis.\nQuestion 5\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ = 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nAnswer\nA. Jones, t = 519-500/10 = 1.95\nse= s/sqrt(n)\nHere, we are conducting a 2-sided test. We want to find the area to the right of 1.95 and the area to the left of -1.95. These two areas together will give us the t-value. We will try and find the area on the left first\ndf=999\np-value = pt(-1.95, 999)*2 = 0.051\nSmith\nt= 519.7-500/10 = 1.97\np-value = pt(-1.97, 999)*2 = 0.049\nB. For Jones, we fail to reject the null hypothesis as the p-value is above 0.051 which means the results are statistically insignificant. For Smith, we can reject the null hypothesis which means that the results are statistically significant.\nC. The p-values are so similar that the results are not desirable. It is not fair to Jones. The results are so close that the basically got the same results but Jones got the shorter end of the stick and only his results are statistically insignificant.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAnswer\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\nHo: M<45 Ha: M<(less than or equal to) 45\nmean(gas_taxes) sd(gas_taxes)\nt= 40.86-45/9.31/sqrt(18) = -1.89\ndf= pt(-1.89, 17) p-value = 0.038 ^^This is a one-sided test so no need to multiply it\nThe conclusion is that we can reject the null hypothesis which gives us strong evidence to claim that tax rate is less than 45% at 5% significance level.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomalexanderhong86870632/",
    "title": "DACSS 603 HW#1",
    "description": "First homework for DACSS 603.",
    "author": [
      {
        "name": "Alexander Hong",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\n##Question 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\r\nSurgical Procedure\r\nSample Size\r\nMean Wait Time\r\nStandard Deviation\r\nBypass\r\n539\r\n19\r\n10\r\nAngiography\r\n847\r\n18\r\n9\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\n\r\n\r\nbypass_l <- 19 - (1.65 * (10/(539^.5)))\r\nbypass_h <- 19 + (1.65 * (10/(539^.5)))\r\nbypass_diff <- bypass_h - bypass_l\r\n\r\nangio_l <- 18 - (1.65 * (9/(847^.5)))\r\nangio_h <- 18 + (1.65 * (9/(847^.5)))\r\nangio_diff <- angio_h - angio_l\r\n\r\n\r\n\r\nCI for Bypass \\(19 \\pm 1.65 ( 10/sqrt(539) )\\) | Difference = 1.4214106\r\nCI for Angiography \\(18 \\pm 1.65 ( 9/sqrt(847) )\\) | Difference = 1.0205041\r\nThe confidence interval is narrower for angiography surgery.\r\n##Question 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\n\r\n\r\np2 = 567 / 1031\r\nn2 = 1031\r\nz2 = 1.96\r\n\r\np2_l <- p2 - (z2*(p2*(1-p2))^.5) / n2\r\np2_h <- p2 + (z2*(p2*(1-p2))^.5) / n2\r\n\r\n\r\n\r\nCI = \\(.55 \\pm 1.96 * sqrt( .55 / 1 -.55 )\\)\r\n95% of confidence intervals calculated would contain If this survery is repeated as many times, it is expected that 95% of those confidence intervals will contain the proportion that almost 55% of adult Americans believe that college education is essential for success.\r\n##Question 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nz = 1.96\r\nSD = $170 * .25 = $42.5 (The quarter of the range is .25*(200-30) )\r\nMean = Assuming most of the book values of the mean is between $30 and $200, the mean can be derived from ( $200 + $30 / 2 ) = $115\r\n\r\n\r\n\r\nA sample size of 277 textbooks should be needed to estimate the mean cost of textbooks per quarter. Using this sample size, given the standard deviation is $42.5, and the mean price of textbooks is $115, our confidence interval will have a length of $10.01\r\n##Question 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\n\r\n\r\nbar = 410\r\ns = 90\r\nn = 9\r\nmu = 500\r\n\r\ntscore <- (bar - mu) / (s / 9^.5)\r\n\r\np_value_l <- pt(tscore, df = n - 1, lower.tail = TRUE)\r\ncat(\"P-value is:\", p_value_l)\r\n\r\n\r\nP-value is: 0.008535841\r\n\r\np_value_h <- pt(tscore, df = n - 1, lower.tail = FALSE)\r\ncat(\"P-value is:\", p_value_h)\r\n\r\n\r\nP-value is: 0.9914642\r\n\r\nHypothesis 1:\r\nHo: μ = 500\r\nHa: μ < 500\r\nTest Statistic: -3\r\n0.0085358\r\nWe reject the null hypothesis and conclude that the mean salaries of female senior employees are not statistically significantly less than the $500 / week of senior employees.\r\nHypothesis 2:\r\nHo: μ = 500\r\nHa: μ > 500\r\nTest Statistic: -3\r\n0.9914642\r\nWe fail to reject the null hypothesis and conclude that the mean salaries of female senior employees are not statistically significantly higher than the $500 / week of senior employees.\r\n##Question 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05”, or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\n\r\n\r\njones_t <- round(pt(q=1.95, df=999, lower.tail = FALSE) * 2, 3)\r\nsmith_t <- round(pt(q=1.97, df=999, lower.tail = FALSE) * 2, 3)\r\n\r\n\r\n\r\nT Statistic for Jones = (519.5 - 500) / 10 = 1.95, p-value = 0.051\r\nT Statistic for Smith = (519.7 - 500) / 10 = 1.97, p-value = 0.049\r\nUsing an α = 0.05\r\nThe Jones study is not considered to be statistically significant, given the p-value for the test statistic is .051, which is barely above the .05 threshold.\r\nThe Smith study considered to be statistically significant, given the p-value for the test statistic is .049, which is barely above the .05 threshold.\r\nFor this example, if a result was listed as “P ≤ 0.05”, the range of p-values for Smith’s study can range from .05 to almost 0, with the actual p-value being .049.. If a result was listed as “P > 0.05”, the range of p-values for Jones’ study can range from .05 to 1, while the actual p-value was .051. These ranges diminish the reality that these studies were barely statistically significant or not, which would lead to possibly diminishing the motivation to look further into these studies and the nuances of the study which led to producing said result.\r\n##Question 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\ngas_m <- mean(gas_taxes)\r\ngas_sd <- (var(gas_taxes))^.5\r\n\r\ngas_l <- gas_m - (1.96 * (gas_sd/length(gas_taxes)^.5))\r\ngas_g <- gas_m + (1.96 * (gas_sd/length(gas_taxes)^.5))\r\n\r\n\r\n\r\nMean = 40.8627778 Standard Deviation = 9.3083168\r\n\\(40.86 \\pm 1.96 (9.31/sqrt(18))\\) = (36.5625548, 45.1630008)\r\nAt the 95% confidence level, there is not enough evidence to conclude that the average tax per gallon of gas was less than $.45. The upper end of the confidence interval is just over $.45.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomchester870152/",
    "title": "Homework One",
    "description": "DACSS 603",
    "author": [
      {
        "name": "Cynthia Hester",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nQuestion 1\r\nSolution\r\nAnalysis\r\n\r\nQuestion 2\r\nSolution\r\n\r\nQuestion 3\r\nSolution\r\n\r\nQuestion 4\r\nSolution\r\n\r\nQuestion 5\r\nSolution\r\n\r\nQuestion 6\r\nSolution\r\n\r\n\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nSolution\r\nFirst we assign variable names to the sample summary statistics\r\n\r\n\r\nbypass_sample<-539 #bypass_sample\r\nangio_sample<-847  #angiography_sample\r\nbypass_mean<-19    #bypass_sample_wait_time_mean\r\nangio_mean<-18     #angiography_sample_wait_time_mean\r\nbypass_sd<-10      #bypass_standard_deviation\r\nangio_sd<-9        #angiography_standard_deviation\r\n\r\n\r\n\r\nWe then calculate the t-confidence interval or t-score of each sample. To do this we start by calculating the degrees of freedom of each sample (bypass and angiography)\r\nCalculating degrees of freedom of the bypass and angiography samples\r\n\r\n\r\nbypass_df<-bypass_sample - 1  #bypass degrees of freedom\r\nangio_df<-angio_sample - 1    #angiography degrees of freedom\r\n\r\n\r\n\r\nNow that we have degrees of freedom we can calculate the t-critical values or t-score for the respective 90% intervals of each sample.\r\n\r\n\r\n#Calculating the t-critical value for the **angiography** sample\r\n\r\nangio_sample<-847\r\nangio_df<-angio_sample - 1\r\nt_score_angio<-qt(p=0.05, df=angio_df,lower.tail=F)\r\nprint(t_score_angio)\r\n\r\n\r\n[1] 1.646657\r\n\r\n# Calculating the t-critical value for the **bypass** sample\r\n\r\nbypass_sample<-539\r\nbypass_df<-bypass_sample - 1\r\nt_score_bypass<-qt(p=0.05, df=bypass_df,lower.tail=F)\r\nprint(t_score_bypass)\r\n\r\n\r\n[1] 1.647691\r\n\r\n# We now find the margin of error for both samples\r\n\r\n\r\nmargin_angio<- qt(0.05,df=angio_df)*9/sqrt(847)    #margin of error angiography\r\nprint(margin_angio)\r\n\r\n\r\n[1] -0.5092182\r\n\r\nmargin_bypass<-qt(0.05,df=bypass_df)*10/sqrt(539)   #margin of error bypass\r\nprint(margin_bypass)\r\n\r\n\r\n[1] -0.7097107\r\n\r\n# To calculate the lower bound and upper bound of the angiography sample\r\n\r\nlower_bound_angio<-angio_mean-margin_angio\r\nprint(lower_bound_angio)\r\n\r\n\r\n[1] 18.50922\r\n\r\nupper_bound_angio<-angio_mean+margin_angio\r\nprint(upper_bound_angio)\r\n\r\n\r\n[1] 17.49078\r\n\r\n# To calculate the lower bound and upper bound of the bypass sample\r\n\r\nlower_bound_bypass<-bypass_mean-margin_bypass\r\nprint(lower_bound_bypass)\r\n\r\n\r\n[1] 19.70971\r\n\r\nupper_bound_bypass<-bypass_mean+margin_bypass\r\nprint(upper_bound_bypass)\r\n\r\n\r\n[1] 18.29029\r\n\r\nTo determine which confidence interval is narrower I subtract the upper bound from the lower bound of each respective procedure.\r\nAngiography : [17.49,18.51] 18.51-17.49 = 1.02 \r\nBypass : [18.29,19.71] 19.71-18.29 = 1.42 \r\nThe angiography 90% confidence interval is narrower at 1.02 compared to the bypass of 1.42\r\nAnalysis\r\nWe see the mean wait time of the 90% confidence interval is from 17.49 to 18.51 days for the angiography procedure. Whereas the mean wait time of the 90% confidence interval is from 18.29 to 19.71 for the bypass procedure. This results in a narrower wait time of 1.02 days for the angiography compared to the bypass of 1.42 days. This could be attributed to the larger sample size for the angiography of 847 compared to a sample of 539 for the bypass. As well as a smaller standard deviation of 9 for the angiography compared to a standard deviation of 10 for the bypass.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success.\r\nFind the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\r\nConstruct and interpret a 95% confidence interval for p.\r\nSolution\r\nFirst, we find the point estimate,p, of the proportion.\r\n\r\n\r\n# Specify sample occurrences (x), sample size(n), and confidence_level\r\n\r\n\r\nx<- 567                     # survey respondents  (successes)\r\nn<- 1031                    # total surveyed\r\nconfidence_level<-0.95      # confidence level\r\npoint_estimate<-x/n         # the point estimate is the sample proportion\r\n\r\n\r\n\r\nNow to determine the 90% confidence interval I must find the alpha,the critical z-value, standard error and the margin of error.\r\n\r\n\r\nalpha<-(1-confidence_level)\r\ncritical_z<-qnorm(1-alpha/2)\r\nstandard_error<-sqrt(point_estimate*(1-point_estimate)/n)\r\nmargin_of_error<-critical_z*standard_error \r\n\r\n\r\n\r\nThe lower bound and upper bound of the confidence interval are calculated.\r\n\r\n\r\nlower_bound<-point_estimate-margin_of_error \r\nupper_bound<-point_estimate+margin_of_error\r\n\r\n\r\n\r\nResults\r\n\r\n\r\nsprintf(\"Point Estimate: %0.3f\", point_estimate)\r\n\r\n\r\n[1] \"Point Estimate: 0.550\"\r\n\r\nsprintf(\"Critical Z-value: %0.3f\", critical_z)\r\n\r\n\r\n[1] \"Critical Z-value: 1.960\"\r\n\r\nsprintf(\"Margin of Error: %0.3f\", margin_of_error)\r\n\r\n\r\n[1] \"Margin of Error: 0.030\"\r\n\r\nsprintf(\"Confidence Interval: [%0.3f,%0.3f]\", lower_bound,upper_bound)\r\n\r\n\r\n[1] \"Confidence Interval: [0.520,0.580]\"\r\n\r\nsprintf(\"The %0.1f%% confidence interval for the population proportion is:\", confidence_level*100)\r\n\r\n\r\n[1] \"The 95.0% confidence interval for the population proportion is:\"\r\n\r\nsprintf(\"between %0.4f and %0.4f\",lower_bound,upper_bound)\r\n\r\n\r\n[1] \"between 0.5196 and 0.5803\"\r\n\r\nConfidence interval interpretation\r\nWe have 95% confidence that the interval from the lower bound to the upper bound,[0.5196,0.5803] actually contains the true value of the population proportion of those in the sample believing a college education is valuable.\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nSolution\r\nHere’s what we know:\r\nmean population error = +/-5\r\nrange of the data - upper range - lower range = 200-30 = 170\r\npopulation standard deviation = Range/a quarter 170/4=42.5 which is sigma\r\nsignificance level or alpha = 0.05\r\nfrom this we can calculate the z-score or critical value\r\n\r\n\r\ncritical_z<-qnorm(1-0.05/2) #using the significance level or alpha we calculate z-score                 \r\nprint(critical_z)\r\n\r\n\r\n[1] 1.959964\r\n\r\n\r\n\r\nn_sample_size<-((1.96*42.5)/5)**2      #n=(z-score*standard deviation)/margin of error)**2\r\nprint(n_sample_size)                   #sample size\r\n\r\n\r\n[1] 277.5556\r\n\r\nThus, we see that we would need a minimum sample of 277.5556 or 278\r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\na)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses,test statistic, and P-value. Interpret the result.\r\nb)Report the P-value for Ha : μ < 500. Interpret.\r\nc)Report and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nSolution\r\na)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nHere’s what we know:\r\n\\(\\mu\\) mean income for all senior level workers = $500/ week\r\n\\(\\bar{y}\\) random sample of 9 female employees income = $410/week\r\ns standard deviation = 90\r\nn random sample female employees = 9\r\nHypotheses\r\nThe null and alternative hypotheses are:\r\n\\(H_0\\): \\(\\mu\\) = $500 per week\r\nThe null hypothesis weekly income for all senior-level workers is $500 per week\r\n\\(H_a\\): \\(\\mu\\) ≠ $500 per week\r\nThe alternative hypothesis suggests the mean weekly income is ≠ $500 (two-sided)\r\nTest Statistic\r\n\r\n\r\nt_test_income<-(410-500)/(90/sqrt(9))          #Test statistic using t-test\r\nprint(t_test_income)\r\n\r\n\r\n[1] -3\r\n\r\nP-Value\r\n\r\n\r\nn_random_sample<-9                      #random sample female employees\r\ndf_sample<-(n_random_sample-1)          #degrees of freedom\r\nt_test_income<-(410-500)/(90/sqrt(9))   #test statistic\r\np_val<-pt(t_test_income,df_sample)*2    #p-value\r\nprint(p_val)\r\n\r\n\r\n[1] 0.01707168\r\n\r\nInterpretation:\r\npart a\r\nAssuming alpha α = 0.05 and we know the p-value is 0.0171 The p-value 0.0171 < 0.05 , we reject the null hypothesis There is therefore sufficient evidence to claim the mean differs from the weekly income of $500.\r\npart b:\r\nReport the P-value for Ha : μ < 500. Interpret.\r\nHypotheses\r\n\\(H_0\\): \\(\\mu\\) = $500 per week\r\n\\(H_a\\): \\(\\mu\\) < $500 per week (left-tail test)\r\np-value = p(t < t_test_income) p(t < -3)\r\nP-value for \\(H_a\\) < $500 per week (left-tail test)\r\n\r\n\r\n#using the formula: pt(q,df,lower.tail=TRUE,log.p=FALSE) to find the p-value\r\n\r\n\r\nq<-(-3)\r\nn_random_sample<-9\r\ndf_sample<-(n_random_sample-1)                            #degrees of freedom \r\nleft_p_value<-pt(q,df_sample,lower.tail = T,log.p = F)    #p value for alternative hypothesis\r\nprint(left_p_value)         \r\n\r\n\r\n[1] 0.008535841\r\n\r\nInterpretation:\r\npart b\r\nSince the P-value 0.0085 is less than the presumed significance level,alpha α = 0.05 I reject the null hypothesis,\\(H_0\\). What this suggests is that there is sufficient evidence to conclude that the mean is < less than 500.\r\npart c:\r\nReport and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nHypotheses\r\n\\(H_0\\): \\(\\mu\\) = $500 per week\r\n\\(H_a\\): \\(\\mu\\) > $500 per week (right-tail test)\r\np-value = p(t > t_test_income) p(t > -3)\r\nP-value for \\(H_a\\) >$500 per week (right-tail test)\r\n\r\n\r\n#using the formula: pt(q,df,lower.tail=TRUE,log.p=FALSE) to find the p-value\r\n\r\n\r\nq<-(-3)\r\nn_random_sample<-9\r\ndf_sample<-(n_random_sample-1)                                  #degrees of freedom \r\nright_p_value<-pt(q,df_sample,lower.tail = F,log.p = F)         #p-value for alternative hypothesis\r\nprint(right_p_value)   \r\n\r\n\r\n[1] 0.9914642\r\n\r\n#verification sum of left and right tail p-values equal 1\r\n\r\nleft_p_value<-pt(q,df_sample,lower.tail = T,log.p = F) \r\nright_p_value<-pt(q,df_sample,lower.tail = F,log.p = F)\r\nleft_right_sum<-(left_p_value+right_p_value)\r\nprint(left_right_sum)\r\n\r\n\r\n[1] 1\r\n\r\nInterpretation:\r\npart c\r\nSince the p-value 0.9915 is greater than the presumed significance level alpha α = 0.05 \\(H_a\\) > 500,therefore we do not reject the null hypothesis \\(H_0\\). There is insufficient evidence to support the claim \\(\\mu\\) mean is > 500.\r\nQuestion 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7,with se = 10.0.\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nSolution\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nHere’s what we know :\r\nsample_n = 1000 each for Jones and Smith\r\ndf_sample_n<-(1000-1) degrees of freedom\r\nJones\r\n\\(\\bar{y}\\) = 519.5\r\nt = 1.95\r\np-value = 0.051\r\nse = 10.0\r\nSmith\r\n\\(\\bar{y}\\) = 519.7\r\nt = 1.97\r\np-value = 0.049\r\nse = 10.0\r\nHypotheses\r\n\\(H_0\\): \\(\\mu\\) = 500\r\n\\(H_a\\): \\(\\mu\\) ≠ 500\r\nJones We were already given both the test statistic 1.95 and p-value 0.051 for Jones so we are just verifying both.\r\nTest_statistic = (\\(\\bar{y}\\) - \\(\\mu\\))/10\r\n\r\n\r\ndf_sample_n<-(1000-1)  #degrees of freedom\r\n\r\nt_test_jones<-(519.5-500)/10    #Test statistic using t-test\r\nprint(t_test_jones)\r\n\r\n\r\n[1] 1.95\r\n\r\np_value_jones<-pt(t_test_jones,df_sample_n,lower.tail = F,log.p = F)*2\r\nprint(p_value_jones)\r\n\r\n\r\n[1] 0.05145555\r\n\r\nSmith\r\nWe were already given both the test statistic 1.97 and p-value 0.049 for Smith so we are verifying both.\r\nTest_statistic = (\\(\\bar{y}\\) - \\(\\mu\\))/10\r\n\r\n\r\ndf_sample_n<-(1000-1)            #degrees of freedom\r\n\r\nt_test_smith<-(519.7-500)/10    #Test statistic using t-test\r\nprint(t_test_smith)\r\n\r\n\r\n[1] 1.97\r\n\r\np_value_smith<-pt(t_test_smith,df_sample_n,lower.tail = F,log.p = F)*2       #p-value for smith\r\nprint(p_value_smith)                                                         #output smith\r\n\r\n\r\n[1] 0.04911426\r\n\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nFor α = 0.05 we reject the null hypothesis \\(H_0\\) if the p-value is greater than 0.05 and do not reject if the p-value is equal or greater to => \\(H_0\\). So in the case of Jones since the p-value 0.051 is negligibly larger than alpha, it is not statistically significant and we fail to reject the null hypothesis \\(H_0\\). In the case of Smith, the p-value of 0.049 is less than alpha, α = 0.05 which is less than the level of significance, we reject the null hypothesis since a smaller p-value suggests statistical significance.\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nIn this study the p-values are negligibly the same between Jones and Smith. However, in spite of this because our predetermined significance p-value is ≤ 0.05 in the case of Smith, the null hypothesis is rejected and we conclude there is statistical evidence for the alternative hypothesis \\(H_a\\). Conversely, if our predetermined significance p-value is greater than 0.05 as in the case of Jones then we fail to reject the null hypothesis. There is insufficient evidence to draw any conclusions.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nSolution\r\nHere’s what we know:\r\nsample_gas_taxes<-18\r\ndf_gas_taxes<-sample_gas_taxes-1\r\n95% confidence level (presumptive)\r\nsignificance level alpha = 0.05 based on a 95% confidence level\r\nz-score = 1.96 based on 95% confidence level\r\nFrom this information we can determine the following:\r\n\r\n\r\n#manual calculation of t-score used for finding the upper and lower interval of gas_tax_sample\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\ngas_taxes_sample<-18                                           #gas taxes sample size\r\n\r\ndf_gas_taxes<-gas_taxes_sample-1                               #degrees of freedom \r\nmean_gas_taxes<-mean(gas_taxes)                                #mean gas taxes\r\nt_score_gas_taxes<-qt(p = 0.05,df=df_gas_taxes,lower.tail = F) #t_score\r\nsd_gas<-sd(gas_taxes)                                          #standard deviation\r\nm_error_gas_taxes<-qt(0.05,df=df_gas_taxes)*sd_gas/sqrt(18)    #margin of error gas taxes\r\n\r\n\r\n\r\n\r\n \r\n#Now that all of the needed parameters for lower and upper bounds have been calculated, I can find the confidence interval for the gas taxes sample.\r\n\r\n\r\n\r\nmean_gas_taxes<-mean(gas_taxes)  \r\nm_error_gas_taxes<-qt(0.05,df=df_gas_taxes)*sd_gas/sqrt(18)\r\n\r\nlower_gas_tax<-(mean_gas_taxes-m_error_gas_taxes) #lower bound using mean and margin of error\r\nprint(lower_gas_tax)\r\n\r\n\r\n[1] 44.67946\r\n\r\nupper_gas_tax<-(mean_gas_taxes+m_error_gas_taxes) #upper bound using mean and margin of error\r\nprint(upper_gas_tax)\r\n\r\n\r\n[1] 37.0461\r\n\r\nThe 95% confidence interval is [37.0461 ,44.6795] using manual calculations.\r\nBecause the average tax per gallon is less than 45 cents, it is within the lower and upper bounds of the 95% confidence interval.We can therefore reasonably conclude that there is sufficient evidence that the confidence interval contains taxes less than 45 cents.\r\nAlternative outcome using t.test:\r\n\r\n\r\ngas_taxes<- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nmean(gas_taxes)\r\n\r\n\r\n[1] 40.86278\r\n\r\nt.test(gas_taxes,conf.level = 0.95)                            #one sample t-test\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 9.555e-13\r\nalternative hypothesis: true mean is not equal to 0\r\n95 percent confidence interval:\r\n 36.23386 45.49169\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThe 95% confidence interval is [36.23386 ,45.49169]\r\nThere is not enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents because 45 cents is inside the confidence interval. The confidence interval contains taxes greater than 45 cents.\r\nPlease note I am not sure why there is a difference between 95% confidence intervals when calculated manually versus the t.test function. I therefore include both outcomes.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomdacssjoe870700/",
    "title": "603, Homework 1",
    "description": "First homework assignment for DACSS 603",
    "author": [
      {
        "name": "Joe Davis",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nI wanted to do this one a bit longer-form than necessary, but I also appreciate the chance to practice organizing my R code neatly while solving. I need some good repetition to build those good habits.\nFor this question I assigned all of the values in the table to objects to use in the calculations of each treatment’s t-scores, margins of error, and confidence intervals. I did a quick check of the qnorm(.95) to see if the t-scores had converged to the normal distribution given the relatively high sample sizes of each treatment. Surprisingly, to me at least, rounding at the .000 level they still had very slightly different levels both to the normal distribution and to each others’ scores.\nI know I have the standard deviations, wait time would be a continuous variable, and these samples are larger than n = 30 so I can use qnorm and the 1.645 z critical value clearing those assumptions. But, it seems like I should use the distribution with fatter tails relative to the sample size values from qt regardless if I could assume normality from the other conditions, as to err on the side of caution for analyzing and communicating medical procedure data. I have a note in my code on that point, mostly for my own reference and will solve for the normal distribution values as well. After running the calculations the practical effect would be introducing slightly more uncertainty in the mean wait time for the angiography procedure while still keeping that procedures’ interval narrower, as we’d expect, than that of the bypass procedure. Especially if this was to be used to set patient expectations on wait times, the wider range would be the option a hospital or doctor would communicate, but we do appear to be in the range where t and z distributions are becoming quite similar.\n\n\n## Means, Total N,  and SDs from full question text. B = Bypass A = Angiography\n  #mean wait times for each procedure\n  mean_b <- 19 \n  mean_a <- 18 \n  \n  #standard deviations\n  sd_b <- 10\n  sd_a <- 9\n  \n  #total N\n  n_b <- 539\n  n_a <- 847\n\n## 90% CI score finding. .90 = 1 - a and I need  a/2 for the two tails since I have no theory on why the direction of the error would be important. Did a little side exploring of the t distribution and normal distribution scores below.\n  \n\n#Rounded to 1.645 but not used for calculations. I thought it was a sufficiently large sample size that that qt and qnorm should have returned the same values at .000 rounding, but after checking those, each T rounded up differently at the 3rd digits and for medical data I would rather err on the side of caution  \n\n    #normal z score\n  z_heart <- qnorm(.95) \n  round(z_heart, digits = 3)\n\n\n[1] 1.645\n\n  #check this for t scores\nb_t <- round(qt(.95, df = n_b -1), digits = 3)\na_t <-  round(qt(.95, df = n_a -1), digits = 3)\n  \n  #print them\n  b_t\n\n\n[1] 1.648\n\n  a_t\n\n\n[1] 1.647\n\n## 90% CI standard error of mean/ margin of error\n  #Take the score multiplied by standard deviations and sqrt of Ns, t value\n  mofe_bypass <- b_t*(sd_b/sqrt(n_b))\n  mofe_angiography <- a_t*(sd_a/sqrt(n_b))\n  \n  #doing this with z 1.645\n  mofe_bypass_z <- 1.645*(sd_b/sqrt(n_b))\n  mofe_angiography_z <- 1.645*(sd_a/sqrt(n_a))\n  \n  #print the normal distribution score MoE\n  mofe_bypass_z\n\n\n[1] 0.7085517\n\n  mofe_angiography_z\n\n\n[1] 0.5087058\n\n  # Print them using t value\n  mofe_bypass\n\n\n[1] 0.7098439\n\n  mofe_angiography\n\n\n[1] 0.6384718\n\n## +/- from the data set mean for the range\n  \n  #bypass upper and lower\n  bypass_lower <- mean_b - mofe_bypass\n  bypass_upper <- mean_b + mofe_bypass\n  \n  #combine the upper and lower to list interval\n  ci_bypass <- print(c(bypass_lower, bypass_upper))\n\n\n[1] 18.29016 19.70984\n\n  #angiography upper and lower\n  angio_lower <- mean_a - mofe_angiography\n  angio_upper <- mean_a + mofe_angiography\n  \n\n  \n  #combine the upper and lower to list interval\n  ci_angiography <- print(c(angio_lower, angio_upper))\n\n\n[1] 17.36153 18.63847\n\n#Normal distribution 90% CI\n  #bypass Z\n  bypass_lower_z <- mean_b - mofe_bypass_z\n  bypass_upper_z <- mean_b + mofe_bypass_z\n  \n  ci_bypass_z <- (print(c(bypass_lower_z, bypass_upper_z)))\n\n\n[1] 18.29145 19.70855\n\n  #angiography Z\n  angiography_lower_z <- mean_a - mofe_angiography_z\n  angiography_upper_z <- mean_a + mofe_angiography_z\n  \n  ci_angiography_z <- (print(c(angiography_lower_z, angiography_upper_z)))\n\n\n[1] 17.49129 18.50871\n\nThe confidence interval is narrower for angiography, as we have a larger sample size for that procedure’s wait time and a smaller standard deviation. We would expect this as in theory as the larger sample size mean should be closer to the true population mean, and the smaller standard deviation means we have less variation to begin with. That expectation is shown in both the smaller numerator and the larger denominator produced during the margin or error calculation. For comparison on the denominators, since the standard deviations were already listed in the table: 23.2163735 for bypass and 29.1032644 for angiography. For completeness, the normal distribution CI’s are 17.4912942, 18.5087058 for angiography and 18.2914483, 19.7085517 for the bypass procedure.\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nSince we are looking to construct this interval around a proportion, I used the prop.test function to construct the interval using the total sample size as the n input and the 567 raw respondents for college “being essential for success” as the success vector in the function. I decided to use this function versus hand calculating as in question 1.\n\n\nprop_coll_success <- prop.test(567, 1031, conf.level = .95)\nprop_coll_success\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nnames(prop_coll_success)\n\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"   \n[5] \"null.value\"  \"conf.int\"    \"alternative\" \"method\"     \n[9] \"data.name\"  \n\nThe point estimate from the survey data is 0.55 The 95% confidence The interval is 0.52, 0.58. This interval means that we could say we are 95% confident that the proportion of American adults who believe that “college education is essential for success” is somewhere between the lower and upper end of our confidence interval – assuming the initial survey was indeed random and representative. Representativeness (and randomness, but that’s already extremely difficult with surveying) would be especially important for this question depending on what variables were used to determine that the initial sample was representative, as beliefs around college education are increasingly subject to the impacts political polarization and thus we could be breaking some of our assumptions needed for our analysis to be accurate depending on which variables were used.\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nFor this problem, I started off assigning all of the elements of the problem to objects as I did for the prior two. The standard deviation problem is shown in sd_books below, as 1/4 of the difference between $200 and $30. The critical z for 5% significance level is shown in finding the qnorm result for half of the alpha level. The margin of error we need to aim for is 5 dollars and that is assigned to book_moe. I assumed it would take a decent sized sample, at least larger than 30, to get within 5 dollars with that large of a standard deviation and the we would be able to randomly collect this sample, so I used the large sample size for estimating mean equation to find the sample size n. It took too long to figure out the fancy letters in Rmarkdown, so I used abbreviations for standard deviation and such, unfortunately. \\[n =  sd ^ {2} (z /  M) ^ {2}\\]\n\n\n#Assign all of the elements of the problem to objects\nsd_books <- (200-30)*.25\nsd_books\n\n\n[1] 42.5\n\nbook_z <- qnorm(.975) \nbook_z\n\n\n[1] 1.959964\n\nbook_moe <- 5\n\n# Formula for n from margin of error calculation, large sample is n = sd^2 * z a/2 / M. Calculate by hand first.\nbook_n_by_hand <- sd_books ^ 2 * (book_z / book_moe) ^ 2\n\nbook_n_by_hand\n\n\n[1] 277.5454\n\n#Use the samplingbook package to confirm.\nbook_n_package <- sample.size.mean(e = book_moe, S = sd_books, level = .95)\n\nbook_n_package\n\n\n\nsample.size.mean object: Sample size for mean estimate\nWithout finite population correction: N=Inf, precision e=5 and standard deviation S=42.5\n\nSample size needed: 278\n\nAfter finding the answer of 278 –rounded up– students in the sample by hand calculating n, I wanted to check my work using the samplingbook package and putting the same elements from the problem into the function. That answer, shown above in book_n_package matched my “hand” calculation of 278 students in the sample needed to have a mean estimate within 5 dollars of the true population mean of textbook costs.\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s =90. \n-A)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n-B) Report the P-value for Ha : μ < 500. Interpret.\n-C) Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nSince this question has several follow up questions to report out, I’m going to dive right in to the code portion of the work, and I’ll have all of the narrative explanations and steps of work described there.\n\n\n#question elements assigned to objects, will use alpha = .05\n  \n  #population\n  all_union_mean <- 500\n  \n  #sample\n  women_mean <- 410\n  women_sd <- 90\n  women_n <- 9\n\n#get the test statistic t\n\n  #estimated standard error\n  women_se <- women_sd / sqrt(women_n)\n  women_se\n\n\n[1] 30\n\n  #test statistic\n  women_t <- (women_mean - all_union_mean) / women_se\n  women_t\n\n\n[1] -3\n\n#find two tail p-value, round to two digits\n  women_p <- round(2*pt(-abs(women_t), df = 8, lower.tail = FALSE ), digits = 2)\n  women_p\n\n\n[1] 1.98\n\n#p for < 500, round to two digits\n  women_lower_p <- round(pt(-abs(women_t), df = 8, lower.tail = TRUE), digits = 2)\n  women_lower_p\n\n\n[1] 0.01\n\n#p for > 500, round to two digits\n  women_greater_p <- round(pt(-abs(women_t), df = 8, lower.tail = FALSE), digits = 2)\n  women_greater_p\n\n\n[1] 0.99\n\nAssumptions: Because this is a small sample, I’m using the two-sided t-test as it is robust when the data may not clear the normality assumptions. Given the smaller sample size of the study, highly skewed data could impact one-tailed tests and the question didn’t explicitly state that they were looking higher or lower than the overall union average\nHypotheses: Null hypothesis is that the mean wage for women = 500 dollars, the same as the contract required mean for all senior workers at the union. The alternative hypothesis is that the mean wage for women =/= 500 dollars.\nTest statistic: The test statistic women_t has a value of -3. Since this is a negative value due to the sample mean being lower than the population mean, it’s important to remember that the absolute value should be used in calculating the p value.\nP value: The two-sided P value from women_t with 8 degrees of freedom equals 1.98.\nInterpreting the results: Since the two-tailed P value is lower than our pre-selected alpha level of .05 by some distance, we can reject the null hypothesis that the mean wage for women is equal to 500 dollars, and accept the alternative hypothesis that it is not equal to 500. The below data points make it seem as though it is very likely below the overall union mean, which lines up with the mean and sd data from their study. If only the very outer bounds of the deviation hits the overall mean, it seems like this all confirms that the female employee mean is < 500 dollars. I would suggest initiating the process of review, confirmation of the study, and the grievance process.\nQuestion B: The P value for the womens’ mean weekly wage being less than 500 is 0.01. This means that if our null hypothesis was true, we would have a 99% chance of getting a value below 500 for our sample mean. Being that the contract specifies an overall mean of 500 and it’s stated that this is a large company so we can assume normality through the Central Limit Theorem, we have a lot of evidence that the mean weekly wage for women is likely lower than what the contract specifies. Being that we have a small sample size it is possible for the one-tailed value to be off with highly skewed data, however. Given the rejection of the null and the one tail results here and below for greater than 500, I think the women’s group would have a strong case that their mean wage is not 500 and most likely lower than 500 dollars.\nQuestion C: The P value for the mean weekly wage for women being greater than 500 is 0.99. This means that if the null hypothesis was true, we would expect to get a mean weekly wage for women greater than the population mean wage 1% of the time. With the same small sample size caveat as above on one tailed results, this split in P values which would have us fail to reject and then reject the null hypothesis respectively, for above and below the population mean is quite extreme and would be another supporting point in the group filing a grievance.\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519. 7,with se = 10.0.\nA) Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. \nB) Using α = 0.05, for each study indicate whether the result is “statistically significant.” \nC)Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nSame as above, I’m going to assign out the objects and then go through the solution steps below the R code chunk. The answer for question A will be in the R code chunk, while I will answer the narrative aspects of the other questions in the text below.\n\n\n#Question Elements\n  #both studies with same n\n  study_n <- 1000\n\n  #null mean = 500\n  #alt mean =/= 500\n\n  #jones study elements\n  jones_mean <- 519.5\n  jones_se <- 10.0\n\n  #smith study elements\n  smith_mean <- 519.7\n  smith_se <- 10.0\n\n\n#Solve for Smith\n  \n  #t for Smith, use two digits and it must equal 1.97\n  smith_t <- round((smith_mean - 500) / smith_se,  digits = 2)\n  smith_t #It's 1.97 like it's supposed to be,  yay!\n\n\n[1] 1.97\n\n  #P for Smith, use three digits and two-tailed \n  smith_p <- round(2 * pt(-abs(smith_t), df = 999), digits = 3)\n  smith_p # It's .049 like it's supposed to be!\n\n\n[1] 0.049\n\n#Solve for Jones\n  \n  #t for Jones, use two digits and it must equal 1.95.\n  jones_t <- round((jones_mean - 500) / jones_se, digits = 2)\n  jones_t #It's 1.95 like it's suppost to be, yay!\n\n\n[1] 1.95\n\n  #P for Jones, use three digits and it must equal .051\n  jones_p <- round(2 * pt(-abs(jones_t), df = 999), digits = 3)\n  jones_p #It's .051 like it's supposed to be!\n\n\n[1] 0.051\n\nQuestion B and C: I thought it made sense to answer these questions in one response versus splitting them up by bullet point. Technically, both studies would be “statistically significant” at the .05 level as rounding .049 and .051 to two digits would take them both to less than or equal to .05. Rounding them without disclosing that would be no good. This is also quite misleading to attribute practical significance to a difference in P values of .002 to our arbitrarily set level of significance. Practically speaking, there is no real difference in the outcomes of these studies.\nWithout rounding, only Smith’s .049 would be below the alpha level and Jones’ .051 would be above. With these fine margins, reporting only whether the null was rejected or if we failed to reject it, or even just listing the “p less than or equal to .05 or p greater than .05” statement without the raw p values included would also ascribe practical significance in difference to these two studies even though they are very nearly identical. The differences could very well be random noise and chance, and reporting out the statement alone would make it harder to asses that.\nChoosing to publish Smith’s study just because it cleared the threshold and not Jones’ as well could make it harder to see the full picture of the parameter they studied or analyze the overall random variability in the experimental findings in a meta analysis setting.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period.\nThe sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAfter reading the question, I don’t think our initial sample was collected in a way that would allow us to be confident in a confidence interval constructed from it to look at the average price in the nation as a whole in 2005. If the federal tax is constant at 18.4, it looks like most of the variability from city to city in the sample data is driven by state and local tax levy decisions. The mean for the gas_tax data is 40.86, so over half of our mean amount comes from state and local variables that do not look to be accounted for in the sampling description as is. Most Americans in 2005 (and still today), did not live in very large cities, the amount depending of course on the exact definition of “large.”\nOur sample is only from 18 large cities in the country, and I think it’s reasonable to assume large cities have, or the very least could have, systemically different gas tax policies than other population densities. Some states could have caps or specific legislation that could impact the overall national average, and there’s not enough information about how the 18 large cities were selected or sampled to clear all of the assumptions, even if we didn’t think large cities varied from suburban, exurban or rural geographies in a way that would skew the small sample of data that we do have. Exploring bootstrapping or other approaches would also be impacted by this fact.\nNow, if I’m reading entirely too much into this set up and I should just show that I can evaluate if our mean is below a set level using a confidence interval, I’ll proceed to do that, too! To answer this question, I’ll use the psych package to get the descriptive statistics and look those over for fun and possible use to calculate by hand if the t.test result looks funny. Then I’ll use t.test(gas_tax) and look at the 95% CI range.\nIf the entire confidence interval is below 45 cents we would have enough evidence to say we think it’s 95% likely that the mean gas tax is below the 45 cent level, the equivalent of rejecting the null hypothesis and accepting the alternative hypothesis of gas taxes were likely lower than 45 cents. If the interval includes 45 and/or above that level we don’t have enough information, and would be doing the equivalent of failing to reject the null hypothesis.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n#Use describe from the psych package for overview of gas_tax data, could use the variables here to hand calculate the CI.\ngas_tax_summary <- describe(gas_taxes)\ngas_tax_summary\n\n\n   vars  n  mean   sd median trimmed  mad   min   max range  skew\nX1    1 18 40.86 9.31  41.47   41.41 9.72 18.49 54.41 35.92 -0.58\n   kurtosis   se\nX1    -0.32 2.19\n\n#Use t.test on gas_taxes to see the 95% CI\nt.test(gas_taxes)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 18.625, df = 17, p-value = 9.555e-13\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\nThe 95% interval does contain 45 cents, so we cannot say that we have enough evidence at this confidence level and interval to do the equivalent of rejecting the null hypothesis. Since the null value is at the very very top of the interval, and putting aside the other issues with the sample to begin with, it seems like the sort of result where we could say in practice it was likely lower than 45 cents. Since gas taxes and prices go to the third digit at the pump, our result would have a maximum 95% CI level “at the pump” reading of .455. It feels safe to describe in actual practice with that interval that we’re confident it was 45 cents or lower, even if we’d need a bit more data to say it in specific statistical terms.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-27T23:56:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomegeeslindacss-603-hw1/",
    "title": "HW1_DACSS603",
    "description": "DACSS 603 Homework 1",
    "author": [
      {
        "name": "Eliza Geeslin",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n  surgical_procedure sample_size mean_wait_time standard_deviation\n1            Bypasss         539             19                 10\n2        Angiography         847             18                  9\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nI will do the following to calculate the confidence intervals:\nCalculate the mean\nCalculate the standard error of the mean\nCalculate n\nDetermine a confidence level\nFind the t-score\nCalculate interval\nI will start with the values we know.\n\n\n# we already know the mean, sample size, and standard deviation\n# creating variables for all the values we do know\nbypass_n <- 539\nbypass_mean_wait_time <- 19\nbypass_sd <- 10\nangio_n <- 847\nangio_mean_wait_time <- 18\nangio_sd <- 9\n\n\n\nNext, I will specify the confidence level and use that the calculate the tail area.\n\n\n# specify confidence level\n# calculate tail area - we can use this for both the angio and bypass confidence intervals\n\nconfidence_level <- 0.90\ntail_area <- (1 - confidence_level)/2 # divide by two because we care about both sides.\n\ntail_area\n\n\n[1] 0.05\n\nThen I will use the tail areas to calculate the t-scores and confidence intervals. I will start with the bypass surgery.\n\n\n# bypass\n# calculate t-score\nbypass_t_score <- qt(p = 1 - tail_area, df = bypass_n - 1)\n\nbypass_t_score\n\n\n[1] 1.647691\n\n\n\n# bypass \n# calculate confidence internal\n\nbypass_lower <- bypass_mean_wait_time - bypass_t_score * bypass_sd / sqrt(bypass_n)\nbypass_upper <- bypass_mean_wait_time + bypass_t_score * bypass_sd / sqrt(bypass_n)\n\nprint(c(bypass_lower, bypass_upper))\n\n\n[1] 18.29029 19.70971\n\nThe confidence interval for the bypass surgery is between 18.29029 and 19.70971 days.\nNext, I will do the same for the angio surgery.\n\n\n# angio\n# calculate t-score\n\nangio_t_score <- qt(p = 1 - tail_area, df = angio_n - 1)\n\nangio_t_score\n\n\n[1] 1.646657\n\n\n\n# margin of error and confidence interval - angio\n\nangio_lower <- angio_mean_wait_time - angio_t_score * angio_sd / sqrt(angio_n)\nangio_upper <- angio_mean_wait_time + angio_t_score * angio_sd / sqrt(angio_n)\n\nprint(c(angio_lower, angio_upper))\n\n\n[1] 17.49078 18.50922\n\nThe confidence interval for the angiography surgery is between 17.49078 and 18.50922 days.\nWe can calculate that the confidence interval for mean days waiting is narrower for the angiography surgery than for the bypass surgery, but it also may be easier to see in graph form:\n\n\n# add confidence intervals to df\nsurgical_procedure = c('Bypasss', 'Angiography')\nsample_size = c(bypass_n, angio_n)\nmean_wait_time = c(bypass_mean_wait_time, angio_mean_wait_time)\nstandard_deviation = c(bypass_sd, angio_sd)\nlower = c(bypass_lower, angio_lower)\nupper = c(bypass_upper, angio_upper)\n\ndf <- data.frame(surgical_procedure, sample_size, mean_wait_time, standard_deviation, lower, upper)\n\n# compare confidence intervals - plot\n\nggplot(df) +   \n  geom_point(aes(x = surgical_procedure, y = mean_wait_time), color = \"#9784c2\", size = 3) +\n  geom_errorbar(aes(x = surgical_procedure, ymin = lower, ymax = upper), color = \"#9784c2\", width = 0.5) +\n  labs(x = \"Surgical Procedure\", y = \"Mean Wait Time (Days)\") +\n  geom_text(aes(x = surgical_procedure, y = upper, label = round(upper, digits = 2)), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -3) +\n  geom_text(aes(x = surgical_procedure, y = lower, label = round(lower, digits = 2)), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -3) +\n  geom_text(aes(x = surgical_procedure, y = mean_wait_time, label = mean_wait_time), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -1) +\n  theme(axis.text.x = element_text(family = \"Avenir\", color = \"#33475b\", size=10),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#33475b\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#33475b\", size=13),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#33475b\", size=13))\n\n\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nFirst, we want to find the point estimate (p) and then construct the confidence interval because that will be much more accurate than a single point.\n\n\n#find the point estimate\n\ncollege_education_essential <- 567\nsurvey_n <- 1031\n\npoint_estimate <- college_education_essential/survey_n\n\npoint_estimate\n\n\n[1] 0.5499515\n\nThe next thing I am going to do is calculate the margin of error on either side of the point estimate. For a 95% confidence interval, the alpha is 0.05, which means that the z-score is 1-(0.05/2) = 0.975). We can use the z-score because we are assuming a normal distribution and the sample size is greater than 30.\n\n\n# calculate the error\n\nerror <- qnorm(0.975)*sqrt(point_estimate*(1-point_estimate)/survey_n)\n\nerror\n\n\n[1] 0.03036761\n\n\n\n# calculate the confidence interval\n\nupper2 <- point_estimate + error\nlower2 <- point_estimate - error\n\nprint(c(lower2, upper2))\n\n\n[1] 0.5195839 0.5803191\n\nprint(c(round(lower2, digits = 3), round(upper2, digits = 3))) # round\n\n\n[1] 0.52 0.58\n\nHere we can see for that our sample proportion our point of estimate is 0.5499515. The 95% confidence interval indicates that the population mean is between .520 and .580. In other words, the percentage of Americans who believe college is important is between 52% and 58%. This means that when a a series of representative samples are created, 95% of the time the true mean should be between .520 and .580 (the result of % of Americans who believe college is important should be between 52% and 58%).\nAlternative Approach\nWe could also use prop.test() using the same numbers that would tell us automatically what the point of estimate and confidence are.\nconf.level = 0.95 (this is also the default for prop.test() but we will still specify)\nx = the number of “successes” (in this case it is the number of survey respondents who say that college education is needed)\nn = number of survey respondents.\n\n\n# calculate the confidence interval\n\nprop.test(x = 567, n = 1031, conf.level = 0.95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nNOTE: There is a slight difference in the margin of error (less than .001). I suspect this has to do with how standard deviation is calculated (rounded) by the prop.test() in r. If we assume that we are rounding to the nearest hundredth this might not even be noticed.\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nRight away we know a few key things:\nrange = the difference between $30 and $200 (170)\nz-value = significance level is 5%, the alpha is 0.05, which means that the z-score is 1-(0.05/2) = 0.975)\nmargin = the estimate is useful within $5 on either side, so the margin is 5.\nThe first thing we will do is calculate the standard deviation, which we know is a quarter of the range.\n\n\n#range - difference between $30 and $200\n\nrange <- 170\n\n#significance level is 5%, alpha is 0.5\nz <- qnorm(1-(0.05/2))\n\n#margin within $5 of the true population mean - margin = 5\n\nmargin <- 5\n\n#calculate sd (\"standard deviation is a quarter of the range\")\n\nsd <- range*0.25\n\nsd\n\n\n[1] 42.5\n\nNow that I have the standard deviation, I can calculate the sample size.\n\n\n#Now I can calculate the sample size with the formula n = (z-value/margin)^2.\n\nsample_size <- ((1.96*sd)/margin)^2\n\nsample_size\n\n\n[1] 277.5556\n\nround(sample_size, digits = 0) #round to the nearest whole person\n\n\n[1] 278\n\nRounding to the nearest whole person, we get 278. Interpreting that, in order for the financial aid office to estimate the mean cost of textbooks (+ or - $5) with a significance level of 5%, they should sample 278 students.\nQuestion 4\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410’and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nReport the P-value for Ha : μ < 500. Interpret.\nReport and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nAssumptions:\nnull hypothesis (H0) is that the mean weekly earnings for the population of women at the company is $500 per week: μ = 500\nalternative hypothesis (Ha) is that the mean weekly earnings for the population of women at the company is not $500 per week: μ =/= 500\nsample size = 9\nsample mean = 410\nsample standard deviation = 90\nFirst, I will find the t-score and then I will calculate the p-value. I will assume a 95% confidence level. t-statistic and use it to find the p-value. We can use t = (sample mean - hypothesized mean)/ (sample standard deviation / sqrt(n))\n\n\n# start with what we know\n\nsalary_mean <- 410\nsalary_sd <- 90\nsalary_n <- 9\n\n# find the t-score t = (sample mean - hypothesized mean) / (sample standard deviation / sqrt(n))\n# hypothesized mean = null hypothesis = 500\n\nsalary_t_score <- (salary_mean - 500)/(salary_sd/sqrt(salary_n))\n\nsalary_t_score\n\n\n[1] -3\n\n# now I will find the p-value using pt()\n\npt(q = salary_t_score, df = salary_n-1)*2 # multiplied by two because this is a two-tailed test\n\n\n[1] 0.01707168\n\nAt the 95% confidence interval we can reject the null hypothesis that mean income is $500 based on the alternate hypothesis that mean income is not $500 because the p value is less than .05. Additionally, we see that, at the 95% confidence interval, the null hypothesis ($500) falls outside of the interval.\nNow we’ll look at the p-value if the alternative hypothesis is μ < 500.\n\n\n#p-value of the right side only (less than 500)\n\npt(q = salary_t_score, df = salary_n-1)\n\n\n[1] 0.008535841\n\nAt the 95% confidence interval we can reject the null hypothesis that mean income is $500 based on the alternate hypothesis that mean income is less than $500 because the p value 0.001, which is less than 0.05.\nNow we’ll look at if the alternative hypothesis is μ < 500.\n\n\n#p-value of the left side only\n\npt(q = salary_t_score, df = salary_n-1, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\nAt the 95% confidence interval we cannot reject the null hypothesis that mean income is $500 based on the alternate hypothesis that mean income is greater than $500 because the p value 0.99, which is greater than 0.05 and is not significant at that level.\nQuestion 5\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519. 7,with se = 10.0.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nStarting with Jones, we will find the t and p values:\nt = (sample mean - null hypothesis)/(sample standard error).\nse = 10\nsample mean = 519.5\nnull hypothesis = 500\nWe will find the p-value using pt(). Since this is a two-tailed test, we will multiply the result by two.\n\n\n# t = (y-hat - H0)/se\n#Jones got population mean of 519.5 with standard error of 10.0\n\njones_t <- (519.5-500)/10\n\n# now we are conducting a 2-sided test. Find the area to the right of 1.95 and the area to the left of -1.95 to get p-value\n# use pt(); degrees of freedom are n-1 = 999\n# pt() finds the area to the left of a value\ndf <- 999\n\njones_p <- pt(q = -1.95, df = 999) + pt(q = 1.95, df = 999, lower.tail = FALSE) #lower tail + upper tail\n\njones_t\n\n\n[1] 1.95\n\njones_p\n\n\n[1] 0.05145555\n\nWe get the same results as the ones we are given; t = 1.95, p = 0.51.\nNow we will verify the t and p values for Smith:\nt = (sample mean - null hypothesis)/(sample standard error).\nse = 10\nsample mean = 519.7\nnull hypothesis = 500\nWe will find the p-value using pt(). Since this is a two-tailed test, we will multiply the result by two.\n\n\n# Smith got population mean of 519.7 with standard error of 10.0\n\nsmith_t <- (519.7-500)/10\n\n# now we are conducting a 2-sided test. Find the area to the right of 1.97 and the area to the left of -1.97 to get p-value\n\nsmith_p <- pt(q = -1.97, df = 999) + pt(q = 1.97, df = 999, lower.tail = FALSE) #lower tail + upper tail\n\nsmith_t\n\n\n[1] 1.97\n\nsmith_p\n\n\n[1] 0.04911426\n\nWe get the same results as the ones we are given; t = 1.97, p = 0.49.\nUsing a significance level of 0.05, Jones will not be able to reject the null hypothesis because his p-value is greater than 0.05 (0.051), and his results are deemed “not statistically significant. Smith, with a p-value of 0.049 is able to reject the null hypothesis and say his results are”statistically significant.\"\nThe issue is that both of these studies are have such similar results, but because one p-value is less than 0.05, that study is deemed significant and would get published, while the other one may not. Additionally, Smith reporting a result of “p < 0.05” rather than the actual p-value can be misleading because it is so close to 0.05, and the reader of the study may assume that the result is more significant than it actual is if they don’t know the actual p-value.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nFirst we calculate what we can and use those values to calculate the t-score. We will assume that p = 0.05 because we are looking at a 95% confidence interval.\n\n\n# calculate values that we can\n\nmean_gt <- mean(gas_taxes) # mean\nsd_gt <- sd(gas_taxes) # standard deviation\nn_gt <- 18 # sample size\nse_gt <-(sd_gt/sqrt(18)) # standard error\n\n# calculate t-score using qt()\n# we know that the p = 0.05 because we are looking for the 95% confidence interval.\n# df = 18 - 1\n# we are looking for the lower.tail, which is the default (\"less than 45%\")\n\nt_gt <- qt(p = 0.05, df = 17)\n\nt_gt\n\n\n[1] -1.739607\n\nFrom here we can calculate the error margin on each side of the mean: error = t * sd/sqrt(n)\n\n\n# calculate margin of error t * sd/sqrt(n)\n\nme_gt <- t_gt * sd_gt/sqrt(n_gt)\n\n# then we get the upper and lower bounds of the confidence interval\n\nupper3 <- mean_gt + me_gt\nlower3 <- mean_gt - me_gt\n\nprint(c(upper3, lower3))\n\n\n[1] 37.04610 44.67946\n\nBased on the sample from the 18 cities, the 95% confidence interval for the average tax per gallon of gas in the US is between 37.05 cents and 44.68 cents.\nIf the null hypothesis is the average tax per gallon of gas is the US in 2005 45 cents (μ = 45), we reject the null hypothesis because the upper bound of the 95% confidence interval is 44.68 cents. If the alternate hypothesis is that the average tax per gallon of gas in the US in 2005 was less than 45 cents (μ < 45) than we accept the alternate hypothesis.\n\n\n\n",
    "preview": "posts/httpsrpubscomegeeslindacss-603-hw1/distill-preview.png",
    "last_modified": "2022-02-27T23:55:46-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomektracy866318/",
    "title": "HW1 V2",
    "description": "603 Homework 1",
    "author": [
      {
        "name": "Erin Tracy",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\nQUESTION 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\r\n\r\n\r\nconfidence_level <- 0.9\r\ns_size_bypass <- 539\r\ns_size_angio <- 847\r\nS_mean_bypass <- 19\r\ns_mean_angio <- 18\r\ns_sd_bypass <- 10\r\ns_sd_angio <- 9\r\n\r\ntail_area <- (1-confidence_level)/2\r\ntail_area\r\n\r\n\r\n[1] 0.05\r\n\r\nt_score_bypass <- qt(p = 1-tail_area, df = s_size_bypass-1)\r\nt_score_bypass\r\n\r\n\r\n[1] 1.647691\r\n\r\nt_score_angio <- qt(p = 1-tail_area, df = s_size_angio-1)\r\nt_score_angio\r\n\r\n\r\n[1] 1.646657\r\n\r\nCI_bypass <- c(S_mean_bypass - t_score_bypass * s_sd_bypass / sqrt(s_size_bypass),\r\n        S_mean_bypass + t_score_bypass * s_sd_bypass / sqrt(s_size_bypass))\r\n\r\nCI_bypass\r\n\r\n\r\n[1] 18.29029 19.70971\r\n\r\nCI_angio <- c(s_mean_angio - t_score_angio * s_sd_angio / sqrt(s_size_angio),\r\n       s_mean_angio + t_score_angio * s_sd_angio / sqrt(s_size_angio))\r\n\r\nprint(CI_bypass)\r\n\r\n\r\n[1] 18.29029 19.70971\r\n\r\nprint(CI_angio)\r\n\r\n\r\n[1] 17.49078 18.50922\r\n\r\n19.70971-18.29029\r\n\r\n\r\n[1] 1.41942\r\n\r\n18.50922-17.49078\r\n\r\n\r\n[1] 1.01844\r\n\r\nI followed the instructions to “Calculate Confidence Interval Manually” from the Tutorial to get to the following conclusions:\r\nThe 90% Confidence Interval for Bypass Surgery Wait Time Mean is 18.29029 to 19.70971 days\r\nThe 90% Confidence Interval for Angiography Surgery Wait Time Mean is 17.49078 to 18.50922 days\r\nIs the confidence interval narrower for angiography or bypass surgery?\r\nThe confidence interval is more narrow for angiography.\r\nQUESTION 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\n\r\n\r\nconfidence_level <- 0.95\r\nx <- 567\r\ns_size2 <- 1031\r\np <- x/s_size2\r\np\r\n\r\n\r\n[1] 0.5499515\r\n\r\n?prop.test\r\nprop.test(x,s_size2)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  x out of s_size2, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nBased on the sample, approximately 55% of adult Americans believe that college education is essential for success. p=0.55\r\nThe 95% confidence interval is 0.5189682 to 0.5805580.\r\nWe are 95% confident that between 51.9% and 58.1% of adult Americans believe that college education is essential for success.\r\nQUESTION 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range.\r\nAssuming the significance level to be 5%, what should be the size of the sample?\r\n\r\n\r\n#n=?\r\n\r\nz <- 1.96\r\nM <-5\r\nsd <- 170/4\r\nsd\r\n\r\n\r\n[1] 42.5\r\n\r\n(sd)*(sd)*(z/M)*(z/M)\r\n\r\n\r\n[1] 277.5556\r\n\r\nI found this formula in section 5.4 of the Agresti test book.\r\nSample size should be at least 278 books.\r\nQUESTION 4 (Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nH0= Mean=500 HA= Mean does not equal 500\r\n\r\n\r\ns_mean4 <- 500\r\ns_mean_alt <- 410\r\ns <- 90\r\nn4 <- 9\r\ndf4<- n4-1\r\ndf4\r\n\r\n\r\n[1] 8\r\n\r\nt.score4 <- (s_mean_alt - s_mean4)/(s*(sqrt(9)))\r\nt.score4\r\n\r\n\r\n[1] -0.3333333\r\n\r\np.value <- 2*pt(-abs(t.score4),df4)\r\np.value\r\n\r\n\r\n[1] 0.747451\r\n\r\ngreater_p.value <- pt(t.score4, df4, lower= FALSE)\r\ngreater_p.value\r\n\r\n\r\n[1] 0.6262745\r\n\r\nless_p.value <- pt(t.score4, df4, lower= TRUE)\r\nless_p.value\r\n\r\n\r\n[1] 0.3737255\r\n\r\ngreater_p.value+ less_p.value\r\n\r\n\r\n[1] 1\r\n\r\nI started by calculating T Score = (sample mean - population mean)/ (Standard Deviation/ Square Root of Sample Size). The T Score is -0.33333\r\nI got a little lost and used google to help my find the formulat to use the T Score to find the (2 sided) P Value , specifically https://www.cyclismo.org/tutorial/R/pValues.html and followed the example in section 10.2 since it seemed similar, to calculate the p.value.\r\nI multiplied 2 (since 2 sided) by the pt calculation using the T Score and Degrees of Freedom. The result was a p value of 0.7475. This is a large P Value, so this result suggests that we should not reject the original hypothesis that $500 is the mean income.\r\nI then calculated the p value in the case that the hypothesis is that the Mean income is greater than $500, the result was 0.6268. I then calculated the p value in the case that the hypothesis is that the Mean income is less than $500, the result was 0.3737. In both of these situations, there is not enough information to reject the original hypothesis. (However it seems more likely that the mean is actuallyless than $500 than it is more than $500.)\r\nThe sum of both p-values in this calculation is 1.\r\nQUESTION 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nH0 = Mean = 500 Ha = Mean does not equal 500 Sample Size = 1000\r\n\r\n\r\npopulation_mean <- 500\r\nn5 <- 1000\r\njones_mean <- 519.5\r\njones_Se <- 10\r\n\r\n\r\njones_t <- (jones_mean - population_mean)/(jones_Se)\r\njones_t\r\n\r\n\r\n[1] 1.95\r\n\r\n?pt\r\njones_pvalue <-2*pt(jones_t,(n5-1), lower.tail= FALSE)\r\njones_pvalue\r\n\r\n\r\n[1] 0.05145555\r\n\r\nsmith_mean <- 519.7\r\nsmith_se <- 10\r\n\r\nsmith_t <- (smith_mean - population_mean)/smith_se\r\nsmith_t\r\n\r\n\r\n[1] 1.97\r\n\r\nsmith_pvalue <-2*pt(smith_t,(n5-1), lower.tail= FALSE)\r\nsmith_pvalue\r\n\r\n\r\n[1] 0.04911426\r\n\r\nThe formulas above do indicate that t = 1.95 and P-value = 0.051 for Jones and t = 1.97 and P-value = 0.049 for Smith.\r\nTechnically the Jones p-value indicates that the Jones results are statistically significant, however they are very close to being insignificant- that should be noted. The Smith p-value indicates that the Smith results are not statistically significant since they are less than 0.05\r\nQUESTION 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nn6 <- 18\r\n\r\nsummary(gas_taxes)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  18.49   35.95   41.48   40.86   47.95   54.41 \r\n\r\ngas_taxes_mean <- mean(gas_taxes)\r\ngas_taxes_mean\r\n\r\n\r\n[1] 40.86278\r\n\r\ngas_taxes_sd <- sd(gas_taxes)\r\ngas_taxes_sd\r\n\r\n\r\n[1] 9.308317\r\n\r\nconfidence_level6 <- 0.95\r\n\r\ntail_area6 <- (1 - confidence_level6)/2\r\ntail_area6\r\n\r\n\r\n[1] 0.025\r\n\r\nt_score6 <- qt(p = 1-tail_area, df = n6-1)\r\nt_score6\r\n\r\n\r\n[1] 1.739607\r\n\r\nCI <- c(gas_taxes_mean - t_score6 * gas_taxes_sd / sqrt(n6),\r\n        gas_taxes_mean + t_score6 * gas_taxes_sd / sqrt (n6))\r\n\r\nCI\r\n\r\n\r\n[1] 37.04610 44.67946\r\n\r\nI’m assuming this data is from 2005, though that’s not incredibly obvious. I’m assuming the sample of gas prices is the sum of local, state and federal.\r\nThe mean gas tax from the sample group is 40.86 cents.\r\nI again used the formula from our tutorial for calculating Confidence Interval Manually.\r\nI calculated the Confidence Interval of 95% based on the sample data provided. The CI is between 36.23 and 45.49\r\nSo I am 95% confident that the average gas tax per gallon in the US for the given time period was between 36.23 cents and 45.49 cents.\r\nBeing conservative, I don’t think it’s correct to say that I am 95% confident that the mean for all of the US gas taxes is less than 45 cents.\r\nR Markdown\r\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\r\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\r\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:56:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscommegangeorgesdacss603-hw1/",
    "title": "DACSS 603: Homework 1",
    "description": "Homework # 1 questions and answers for DACSS 603: Introduction to Quantitative Analysis",
    "author": [
      {
        "name": "Megan Georges",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\nQuestion 1:\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\n\r\n\r\nprocedure <- c('Bypass', 'Angiography')\r\nsamplesize <- c(539, 847)\r\nmeanwait <- c(19, 18)\r\nstandev <- c(10, 9)\r\n\r\nsurgdata <- data.frame(procedure, samplesize, meanwait, standev)\r\n\r\nkable(surgdata, col.names = c(\"Surgical Procedure\", \"Sample Size\", \"Mean Wait Time\", \"Standard Deviation\"), \r\n      align = c('c', 'c', 'c', 'c')) %>%\r\n    kable_styling(fixed_thead = TRUE)%>%\r\n  scroll_box(width = \"100%\", height = \"100%\")\r\n\r\n\r\n\r\n\r\nSurgical Procedure\r\n\r\n\r\nSample Size\r\n\r\n\r\nMean Wait Time\r\n\r\n\r\nStandard Deviation\r\n\r\n\r\nBypass\r\n\r\n\r\n539\r\n\r\n\r\n19\r\n\r\n\r\n10\r\n\r\n\r\nAngiography\r\n\r\n\r\n847\r\n\r\n\r\n18\r\n\r\n\r\n9\r\n\r\n\r\n\r\nAnswer:\r\nWe have the sample size, mean, and standard deviation and can assume the sample is representative of the Ontario population. We do not know the population mean or standard deviation. We will use the t-distribution to produce an interval estimate for the true mean wait times of the two procedures. According to the text (SMSS, section 5.6), “confidence intervals using the t-distribution apply with any n but assume a normal population distribution.”.\r\nUsing formula to calculate confidence intervals:\r\nI will use qt() to calculate the t-score since we know the sample distribution\r\nI will set p to .05 which accounts for .05 right tail and .05 left tail calculations to achieve the 90% confidence interval\r\ndf=n-1 for t-score\r\n\\(\\bar{y}\\) ± t(se)\r\n\r\n\r\n# Bypass procedure\r\nybarB <- 19\r\ntB <- qt(.05, df=539-1)\r\nseB <- 10/sqrt(539)\r\n\r\nybarB + tB*seB\r\n\r\n\r\n[1] 18.29029\r\n\r\nybarB - tB*seB\r\n\r\n\r\n[1] 19.70971\r\n\r\n# Angiography procedure\r\nybarA <- 18\r\ntA <- qt(.05, df=847-1)\r\nseA <- 9/sqrt(847)\r\n\r\nybarA + tA*seA\r\n\r\n\r\n[1] 17.49078\r\n\r\nybarA - tA*seA\r\n\r\n\r\n[1] 18.50922\r\n\r\nReporting the confidence intervals:\r\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 minutes.\r\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 minutes.\r\nWhich confidence interval is narrower?\r\n\r\n\r\n# Bypass confidence interval difference\r\n(ybarB - tB*seB)-(ybarB + tB*seB)\r\n\r\n\r\n[1] 1.419421\r\n\r\n# Angiography confidence interval difference\r\n(ybarA - tA*seA)-(ybarA + tA*seA)\r\n\r\n\r\n[1] 1.018436\r\n\r\nThe confidence interval for the angiography procedure is narrower than the confidence interval for the bypass procedure.\r\nQuestion 2:\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\nAnswer:\r\nSince the data is that of proportions, we will use prop.test() to calculate p and the 95% confidence interval.\r\n\r\n\r\nprop.test(567, 1031, conf.level = .95)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  567 out of 1031, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515.\r\nWe can be 95% confident that the population proportion who believe that a college education is essential for success is between 0.5189682 and 0.5805580.\r\nQuestion 3:\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer:\r\nThe formula used to determine the sample size for estimating mean is n=\\(σ^{2}\\) (\\(\\frac{z}{M})^{2}\\).\r\nThe financial aid office estimates the population standard deviation to be about a quarter or the range, which is \\(\\frac{(200-30)}{4}\\).\r\nThe office wants the confidence interval to have a length of 10 dollars or less. Since confidence interval = point estimate ± margin of error, the margin of error in this case will be \\(\\frac{10}{2}\\).\r\nWith the significance level set at 5%, z=1.96.\r\n\r\n\r\n# Computing sample size\r\nstdevBooks <- (200-30)/4\r\nmargerrorBooks <- (10/2)\r\nzBooks <- 1.96\r\n\r\nstdevBooks^2 * (zBooks/margerrorBooks)^2\r\n\r\n\r\n[1] 277.5556\r\n\r\nTo achieve an estimate of the mean cost of books with the range of a 95% confidence interval equal to or less than $10, the sample size should be at least 278.\r\nQuestion 4:\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nReport the P-value for Ha : μ < 500. Interpret.\r\nReport and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nAnswer:\r\na.\r\nTo test whether the mean income of female employees differs from $500, we will perform a one-sample two-sided significance test for a mean (which uses t-statistic).\r\nWe assume that the sample is random and that the population has a normal distribution.\r\nNull hypothesis: \\(H_{0}\\): μ = 500\r\nAlternative hypothesis: \\(H_{a}\\): μ ≠ 500\r\nThe test statistic is t=\\(\\frac{ȳ-μ_{0}}{se}\\), where se=\\(\\frac{s}{\\sqrt{n}}\\)\r\nWe will reject the null hypothesis at a p-value less than or equal to α=.05\r\n\r\n\r\n# Calculate t-statistic:\r\n(410-500)/(90/sqrt(9))\r\n\r\n\r\n[1] -3\r\n\r\n# Calculate p-value\r\npt(-3, 8)*2 \r\n\r\n\r\n[1] 0.01707168\r\n\r\n# Multiply by 2 to account for two-tails\r\n\r\n\r\n\r\nThe test statistic is t=-3 and the p-value is P=0.01707168. With an α-level of .05, the p-value is substantially less than .05, thus we will reject the null hypothesis. There is strong evidence that the mean income of female employees is not equal to $500.\r\nb.\r\n\r\n\r\n# Calculate p-value for Ha: μ < 500\r\npt(-3, 8, lower.tail = TRUE)\r\n\r\n\r\n[1] 0.008535841\r\n\r\nThe p-value for \\(H_{a}\\): μ < 500 is P=0.008535841. With an α-level of .05, the p-value is substantially less than .05, thus we will reject the null hypothesis. There is evidence that the mean income of female employees is less than $500.\r\nc.\r\n\r\n\r\n# Calculate p-value for Ha: μ > 500\r\npt(-3, 8, lower.tail = FALSE)\r\n\r\n\r\n[1] 0.9914642\r\n\r\nThe p-value for \\(H_{a}\\): μ > 500 is P=0.9914642. With an α-level of .05, we fail to reject the null hypothesis. It is highly unlikely that the mean income of female employees is greater than $500.\r\nQuestion 5:\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nUsing α = 0.05, for each study indicate whether the result is ‘statistically significant.’\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer:\r\na.\r\n\r\n\r\n# Jones t=1.95, P=.051\r\nJonesT <- (519.5-500)/10\r\nJonesT\r\n\r\n\r\n[1] 1.95\r\n\r\nJonesP <- pt(1.95, 999, lower.tail = FALSE)*2\r\nJonesP\r\n\r\n\r\n[1] 0.05145555\r\n\r\n\r\n\r\n# Smith t=1.97, P=.049\r\nSmithT <- (519.7-500)/10\r\nSmithT\r\n\r\n\r\n[1] 1.97\r\n\r\nSmithP <- pt(1.97, 999, lower.tail = FALSE)*2\r\nSmithP\r\n\r\n\r\n[1] 0.04911426\r\n\r\nb.\r\nWith an α-level of .05, the p-values that both Jones (P=.051) and Smith (P=.049) found are very close to equivalent. Although Jones’ P-value is slightly greater than α=.05 and Smith’s P-value is slightly less than α=.05, the proximity of the results should yield the same conclusion. Both P-values provide moderate evidence to reject the null hypothesis and indicate that the mean is not equal to 500. If we were to technically interpret the P-values, then Jones’ test would fail to reject the null hypothesis, and Smith’s test would reject the null hypothesis.\r\nc.\r\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. For example, a P-value of .009 for a significance level of .05 provides much stronger evidence to reject the null than a P-value of .045, however both values allow for rejection of the null at the significance level .05. In the Jones/Smith example, reporting the results only as “P ≤ 0.05” versus “P > 0.05” will lead to different conclusions about very similar results (rejecting versus failing to reject the null).\r\nQuestion 6:\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\n\r\n\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer:\r\n\r\n\r\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 10.238, df = 17, p-value = 1.095e-08\r\nalternative hypothesis: true mean is not equal to 18.4\r\n95 percent confidence interval:\r\n 36.23386 45.49169\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents (up to 45.49169).\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomtpaske870606/",
    "title": "Homework 1",
    "description": "Q1-Q6",
    "author": [
      {
        "name": "Tyler Paske",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nAnswer\r\nTo construct the 90% confidence interval, we first need one piece of missing information, the “Z” value. A 90% confidence interval corresponds to a z value of 1.645 (within 2 standard deviations of the mean by convention). With all the needed pieces of the formula we can now compute the formula in R as shown with the code provided.\r\n                                              Bypass: [18.29,19.71] \r\nDifference = 1.42\r\nR-Code:\r\n\r\nqnorm(.95)\r\n\r\nHow I got .95 was by knowing the corresponding z value. A 90% confidence interval provides tails of .95, .10/2 = .5+.90 = .95 = z value of 1.645\r\n1.644854\r\ncenter <- 19\r\nstddev <-10\r\nn <- 539\r\nerror <- qnorm(0.95)*stddev/sqrt(n)\r\nerror 0.7084886\r\nlower_bound <- center - error\r\nlower_bound 18.29151\r\nupper_bound <- center + error\r\nupper_bound 19.70849\r\n                                           Angiography: [17.49,18.51]\r\nDifference = 1.02\r\nR-Code:\r\ncenter <-18\r\nstddev <-9\r\nn <- 847\r\nerror <- qnorm(0.95)*stddev/sqrt(n)\r\nerror 0.5086606\r\nlower_bound <- center - error\r\nlower_bound 17.49134\r\nupper_bound <- center + error\r\nupper_bound 18.50866\r\n• As we can see the confidence interval is narrower for Angiography. Bypass surgy being .4 more in difference than Angiography makes the confidence interval narrower for Angiography.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\nAnswer\r\nUsing division (567/1031), I found that the point estimate “p” is 0.54995 of American Adults believed that college is essential for success while those that don’t are 45% of the population. To construct and interpret a 95% confidence interval for “p” requires further research using the following elements.\r\nSample proportion; 0.54995 = p\r\nThe sample size; 567 = x\r\nTotal Adults; 1031 = n\r\n\r\nprop.test(x=567, n=1031, p=0.54995, correct=FALSE)\r\n\r\n1-sample proportions test without continuity correction\r\ndata: 567 out of 1031,null probability 0.54995 X-squared = 9.415e-09, df = 1, p-value = 0.9999 alternative hypothesis: true p is not equal to 0.54995 95 percent confidence interval: [0.52, 0.58] sample estimates: p = 0.5499515\r\n                              Lower <- 0.52 Upper <-  0.58\r\nThe confidence interval at 95% has an interval of [.52, .58] which contains the difference between proportions. To further interpret, there are between 51-59% Americans who believe that a college education is essential for success when estimating the proportion.\r\n• The difference in this question from question 1 is that we didn’t have the standard deviation. Not having the standard deviation, we use the Confidence Interval for a Proportion Formula.\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within 5 dollars of the true population mean (i.e. they want the confidence interval to have a length of 10 dollars or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between 30 dollars and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer\r\nTo find the size of the sample you need the following.\r\nN = population size = 170\r\nz = z-score = 1.96\r\ne = margin of error = 95% or 10 as they want a confidence interval of 10\r\np = standard of deviation = 170/4 = 42.5\r\nSample mean = 170*.5 = 85\r\nNext we use the following standard formula to solve for the Sample Size.\r\n                             Sample Size = {(z*sd)/5}2 = 277.5556 \r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals 500 dollars per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\n##PART A Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\n##Answer Assumptions\r\nType of data: Quantitative as the question represents amounts rather than groupings\r\nRandomization: As the question stands our random sample stands to hold a random sample of 9 female employees.\r\nPopulation distribution: Normal distribution.\r\nSample size “n”: 9 Population Mean: 500 Standard Deviation “s”: 90 Sample mean “y”:410\r\nHypothesis The hypothesis is as follows.\r\nHo : μ=$500 H1: μ≠$500\r\nHo is the null hypothesis and represents the population mean which is $500 H1 is the alternative hypothesis and represents the population proportion not equal to $500\r\n                                    Test Statistic & P – Value\r\n• To find the answer I turn to the one sample t-Test.\r\nPopulation Mean = 500 Sample Mean = 410 Sample Size = 9 Sample Standard Deviation = 90\r\n• Assuming the data is normally distributed and the significance of the test is .05\r\nT = (Xbar – μ) / sd / sqrt(n)\r\n(410-500) / 90 / sqrt(9) = -3\r\nThe probability the t value being -3 is less than 1 percent; .85% and since the alternative hypothesis (isn’t equal), we’ll need a two-tailed probability.\r\nq = t-score = -3\r\ndf = degrees of freedom (sample size “n”-1) = 9-1 = 8\r\nLeft-tailed test in R:\r\n                        p_value=pt(q=-3, df=8, lower.tail = TRUE)\r\n0.00853584\r\nThe P-value is as follows due to that we’re calculating a two tailed test,\r\n                                     2 * 0.0085 = 0.0171\r\nWith the P-Value of 0.0171 (lower than the significance level of .05) we can say that there is significance at p <.05 between women and senior level workers making the seniors pay “rejected”. There is enough evidence to say that the women aren’t paid as much.\r\nQuestion 4 B\r\nReport the P-value for Ha : μ < 500. Interpret.\r\nAnswer\r\nRemembering that this is calculated assuming the null hypothesis is true, we look further to the left sided tail test. The P-Value itself validates if a null hypothesis is true or not. The p value is a kind of “credibility rating” of a null hypothesis in light of evidence.\r\n• To find the p-value we must first get the following values\r\nq = t-score = -3 df = degrees of freedom (sample size “n”-1) = 9-1 = 8 Left-tailed test in R:\r\n                              p_value=pt(q=-3, df=8, lower.tail = TRUE)\r\n\r\np_value P-Value: 0.008535841\r\n\r\nIn this case, the p-value is significant. This shows that we can reject the null hypothesis or in other words reject the hypothesis that was claimed.\r\nQuestion 4 C\r\nReport and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\n##Answer Right-tailed test in R:\r\n                            p_value=pt(q=-3,df=8, lower.tail=FALSE)\r\n\r\np_value P-Value: 0.9914642\r\n\r\n                                  0.008535841 + 0.9914642 = 1\r\n    \r\nHaving the P-values for the two possible one-sided tests must sum to 1 I know that I came to right conclusion.\r\nQuestion 5A\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nAnswer\r\nTo first show the t test I used the test statistic formula: t = ȳ - μ / se\r\nJones <- (519.5 - 500) / 10\r\n                                   T-Test for Jones = 1.95\r\nSmith <- (519.7 – 500)/10\r\n                                   T-Test for Smith = 1.97 \r\nNow that we have the T-Tests for both Jones and smith we can solve for the P-Value. Remembering the formula used in question 4 we need the degrees of freedom and p = standard error. Degrees of freedom being n – 1 we find that df = 999.\r\np_value=pt(q=1.95, df=999, lower.tail = FALSE) Multiplied by 2 = .051\r\n                                    Jones P Value = 0.051\r\nPerforming the same operation for Smith we find the following, p_value=pt(q=1.97, df=999, lower.tail = FALSE) Multiplid by 2 = 0.049\r\n                                  Smiths P Value = 0.049\r\nQuestion 5B\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nAnswer\r\nWith a significance level α = 0.05, we can not reject the null hypothesis for Jones as his P Value is .051. The P Value being larger for Jones than the significance level also shows that the results are not statistically significant. Smith on the other hand is the opposite where his P-Value is less than the significance level and can reject the null hypothesis and are statistically significant.\r\nQuestion 5C\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer\r\nThe misleading aspects of reporting the results of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value is that they are based on arbitrary evaluations. Being presented with statistics that are based on random choice only shows one aspects of a situation where there can be an infinite number of outcomes. As an example, and to conclude my explanation we can’t have a binary justification as to reject or not reject a hypothesis as statistics can take a quick snapshot of a scenario in time but does not in any way describe the entire story. However, should we decide to look at a situation, snapshot or circumstance in time, the p-value does provide the best insight for the outcomes significance.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer\r\n95% Confidence interval has a 5% significance level and 1.960 Z Value\r\n• We use the Z value amongst the following values to solve for the confidence interval\r\nConfidence interval Formula: X (+or–) Z s/sqrt(n)\r\n• X is the mean = 40.86278 • Z is the chosen Z-value from the table above = 1.960 • s is the standard deviation = 9.308317 • n is the number of observations = 18\r\nR CODE used\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\n\r\nmean(gas_taxes) [1] 40.86278\r\n\r\n• The last thing that we need in order to find the confidence interval is the standard deviation.\r\n\r\nsd(gas_taxes) [1] 9.308317\r\n\r\nThe value we derived from the formula for the confidence interval is the margin or error. [36.6, 45.2] We can conclude that yes, with 95% confidence the population mean is between 36.6 and 45.2, based on only 18 samples.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomclairebattagliahomework-1-603/",
    "title": "Homework 1",
    "description": "Statistical Inference",
    "author": [
      {
        "name": "Claire Battaglia",
        "url": "https://rpubs.com/clairebattaglia"
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\n\nContents\nQuestion 1\nQuestion\nAnswer\nSolution\n\nQuestion 2\nQuestion\nAnswer\nSolution\n\nQuestion 3\nQuestion\nAnswer\nSolution\n\nQuestion 4\nQuestion\nAnswer\nSolution\n\nQuestion 5\nQuestion\nSolution\n\nQuestion 6\nQuestion\nAnswer\nSolution\n\n\nQuestion 1\nQuestion\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nSurgical Procedure\nSample Size (\\(n\\))\nMean Wait Time (\\(\\overline{x}\\))\nStandard Deviation (\\(s\\))\nbypass\n539\n19\n10\nangiography\n847\n18\n9\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nAnswer\nFor those undergoing bypass surgery the estimated mean wait time is between 18.29 and 19.71 days (19 \\(\\pm\\) .71 days).\nFor those undergoing angiography surgery the estimated mean wait time is between 17.49 and 18.51 days (18 \\(\\pm\\) .51 days).\nThe confidence interval is narrower for the population of patients undergoing angiography surgery.\nSolution\nThere are two populations: patients who have undergone bypass surgery and those who have undergone angiography surgery. I have a sample from each population and the mean and standard deviation of each sample.\nWhile I don’t know the sampling method(s), I am told each sample is representative of its corresponding population and I can see that each is large (\\(n>30\\)). Because the variable wait time is measured in days, I am working with discrete interval data, which I need to know in order to construct the confidence interval correctly.\nA confidence interval is essentially a “best guess” of a population parameter (called the point estimate), plus or minus a margin of error. The confidence level, which is frequently expressed as a percentage, is the proportion of trials in which the confidence interval would contain the true population mean. As a statement of proportion in the long run, it is only meaningful within the frequentist approach to statistics.\nThe confidence interval (\\(CI\\)) is a \\[{point\\;estimate}\\pm{margin\\;of\\;error}\\] where the \\[{margin\\;of\\;error} = t*{standard\\;error}\\] and the \\[{standard\\;error} = \\frac{\\sigma}{\\sqrt{n}}\\] Because I don’t know the true population mean (\\(\\mu\\)), I’ll use the sample mean (\\(\\overline{x}\\)) as my estimator (\\(\\hat{\\mu}\\)) of the population mean. Similarly, because I don’t know the true population standard deviation (\\(\\sigma\\)), I’ll use the sample standard deviation (\\(s\\)) as my estimator (\\(\\hat{\\sigma}\\)) of the population standard deviation. Because I don’t know the population standard deviation and am instead using the sample standard deviation as an estimate, I’m going to use the \\(t\\) distribution, which means I’ll be using a \\(t\\)-score instead of a \\(z\\)-score. It’s worth noting, however, that both samples are large enough that the \\(t\\) distribution will be essentially identical to the standard normal distribution.\nThis finally gives me the formula \\[CI=\\overline{x}\\;\\pm\\;\\left(t*\\frac{s}{\\sqrt{n}}\\right)\\] To calculate the \\(t\\)-score for a 90% confidence interval, I’m going to use R. Because the confidence interval is 90% (.9), the error probability (\\(\\alpha\\)) is .1. \\(\\alpha/2=5\\)%, or .05 on the right tail and .05 on the left tail.\n\n\nShow code\n\n# calculate quantile for t distribution\ntScoreBypass <- qt(p = .95, df = 538)\n\n# view\ntScoreBypass\n\n\n[1] 1.647691\n\nSo the final equation is \\[CI_{90}={19}\\;\\pm\\;\\left(1.647691*\\frac{10}{\\sqrt{539}}\\right)\\]\n\n\nShow code\n\n# calculate lower bound\nlowerBypass <- 19 - (1.647691*(10/sqrt(539)))\n\n# calculate upper bound\nupperBypass <- 19 + (1.647691*(10/sqrt(539)))\n\n# calculate range\nrangeBypass <- upperBypass - lowerBypass\n\n\n\nThe lower bound is 18.29029 and upper bound is 19.70971, making the range 1.41942.\nI’ll do the same calculations for my second population (patients who have undergone angiography surgery).\n\n\nShow code\n\n# calculate quantile for t distribution\ntScoreAngio <- qt(p = .95, df = 846)\n\n# view\ntScoreAngio\n\n\n[1] 1.646657\n\nSo the final equation is \\[CI_{90}={18}\\;\\pm\\;\\left(1.646657*\\frac{9}{\\sqrt{847}}\\right)\\]\n\n\nShow code\n\n# calculate lower bound\nlowerAngio <- 18 - (1.646657*(9/sqrt(847)))\n\n# calculate upper bound\nupperAngio <- 18 + (1.646657*(9/sqrt(847)))\n\n# calculate range\nrangeAngio <- upperAngio - lowerAngio\n\n\n\nThe lower bound is 17.49078 and the upper bound is 18.50922, making the range 1.01844.\nThis means that if I were to sample repeatedly from these two populations, 90% of the intervals created with these models would contain the true populations means.\nThe confidence interval is narrower for the angiography population (1.01844 \\(<\\) 1.41942). This is to be expected because the angiography sample size is larger and the standard deviation is smaller.\nQuestion 2\nQuestion\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nAnswer\nThe point estimate is 0.55 and the 95% confidence interval is 0.52 to 0.58. That is, the estimated proportion of adult Americans who believe that college education is essential for success is 55% \\(\\pm\\) 3%.\nSolution\nThe population is all adult Americans and I have a representative sample (\\(n\\)) of 1031. Of those sampled, 567 believe that a college education is essential for success. This is enough information to calculate the sample proportion (\\(\\hat{\\pi}\\)), which I’ll then use to construct a 95% confidence interval for the true population proportion.\nTo construct the confidence interval, I’ll be using the formula \\[CI=\\hat{\\pi}\\;\\pm\\;z\\sqrt{\\left(\\frac{\\hat{\\pi}\\;(1-\\hat{\\pi})}{n}\\right)}\\] First, I’ll calculate the sample proportion, which will serve as my estimator for the true population proportion.\n\n\nShow code\n\n# calculate sample proportion\nsampProp <- 567/1031\n\n\n\nThus the sample proportion is 0.5499515\nIn a standard normal distribution 95% of all values fall within 1.96 standard deviations of the mean. Because I’m constructing a 95% confidence interval, then, the \\(z\\)-score I’ll be using is 1.96.\nThus \\[CI_{95}=0.5499515\\;\\pm\\;1.96\\sqrt{\\left(\\frac{0.5499515\\;(1-0.5499515)}{1031}\\right)}\\]\n\n\nShow code\n\n# calculate lower bound\nlowerBound <- sampProp - (1.96 * sqrt(((sampProp * (1-sampProp)/1031))))\n\n# calculate upper bound\nupperBound <- sampProp + (1.96 * sqrt(((sampProp * (1-sampProp)/1031))))\n\n\n\nThus the lower bound of the confidence interval is 0.52 and the upper bound is 0.58.\nThis means that using the sample proportion of 0.5499515 as the estimator (\\(\\hat{\\pi}\\)) for the true population proportion and constructing a 95% confidence interval, I can expect that 95% of trials would contain the true population mean within the interval of 0.52 and 0.58 (\\(\\pi\\pm.0304\\)).\nAlternatively, I can use the R function prop.test to calculate everything needed to answer this question. Since I’ve already calculated everything, I’ll simply use it to validate my calculations.\n\n\nShow code\n\n# calculate prop test\nprop.test(567, 1031, p = 0.5499515)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5499515\nX-squared = 0, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\nQuestion 3\nQuestion\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nAnswer\nThe sample size should be 278.\nSolution\nThe formula for determining the sample size for estimating a population mean (\\(\\mu\\)) is \\[n=\\sigma^2\\left(\\frac{z}{M}\\right)^2\\] where \\(n\\) is the sample size (what I’m trying to find), \\(\\sigma\\) is the population standard deviation, \\(z\\) is the \\(z\\)-score for the chosen confidence level, and \\(M\\) is the margin of error.\nWhile I don’t know the true population standard deviation (\\(\\sigma\\)), I am given an estimate, which will suffice. The suspected range is 170 and I am told the population standard deviation is estimated to be about a quarter of that, which is 42.50.\nIf the significance level (\\(\\alpha\\)) is 5%, then the confidence level is 95% (\\(1-{confidence\\;level}=\\alpha\\)). Assuming a normal distribution, the \\(z\\)-score for a 95% confidence level is 1.96.\nThe margin of error is 5.\nThus \\[n=42.5^2\\left(\\frac{1.96}{5}\\right)^2\\]\n\n\nShow code\n\n# calculate sample size\nn <- (42.5^2)*((1.96/5)^2)\n\n\n\nwhich indicates that a sample size of 277.5556 is required to estimate the population mean with a 95% confidence interval.\nBecause the units in the sample are people (i.e. a discrete unit), I’ll round up to the nearest whole number, giving me a total of 278.\n\nBecause I need a minimum of 277.5556, I would round up to the nearest whole number regardless of what the normal rule for rounding would indicate.\nThis means that I would need to sample a minimum of 278 people to estimate the mean cost of textbooks per student per quarter within plus or minus $5. If I repeatedly sample at least this number of people, 95% of the intervals constructed would contain the true population mean.\nQuestion 4\nQuestion\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nReport the P-value for Ha : μ < 500. Interpret.\nReport and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nAnswer\nThere is sufficient evidence to reject the null hypothesis and thereby accept the alternative hypothesis that the mean income for female employees does not equal $500 per week. The test statistic is -3. The \\(P\\)-value is .017.\nFor \\(H_a:\\mu<500\\), the \\(P\\)-value is .009. Because the \\(P\\)-value is less than the most stringent significance level of .01, we can reject the null hypothesis (\\(H_0:\\mu\\ge500\\)) and thereby accept the alternative hypothesis.\nFor \\(H_a:\\mu>500\\), the \\(P\\)-value is .991. Because the \\(P\\)-value is greater than the significance level of .01 (and even the more lenient .05), we cannot reject the null hypothesis (\\(H_0:\\mu\\le500\\)) at this time.\nSolution\nTo solve this problem, I’m going to assume:\nA normal population distribution.\nThe sample of female employees is sufficiently large.\nTo begin, I’ll identify the null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_a\\)).\nThe null hypothesis is that there is no difference between the mean for female employees and the mean for all employees. That is, the mean for female employees is also 500.\n\\[{H_0:}\\;\\mu = 500\\] The alternative hypothesis is that there is a difference. That is, the mean for female employees is not 500. Because I’m interested in any kind of difference (\\(\\mu < 500\\) or \\(\\mu > 500\\)), I’ll be using a two-sided test.\n\\[{H_a:}\\;\\mu\\neq 500\\] Conducting a hypothesis test asks whether what we’ve observed (\\(\\overline{y}=410\\)) would be so unlikely if the null hypothesis (\\(H_0=500\\)) were true that we are obligated to reject it. If we find that what we observed is not that unlikely and could reasonably be explained by sample variability, we will not be able to reject the null hypothesis at this time.\nThe formula for calculating the test statistic is \\[t=\\frac{\\overline{y}-\\mu_0}{se}\\] where \\[se=\\frac{s}{\\sqrt{n}}\\] Thus the standard error is \\[se=\\frac{90}{\\sqrt{9}}\\] and \\[t=\\frac{410-500}{30}\\]\n\n\nShow code\n\n# calculate estimated standard error\nse <- 90/sqrt(9)\n\n# calculate test statistic\ntestStat <- (410-500)/se\n\n\n\n\\(t=\\) -3 and \\(|t|=\\) 3\nNext I’ll calculate the \\(P\\)-value.\n\n\nShow code\n\n# calculate 2-sided P value\npValue <- 2 * (1-pt(q = 3, df = 8))\n\n\n\nThus the \\(P\\)-value \\(=\\) 0.0170717\nThis indicates a 0.0170717 probability that we would observe the sample mean (\\(\\overline{y}\\)) of 410 if the null hypothesis were true. That is, if the mean for female employees (\\(\\mu\\)) were really 500. While this finding isn’t significant at the .01 level, it is significant at the .05 level and I can conclude that if the mean for female employees were 500, I’d be rather unlikely to end up with a sample mean of 410. I feel comfortable, then, in rejecting the null hypothesis and accepting the alternative hypothesis.\nNow I’ll look at the probabilities that the sample mean would be above or below the population mean separately.\n\n\nShow code\n\n# calculate P value for Ha > 500, right-tail\npValueG <- 1-pt(q = 3, df = 8, lower.tail = FALSE)\n\n# calculate P value for Ha < 500, left-tail\npValueL <- 1-pValueG\n\n\n\nFor \\(H_a:\\mu<500\\), \\(P=\\) 0.009. This indicates a 0.009 probability that we would have observed a \\(t\\)-score equal to or lesser than what we what we did in fact observe if the null hypothesis (\\(H_0:\\mu\\ge500\\)) were true. Put more simply, if the null hypothesis were true, it is highly unlikely we would have observed what we observed. We can reject the null hypothesis.\nFor \\(H_a:\\mu>500\\), \\(P=\\) 0.991. This indicates a 0.991 probability that we would have observed a \\(t\\)-score equal to or greater than what we did in fact observe if the null hypothesis (\\(H_0:\\mu\\le500\\)) were true. That is, if the null hypothesis were true, it is highly likely we would have observed what we observed. We can accept the null hypothesis.\nTaken together these lend strong support to the claim that the mean income for female employees is less than the mean for all employees.\nQuestion 5\nQuestion\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nSolution\nThe formula for calculating the test statistic is \\[t=\\frac{\\overline{y}-\\mu_0}{se}\\] Since they both assume the population mean (\\(\\mu\\)) to be 500 (the null hypothesis) and they both got a standard error of 10, the only difference between their tests is the mean of each of their samples (\\(\\overline{y}\\)).\nThus Jones’s test statistic is \\[t=\\frac{{519.5}-{500}}{10}\\] and Smith’s is \\[t=\\frac{{519.7}-{500}}{10}\\]\n\n\nShow code\n\n# calculate test stat Jones\ntStatJ <- (519.5-500)/10\n\n# calculate test stat Smith\ntStatS <- (519.7-500)/10\n\n\n\nFor Jones, \\(t=\\) 1.95\nFor Smith, \\(t=\\) 1.97\nNow I’ll calculate the \\(P\\)-value for each of them. This will tell me the probability of observing the data they actually observed if the null hypothesis is true. Since the alternative hypothesis is non-directional, I’ll calculate the two-sided probability.\n\n\nShow code\n\n# calculate P value Jones\npJones <- 2*pt(q = tStatJ, df = 999, lower.tail=FALSE)\n\n# calculate P value Smith\npSmith <- 2*pt(q = tStatS, df = 999, lower.tail=FALSE)\n\n\n\nFor Jones, \\(P=\\) 0.051\nFor Smith, \\(P=\\) 0.049\nSince the significance level (\\(\\alpha\\)) is .05, Jones is unable to reject the null hypothesis at this time (\\(.051>.05\\)) and his/her results are not statistically significant. Smith, however, is able to reject the null hypothesis (\\(.049<.05\\)) and can claim that his/her results are statistically significant (at the level of .05).\nThis example illustrates the danger of living and dying by whether or not the \\(P\\)-value is statistically significant. For example, if a statistically significant finding is requisite for publishing, then only Smith’s finding would make its way to a larger audience. If the \\(P\\)-value were not included, the reader might wrongly assume that the evidence for rejecting the null hypothesis is strong, when in reality it only nudges us towards that conclusion.\nAlternatively, if both findings were published and the \\(P\\)-values were not included, the reader would see that Jones does not reject the null hypothesis but that Smith does and might wrongly believe that their findings were contradictory. Including the \\(P\\)-values, however, would allow the reader to see that the findings are actually not contradictory.\nIt’s important to remember that the significance level is an arbitrary demarcation. This is ultimately a problem of using a binary framework (reject/fail to reject) in a world in which very few things (if any) are actually binary. Maintaining a larger perspective is paramount—statistics can and should inform our understanding of the world but significance testing is not a substitute for critical thinking.\nQuestion 6\nQuestion\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAnswer\nYes, there is sufficient evidence to reject the null hypothesis and thereby accept the alternative hypothesis that in 2005 the average gas tax in the United States was less than 45.00 cents.\nSolution\nTo answer this question, I’m going to conduct a one-sided significance test. To do so, I’m going to assume a random sample and normal distribution.\nThe null hypothesis is that the average tax per gallon is 45.00 cents. \\[H_0:\\mu=45.00\\] The alternative hypothesis is that the average tax per gallon is less than 45.00 cents. \\[H_a:\\mu<45.00\\] Conducting a hypothesis test asks whether the observed data (the gas taxes for the 18 cities in our sample) would be so unlikely if the null hypothesis were true that we are forced to reject it.\nFirst I’ll load the given sample data and calculate some summary statistics.\n\n\nShow code\n\n# create vector of sample data\ngasTaxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# calculate mean\nsampMean <- mean(gasTaxes)\n\n# calculate sd\nsampSD <- sd(gasTaxes)\n\n\n\nThe sample mean (\\(\\overline{y}\\)) is 40.8627778 and the sample standard deviation (\\(s\\)) is 9.3083168.\nThe formula for calculating the test statistic is \\[t=\\frac{\\overline{y}-\\mu_0}{se}\\] where \\[se=\\frac{s}{\\sqrt{n}}\\] Thus \\[se=\\frac{9.3083168}{\\sqrt{18}}\\] and \\[t=\\frac{40.8627778-45.00}{2.193991}\\]\n\n\nShow code\n\n# calculate standard error\nse <- 9.3083168/sqrt(18)\n\n# calculate t statistic\ntStatGas <- (40.8627778-45.00)/se\n\n\n\nThus the test statistic is -1.8857058.\nFinally, I’ll calculate the \\(P\\)-value.\n\n\nShow code\n\n# calculate P value\npGas <- pt(q = tStatGas, df = 17, lower.tail=TRUE)\n\n\n\nThus the \\(P\\)-value is 0.038\nThis means that if the null hypothesis were true there is a 0.038 probability of observing the data we observed or data more extreme.\nA 95% confidence interval means a significance level of .05 (\\(1-{confidence\\;level}=\\alpha\\)). Given that the \\(P\\)-value is less than the significance level of .05, I can say that, yes, given a 95% confidence interval there is sufficient evidence to reject the null hypothesis and accept the alternative hypothesis that in 2005 the average gas tax in the United States was less than 45.00 cents.\nAlternatively, I can use the R function t.test to calculate everything needed to answer this question. Since I’ve already calculated everything, I’ll simply use it to validate my calculations.\n\n\nShow code\n\nt.test(gasTaxes, mu = 45.00, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  gasTaxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\nAnother approach to hypothesis testing is constructing the appropriate confidence interval and determining whether the mean of the null hypothesis is contained within that interval or not.\nThe t.test function has returned the confidence interval and since I’m looking at it only to confirm my decision to reject the null hypothesis, I won’t calculate it manually. Because the alternative hypothesis is directional, the confidence interval is likewise one-sided. In this case it is [-\\(\\infty\\), 44.67946]. This interval does not contain the mean of the null hypothesis, which confirms my decision to reject the null hypothesis.\n\nIt’s worth noting that the upper bound of the interval is close to 45.00. This raises the question of whether the difference has any practical significance. I don’t follow conversations about gas tax policy so I can’t comment on how meaningful it is to say that the average tax is less than 45.00 cents if it could reasonably be 44.68 cents.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomjflattery870090/",
    "title": "Homework 1",
    "description": "My HW 1 for 603",
    "author": [
      {
        "name": "Justin Flattery",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nQuestion 1\nFor each procedure we will construct a confidence interval around our estimate of the actual mean by fitting the sample about a t distribution. We can do this since we have the sample mean, sample size (thus degrees of freedom) and sample standard deviation\nBYPASS Confidence Interval\nFirst we will find the T score associated with the degrees of freedom and level of confidence\n\n\nqt(p = .9, df = 538)\n\n\n[1] 1.283127\n\nWe will now plug this t score into the formula to find the standard error,\n\n\n(1.283127 * (10/(sqrt(539))))\n\n\n[1] 0.5526819\n\nWe will now add/subtract the sample mean from the standard error to produce our 90% confidence interval\n\n\n19 + (1.283127 * (10/(sqrt(539))))\n\n\n[1] 19.55268\n\n\n\n19 - (1.283127 * (10/(sqrt(539))))\n\n\n[1] 18.44732\n\nTherefore the 90% confidence interval for bypass surgery is [18.45,19.55]\nAngiography Procedure\nAgain we will find the T score associated with the degrees of freedom and level of confidence)\n\n\nqt(p = .9, df = 846)\n\n\n[1] 1.282553\n\nWe will now plug this t score into the formula to find the standard error\n\n\n(1.282553 * (9/(sqrt(847))))\n\n\n[1] 0.3966214\n\nWe will now add/subtract the sample mean from the standard error to produce our 90% confidence interval\n\n\n18 + (1.282553 * (9/(sqrt(847))))\n\n\n[1] 18.39662\n\n18 - (1.282553 * (9/(sqrt(847))))\n\n\n[1] 17.60338\n\nTherefore the 90% confidence interval for angiography surgery is [17.6,18.40]\nWhich is narrower?\nAngiography - 17.60 - 18.39 Bypass - 18.45 - 19.55\nAngiography surgery because it has a smaller range to its interval (0.79 vs 1.1)\nQuestion 2 n=1031 567=success 95% confidence\n\n\n567/1031\n\n\n[1] 0.5499515\n\nPoint estimate: 0.55 - 55% of americans believe\nI will use r’s prop test formula to find the 95% confidence interval for p, using the knowledge of 567 believed in a 1031 sample.\n\n\nprop.test(567,1031,conf.level = 0.95)$conf.int\n\n\n[1] 0.5189682 0.5805580\nattr(,\"conf.level\")\n[1] 0.95\n\nQuestion 3 First I will find the population standard deviation based on information about range\n\n\nsigma = (200-30)/4\n\n\n\nNext, given the information above, assuming we have the population standard deviation, we can assume a normal distribution of the sample. So I will find the z value based on a significance level of 5%. Since this will be a two tailed sample, this will leave 2.5% on each side, so I will find for (0.975)\n\n\nzstar = qnorm(.975)\n#I will input 42.5 as my sigma (population sd variable)\nsigma = 42.5\n#E is my desired width/range of my confidence interval\nE = 10\n\n\n\nI will now input these variables into the appropriate formula - z^2 * sigma^2 / (E^2) to find the necessary sample size\n\n\n(zstar*zstar) * (sigma*sigma)/ (E*E)\n\n\n[1] 69.38635\n\nSince the value is 69.38, we will round up to the next whole number, 70 in order to ensure we have a large enough sample to estimate the mean\nQuestion 4 In order to evaluate if the mean income is different for female employees, we must set up a hypothesis test. In order to evaluate whether it is NOT $500 we will set a null hypothesis that the mean income = $500 per week. The alternative hypothesis will be the opposite of this, that it is different from $500 a week so:\nH0: mu= $500 Ha mu not = 500\nTest assumes data obtained through randomization and sample is representative of larger population.\nFirst we will find the value of the test statistic (tscore), (how far the sample distance is from the mean in a t distribution)\n\n\n(410 - 500)/((90/3))\n\n\n[1] -3\n\nWe will next use r to find the value of p (the probability of observing this occurrence assuming the null hypothesis)for a two tailed test, inputting our test statistic of -3 and df (9-1)\n\n\n2*pt(q=-3, df=8, log = FALSE)\n\n\n[1] 0.01707168\n\nThis produces a p value of 0.017. Since this is a very small number ( less than 0.05), it indicates a very small probability of occurrence and that we can reject the null hypothesis that the population mean is 500 at the 5% significance level.\nB For part b) We will set up a different hypothesis: H0: mu> $500 Ha mu < $500\nWe will plug in the same values (t statistic, df) but use a one tailed test:\n\n\npt(q=-3, df=8,lower.tail = TRUE)\n\n\n[1] 0.008535841\n\nThis produces a p value of 0.008. Since this is a very small number ( less than 0.05), it indicates a very small probability of occurrence and that we can reject the null hypothesis that the population mean is greater than 500 at the 5% significance level.\nC\nFor part c) We will set up a different hypothesis:\nH0: mu< $500 Ha mu > $500\n\n\npt(q=-3, df=8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\nThis produces a p value of 0.99. Since this is a very large number/close to one and greater than significance level of 0.05, it indicates a very high probability of occurrence and therefore we cannot reject the null hypothesis that the population mean is less than 500 at the 5% significance level.\nQuestion 5 H0: μ = 500 Ha : μ = 500, each with n = 1000. Jones has ȳ = 519.5, with se = 10.0. Smith has ȳ = 519.7, with se = 10.0.\nFirst I will find the t statistic for Jones using formula used in previous questions\n\n\n(519.5 - 500)/(10)\n\n\n[1] 1.95\n\nJones calculating p value\n\n\n2*pt(q=1.95, df=499, lower.tail=FALSE)\n\n\n[1] 0.05173574\n\nNow I will find the t statistic for Smith using formula used in previous questions\n\n\n(519.7 - 500)/(10)\n\n\n[1] 1.97\n\nSmithh calcultaing p value\n\n\n2*pt(q=1.97, df=499, lower.tail=FALSE)\n\n\n[1] 0.04939092\n\nThe result for Smith is statistically significant at the 5% level (since the value of p is 0.049 <.05), the result for Jones it is not statistically significant at 5% level (p = 0.051>.05)\nWithout reporting the actual p value , data can be manipulated to state the sample is significant for example, for Jones, it would be misleading to round down 0.051 to 0.05 and report that p is less than or equal to 0.05, and thus statistically significant. By reporting instead as the reality that P>0.05 (with p included) is much more transparent. Additionally, the p value should be reported along with the significance level when making a statement around rejecting the null hypothesis. For example, we could reject the null hypothesis at the 10% level for Smith - but this would be manipulating the data from our original planned significance level.\nQuestion 6\nFirst I will load in the dataset of gas_taxes based on the sample\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\nNext, I used r to calculate a t test of the data set under a 95% confidence level\n\n\nt.test(gas_taxes, conf.level = 0.95)$conf.int\n\n\n[1] 36.23386 45.49169\nattr(,\"conf.level\")\n[1] 0.95\n\nConclusion - No - the upper bound of a 95% confidence interval is over 45 cents, therefore we can not definitively say that the average tax per gallon was less than 45C.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomkbec864954/",
    "title": "Homework 1 DACSS 603",
    "description": "Descriptive Statistics, Probability, Statistical Inference, and Comparing Two Means",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nSurgical Procedure - Representative Sample\r\nSurgical Procedure\r\nSample Size\r\nMean Wait Time\r\nStandard Deviation\r\nBypass\r\n539\r\n19\r\n10\r\nAngiography\r\n847\r\n18\r\n9\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\r\nIs the confidence interval narrower for angiography or bypass surgery?\r\nAnswer 1\r\nI calculated the answer by first calculating the standard error for each procedure given the mean, standard deviation, and sample size for each. I do so using 0.95 for the qnorm function so that I can determine the 5% confidence level for both the right and left side of the normal distribution, since the sample is larger than n=30. By calculating the 5% margin for each side of the distribution, this gives me the effective 90% confidence interval overall.\r\n\r\n\r\n#Calculate the actual mean wait time for the bypass:\r\n\r\nxbar1 <- 19 #sample mean\r\nsd1a <- 10 #sample standard deviation\r\nn1a <- 539 #sample size\r\n\r\nerror1a <- qnorm(0.95)*sd1a/sqrt(n1a)\r\nerror1a\r\n\r\n\r\n[1] 0.7084886\r\n\r\nlower1a <- xbar1-error1a\r\nupper1a <- xbar1+error1a\r\n\r\nlower1a\r\n\r\n\r\n[1] 18.29151\r\n\r\nupper1a\r\n\r\n\r\n[1] 19.70849\r\n\r\n\r\n\r\n#Calculate the actual mean wait time for the angiography:\r\n\r\nxbar1b <- 18 #sample mean\r\nsd1b <- 9 #sample standard deviation\r\nn1b <- 847 #sample size\r\n\r\nerror1b <- qnorm(0.95)*sd1b/sqrt(n1b)\r\nerror1b\r\n\r\n\r\n[1] 0.5086606\r\n\r\nlower1b <- xbar1b-error1b\r\nupper1b <- xbar1b+error1b\r\n\r\nlower1b\r\n\r\n\r\n[1] 17.49134\r\n\r\nupper1b\r\n\r\n\r\n[1] 18.50866\r\n\r\nNext, I created a data frame with the information.\r\n\r\n\r\nShow code\r\n\r\n#Create a data frame with the information:\r\n\r\ndf1 <- data.frame( c('Bypass', 'Angiography')\r\n                   ,c(19, 18)\r\n                   ,c(539, 847)\r\n                   ,c(10, 9)\r\n                   ,c(18.29151, 17.49134)\r\n                   ,c(19.70849, 18.50866))\r\nnames(df1) <- c('Procedure', 'Mean', 'Sample', 'SD', 'Lower', 'Upper')\r\n\r\ndf1\r\n\r\n\r\n    Procedure Mean Sample SD    Lower    Upper\r\n1      Bypass   19    539 10 18.29151 19.70849\r\n2 Angiography   18    847  9 17.49134 18.50866\r\n\r\nFinally, I created a plot to visualize the results. The visualization communicates that for each procedure, there is a range of values where we can expect 90% of the estimates to include the population mean given the sample mean and standard deviation.\r\n\r\n\r\nShow code\r\n\r\ngg1 <- ggplot(data = df1)\r\ngg1 <- gg1 + geom_point(aes(x = Procedure, y = Mean), size = 5, color = \"blue\")\r\ngg1 <- gg1 + geom_errorbar(aes(x = Procedure, y = Mean, ymin=Lower, ymax=Upper), width=.1, color = \"blue\")\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Lower, label = round(Lower, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Upper, label = round(Upper, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Mean, label = round(Mean, 2)), hjust = -0.5)\r\ngg1 <- gg1 + labs(x = \"Procedure\", y = \"Mean\")\r\ngg1 <- gg1 + labs(title = \"Chart Showing Mean With Upper and Lower Confidence Intervals at 90%\")\r\ngg1 <- gg1 + theme_classic()\r\n\r\ngg1\r\n\r\n\r\n\r\n\r\nFor the angiography, we can be 90% sure that the population mean for the wait time to the procedure falls between 17.49 and 18.51 days.\r\nFor the bypass, we can be 90% sure that the population mean for the wait time to the procedure falls between 18.29 and 19.71 days.\r\nThe confidence interval was narrower for the angiography surgery.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success.\r\nFind the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\r\nConstruct and interpret a 95% confidence interval for p.\r\nAnswer 2\r\nTo construct the 95% confidence interval, I began by calculating the point sample estimate of the population proportion:\r\n\r\n\r\n#Calculate the point sample estimate of the population proportion using (p = x/n)\r\n\r\nx2 = 567 #affirmative response size\r\nn2 = 1031 #sample size - survey participants\r\n\r\np2 <- x2/n2\r\np2\r\n\r\n\r\n[1] 0.5499515\r\n\r\nThis tells me that the sample proportion of those who believe a college education is essential for success is ~45%.\r\nSince np >= 5 and n(1-p) >= 5, I know that I will calculate the confidence interval of that population proportion as follows: p +/- z * square root of (p) x (1-p)/n\r\n\r\n\r\n#Calculate the confidence interval for p:\r\n\r\nerror2 <-qnorm(0.975)*sqrt(p2*(1-p2)/n2)\r\nerror2\r\n\r\n\r\n[1] 0.03036761\r\n\r\nlower2 <- p2-error2\r\nupper2 <- p2+error2\r\n\r\nlower2\r\n\r\n\r\n[1] 0.5195839\r\n\r\nupper2\r\n\r\n\r\n[1] 0.5803191\r\n\r\nThis tells me that we can be 95% confident that the proportion of adult Americans who believe that a college education is essential for success lies between 51.96% and 58.03% of the population.\r\nAlternatively, using the R prop.test function, I can compare the calculation to my manual calculation and find it is the same for finding the point, but slightly different (at 4 decimal points) on the calculation of the confidence interval.\r\n\r\n\r\nprop.test(x2, n2)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  x2 out of n2, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range.\r\nAssuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer 3\r\nI will start by taking the information given and calculating the variables I need to know.\r\nIf the aid office believes the amount spent on books is between $30 and $200, I have a range of $170 to consider.\r\nI want to be 95% confident that the interval estimate contains the population mean, so my z = 1.96.\r\nI also know that the margin of error should be no more than +/- $5 on each end of the estimate, so my margin of error = 5\r\n\r\n\r\nerror3 <- (5)\r\n\r\n#I need to calculate the standard deviation at the given estimate that it is a quarter of the range:\r\n\r\nsd3 <- 170*0.25 #range * 25%\r\nsd3 #standard deviation = 42.5\r\n\r\n\r\n[1] 42.5\r\n\r\n#Now I can calculate the sample size with the formula n=(zσ/M)2.\r\n\r\nss3 <- ((1.96*sd3)/error3)^2\r\nss3 #sample size\r\n\r\n\r\n[1] 277.5556\r\n\r\nUsing these calculations, I can estimate that the financial aid office will need to use a sample size of 278 people.\r\nQuestion 4\r\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nB. Report the P-value for Ha : μ < 500. Interpret.\r\nC. Report and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nAnswer 4\r\nA. I will start by taking the information given and determining my hypotheses:\r\nH0: The mean weekly earnings for the population of women at the company is μ=$500#\r\nHa: The mean weekly earnings for the population of women at the company is μ≠$500\r\nI cannot assume that the population distribution is normal as I have a low sample size of 9 and it is not stated it is a random sample, but a representative sample.\r\nMy other assumptions are:\r\npopulation mean (mu4) = 500\r\nsample size (n4) = 9\r\nsample mean (xbar4) = 410\r\nsample standard deviation (sd4) = 90\r\nI also need to make a decision about using a significance level of 5%.\r\nI will use the test statistic formula to find the t-value: [t = (x̄) - (μ) / (sd/sqrt(n)]\r\n\r\n\r\na4 <- 500\r\nn4 <- 9\r\nxbar4 <- 410\r\nsd4 <- 90\r\n\r\n#Test statistic:\r\n\r\nt4 <-(xbar4-a4)/(sd4/sqrt(n4))\r\n\r\nt4\r\n\r\n\r\n[1] -3\r\n\r\nGiven that my test statistic = (-3), I can determine the p-value is .00135*2 (to get the sum of both tail probabilities) or 0.0027.\r\nSince my confidence level is 0.05 and my p-value of 0.0027 < 0.05, I can reject the null hypothesis that the mean weekly earnings for the population of women at the company is μ=$500.\r\n\r\n\r\n#Then I take the t-statistic result (-3) and the degrees of freedom by taking \"sample size - 1\" or (\"9\" - 1) = 8.\r\n\r\npval4a <- pt(-3, 8)\r\n\r\npval4a\r\n\r\n\r\n[1] 0.008535841\r\n\r\nB. For the alternative hypothesis Ha: μ < 500:\r\n\r\n\r\npval4b <- pt(-3, 8, lower.tail=FALSE)\r\n\r\npval4b\r\n\r\n\r\n[1] 0.9914642\r\n\r\nPer the hint, I can confirm that these are logical answers by adding the two probabilities together and confirming they equal 1:\r\n\r\n\r\npval4a+pval4b\r\n\r\n\r\n[1] 1\r\n\r\nQuestion 5\r\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000.\r\nJones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7.\r\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer 5\r\nA. To show the t-scores, I will use the test statistic [t = (ybar) - (μ) / (se)] given the results for each:\r\n\r\n\r\n#Test statistic for Jones:\r\n\r\nybar5a <- 519.5\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5a <-(ybar5a-a5)/(se5)\r\n\r\nt5a\r\n\r\n\r\n[1] 1.95\r\n\r\n#Test statistic for Smith:\r\n\r\nybar5b <- 519.7\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5b <-(ybar5b-a5)/(se5)\r\n\r\nt5b\r\n\r\n\r\n[1] 1.97\r\n\r\nTo show the p-values, I will use the pt() function and use the t-statistic results from each of the tests and the degrees of freedom by taking “sample size - 1” or (“100” - 1) = 999. I will need to multiply each result by 2 to account for the probabilities in each tail of the normal distribution.\r\n\r\n\r\n#For Jones' results:\r\n\r\npval5a <- pt(1.95, 999, lower.tail = FALSE) * 2\r\n\r\npval5a\r\n\r\n\r\n[1] 0.05145555\r\n\r\n#For Smith's results:\r\n\r\npval5b <- pt(1.97, 999, lower.tail = FALSE) * 2\r\n\r\npval5b\r\n\r\n\r\n[1] 0.04911426\r\n\r\nB. To use α = 0.05 and look at each result and whether it is “statistically significant”, I can compare the p-values directly to the confidence level of 0.5. Jones’ results gave a p-value of 0.5145, which is just over the threshold of the confidence level given of 0.5. Smith’s results gave a p-value of 0.4911, which is just under the threshold of the confidence level given of 0.5.\r\nHypothesis tests tell us that if the p-value < α, we reject H0 and if p-value ≥ α, we do not reject H0. Given this general statistical guidance, only Smith’s results would be considered “statistically significant”.\r\nC. The results in this example could be very misleading if only whether the results were reported as simply “rejecting” or “not rejecting” H0 or being “statistically significant” or not because that is leaving out vital information on how close the results were to the confidence level used. If the actual p-values were reported instead, they would reflect how marginally “significant” the results really were. The confidence is an artificial threshold that is subjectively applied, and in this case, the results were close enough that they should not be statistically reported as significally different. This is why we need to be sure we report the full p-values and confidence intervals.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer\r\nTo answer whether there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents, we need to use a left-tailed t-test. We will use the sample data in the variable “gas_taxes” in the t.test function. We know that the t.test() function uses the 95% confidence interval as a default, but it also uses a two-sided test as the default, so we need to provide the alternative argument “less”, indicating a left-sided test.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nt.test(gas_taxes, alternative = \"less\")\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 1\r\nalternative hypothesis: true mean is less than 0\r\n95 percent confidence interval:\r\n     -Inf 44.67946\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThis t-test gives us a confidence interval of [inf - 44.67946]. Since the confidence interval includes amounts that are all less than 45 cents, we have enough evidence to conclude that, at that confidence level, the average tax was less than 45 cents.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomkbec864954/distill-preview.png",
    "last_modified": "2022-02-24T15:25:24-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomowenvespa865400/",
    "title": "Statistical Inference II & Comparing two means",
    "description": "DACSS 603 Homework 1",
    "author": [
      {
        "name": "Rhowena Vespa",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\r\n\r\n\r\nlibrary(distill)\r\nlibrary(dplyr)\r\nlibrary(tidyverse)\r\nProblem1<- read.csv('homework1_prob1.csv',TRUE,',',na.strings = \"N/A\")\r\nProblem1\r\n\r\n\r\n  ï..Surgical.Procedure Sample.Size Mean.Wait.Time Standard.Deviation\r\n1                Bypass         539             19                 10\r\n2           Angiography         847             18                  9\r\n\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nFor Bypass group:\r\n\r\n\r\n#Our values are: \r\nBypass.mean<- 19\r\nBypass.sd<-10\r\nBypass.n<-539\r\nBypass.se <- Bypass.sd/sqrt(Bypass.n) #This is standard error of the mean\r\n\r\n\r\n\r\n\r\n\r\n#Find the t.score for CI 90%\r\nalpha = 0.1\r\ndegrees.freedom = Bypass.n - 1\r\nBypass.t.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\r\nprint(Bypass.t.score)\r\n\r\n\r\n[1] 1.647691\r\n\r\n\r\n\r\n#Calculate margin of error\r\nBypass.margin.error <- Bypass.t.score * Bypass.se\r\nprint(Bypass.margin.error)\r\n\r\n\r\n[1] 0.7097107\r\n\r\n\r\n\r\n#Calculate the 90% confidence interval for Bypass\r\n  lower.bound <- Bypass.mean - Bypass.margin.error\r\n  upper.bound <- Bypass.mean + Bypass.margin.error\r\n  print(c(lower.bound,upper.bound))\r\n\r\n\r\n[1] 18.29029 19.70971\r\n\r\nFor Angiography group:\r\n\r\n\r\n#Our values are: \r\nAngio.mean<- 18\r\nAngio.sd<-9\r\nAngio.n<-847\r\nAngio.se <- Angio.sd/sqrt(Angio.n) #This is standard error of the mean\r\n\r\n\r\n\r\n\r\n\r\n#Find the t.score for CI 90%\r\nalpha = 0.1\r\ndegrees.freedomA = Angio.n - 1\r\nAngio.t.score = qt(p=alpha/2, df=degrees.freedomA,lower.tail=F)\r\nprint(Angio.t.score)\r\n\r\n\r\n[1] 1.646657\r\n\r\n\r\n\r\n#Calculate margin of error\r\nAngio.margin.error <- Angio.t.score * Angio.se\r\nprint(Angio.margin.error)\r\n\r\n\r\n[1] 0.5092182\r\n\r\n\r\n\r\n#Calculate the 90% confidence interval for Angiography\r\n  lower.boundA <- Angio.mean - Angio.margin.error\r\n  upper.boundA <- Angio.mean + Angio.margin.error\r\n  print(c(lower.boundA,upper.boundA))\r\n\r\n\r\n[1] 17.49078 18.50922\r\n\r\nAnswer for Question #1-Is the confidence interval narrower for angiography or bypass surgery?:\r\nAngiography patients, at 90% confidence interval, had between (17.49078 and 18.50922) wait time in days which is NARROWER compared to the bypass patients wait time which is between (18.29029 and 19.70971)\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\n\r\n\r\n#Point Estimate, p\r\nn<-1031\r\nk<-567\r\np<-k/n\r\np\r\n\r\n\r\n[1] 0.5499515\r\n\r\nInterpretation of point estimate p:\r\nThe sample proportion of adult Americans who believed that college education is essential for success is 0.5499515 or 55%. This represents our point estimate for the population (adult Americans) proportion.\r\n\r\n\r\n#Construct 95% confidence interval for p\r\n\r\nS.margin <- qnorm(0.975)*sqrt(p*(1-p)/n)  #calculate margin of error\r\n  S.lower.bound <- p-S.margin\r\n  S.upper.bound <- p+S.margin\r\n  print(c(S.lower.bound,S.upper.bound))\r\n\r\n\r\n[1] 0.5195839 0.5803191\r\n\r\nInterpretation of 95% confidence interval for p:\r\nThe 95% confidence interval for the population (adult Americans) proportion is [0.5195839 0.5803191]. This means between 51.9% to 58% of adult Americans believed that college education is essential for success.\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nn= square of ((Z 0.05/2 * sd of pop)/within $5 of true pop mean\r\n\r\n\r\nsd_of_pop =(200-30)/4 #This is standard deviation of population\r\nsd_of_pop\r\n\r\n\r\n[1] 42.5\r\n\r\nsample_size=((1.96*sd_of_pop)/5)**2  #Using Z score 1.96 for significance level 5%\r\nsample_size\r\n\r\n\r\n[1] 277.5556\r\n\r\nANSWER: Sample size needed to achieve significance level of 95% is 278.\r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nassumption: seed set at 123, using rnorm\r\nHo mu=500\r\nHa mu≠500\r\nMean y_hat =410\r\nsd = 90\r\nn =9\r\n\r\n\r\nset.seed(123)\r\nMean_income <- c(rnorm(9, mean = 410, sd = 90)) \r\nMean_income\r\n\r\n\r\n[1] 359.5572 389.2840 550.2837 416.3458 421.6359 564.3558 451.4825\r\n[8] 296.1445 348.1832\r\n\r\n\r\n\r\nt.test(Mean_income, mu = 500) # Ho: mu=500\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  Mean_income\r\nt = -2.6213, df = 8, p-value = 0.03059\r\nalternative hypothesis: true mean is not equal to 500\r\n95 percent confidence interval:\r\n 353.2313 490.6071\r\nsample estimates:\r\nmean of x \r\n 421.9192 \r\n\r\nInterpretation of one sample t-test result:\r\nAt p-value 0.03, considered statistically significant, we reject the null hypothesis. We can conclude that mean income for female employees is not 500.\r\nReport the P-value for Ha : μ < 500. Interpret.\r\n\r\n\r\nt.test(Mean_income, mu=500, alternative = 'less') # Ha: mu<500\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  Mean_income\r\nt = -2.6213, df = 8, p-value = 0.01529\r\nalternative hypothesis: true mean is less than 500\r\n95 percent confidence interval:\r\n     -Inf 477.3087\r\nsample estimates:\r\nmean of x \r\n 421.9192 \r\n\r\nInterpretation of Ha : μ < 500:\r\nAt p-value 0.01529, considered statistically significant, we reject the null hypothesis and accept the alternative hypothesis. We can conclude that mean income for female employees is less than 500.\r\nReport and interpret the P-value for Ha: μ > 500.\r\n\r\n\r\nt.test(Mean_income, mu=500, alternative = \"greater\") # Ha: mu>500\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  Mean_income\r\nt = -2.6213, df = 8, p-value = 0.9847\r\nalternative hypothesis: true mean is greater than 500\r\n95 percent confidence interval:\r\n 366.5297      Inf\r\nsample estimates:\r\nmean of x \r\n 421.9192 \r\n\r\nInterpretation of Ha: μ > 500:\r\nAt p-value 0.9847, considered statistically NOT significant, we fail to reject the null hypothesis. We can reject the alternative hypothesis that mean income for female employees is greater than 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\n\r\n\r\ntotal_p_value=0.9847+0.01529\r\nprint(c(\"Total p-values for the two possible one-sided tests is\",total_p_value))\r\n\r\n\r\n[1] \"Total p-values for the two possible one-sided tests is\"\r\n[2] \"0.99999\"                                               \r\n\r\nQuestion 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\r\n\r\n\r\ntab <- matrix(c(519.5, 519.7, 10, 10), ncol=2, byrow=TRUE)\r\ncolnames(tab) <- c(\"Jones\",\"Smith\")\r\nrownames(tab) <- c(\"y_hat\",\"se\")\r\ntab <- as.table(tab)\r\nprint(tab)\r\n\r\n\r\n      Jones Smith\r\ny_hat 519.5 519.7\r\nse     10.0  10.0\r\n\r\nCalculate t and p-value\r\nt.stat <- (y_hat - mu)/sample.se\r\np.value = pt(q=abs(t.stat), df=degrees.freedom,lower.tail=F) 2 *\r\nShow that t = 1.95 and P-value = 0.051 for Jones.\r\n\r\n\r\nt.stat <- (519.5 - 500)/10\r\nprint(c(\"t.stat\",t.stat))\r\n\r\n\r\n[1] \"t.stat\" \"1.95\"  \r\n\r\n\r\n\r\ndegrees.freedom = 1000 - 1\r\np.value = pt(q=abs(t.stat), df=degrees.freedom,lower.tail=F) * 2\r\nprint(c(\"Two-sided p-value\",p.value))\r\n\r\n\r\n[1] \"Two-sided p-value\"  \"0.0514555476459477\"\r\n\r\nShow that t = 1.97 and P-value = 0.049 for Smith.\r\n\r\n\r\nSmith.t.stat <- (519.7 - 500)/10\r\nprint(c(\"t.stat\",Smith.t.stat))\r\n\r\n\r\n[1] \"t.stat\" \"1.97\"  \r\n\r\n\r\n\r\ndegrees.freedom = 1000 - 1\r\nSmith.p.value = pt(q=abs(Smith.t.stat), df=degrees.freedom,lower.tail=F) * 2\r\nprint(c(\"Two-sided p-value\",Smith.p.value))\r\n\r\n\r\n[1] \"Two-sided p-value\"  \"0.0491142565416521\"\r\n\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\n  α = 0.05\r\n  H0: μ = 500\r\n  Ha : μ ≠ 500\r\n  \r\nFor Jones Data: Since p=0.051, we fail to reject the null hypothesis (H0: μ = 500) and reject the alternative hypothesis (Ha : μ ≠ 500). The p-value is NOT statistically significant.\r\nFor Smith Data: Since p=0.049, we reject the null hypothesis (H0: μ = 500) and accept the alternative hypothesis. We conclude that μ ≠ 500. P-value is statistically significant.\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\n\r\n\r\nprint(tab)\r\n\r\n\r\n      Jones Smith\r\ny_hat 519.5 519.7\r\nse     10.0  10.0\r\n\r\n#α = 0.05\r\n#H0: μ = 500\r\n#Ha : μ ≠ 500\r\n#Zc= 1.96\r\n\r\n\r\n\r\nCalculate z-score= (y_hat -μ )/ se\r\n\r\n\r\nJones.z <- (519.5-500)/10\r\nJones.z\r\n\r\n\r\n[1] 1.95\r\n\r\n\r\n\r\nSmith.z <- (519.7-500)/10\r\nSmith.z\r\n\r\n\r\n[1] 1.97\r\n\r\nInterpretation without reporting the actual P-value:\r\nReporting the result of a test using p-values could be misleading. We can avoid this by using z-score to report the results, using z-score =1.96 as the same 95% confidence level. Jones z-score of 1.95 < 1.96 means we fail to reject the null hypothesis (H0: μ = 500) and reject the alternative hypothesis (Ha : μ ≠ 500). Smith z-score of 1.97 > 1.96 we reject the null hypothesis (H0: μ = 500) and accept the alternative hypothesis(Ha : μ ≠ 500).\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\n#H0: μ = 45\r\n#Ha : μ ≠ 45, specifically μ < 45 assuming one-sided using argument alternative= \"lesser\"\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nt.test(gas_taxes,mu=45, alternative = 'less')\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = -1.8857, df = 17, p-value = 0.03827\r\nalternative hypothesis: true mean is less than 45\r\n95 percent confidence interval:\r\n     -Inf 44.67946\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nInterpretation of one sample t-test:\r\nYes, there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents. With the p-value of 0.03827 < α = 0.05, we reject the null hypothesis that μ = 45 and accept the alternative hypothesis that μ < 45 cents.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomrhyslong96870303/",
    "title": "Homework 1",
    "description": "Here is my submission for homework 1.",
    "author": [
      {
        "name": "Rhys Long",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\n\n\n##QUESTION 1##\n#Sample Size\nAngiography_Sample=847\nBypass_Sample=539\n\n#Mean\nAngiography_Mean=18\nBypass_Mean=19\n\n#Standard Deviation\nAngiography_STD=9\nBypass_STD=10\n\n#Standard Error\nAngiography_SE=Angiography_STD/sqrt(Angiography_Sample)\nBypass_SE=Bypass_STD/sqrt(Bypass_Sample)\n\n#Tail Area\nConfidence_Level=0.9\nTail_Area=(1-Confidence_Level)/2\n\n#T Scores\nAngiography_T=qt(p=0.95,df=(Angiography_Sample-1))\nBypass_T=qt(p=0.95,df=(Bypass_DF=Bypass_Sample-1))\n\n#Margin Of Error\nAngiography_MOE=Angiography_T*Angiography_SE\nBypass_MOE=Bypass_T*Bypass_SE\n\n#Angiography Confidence Interval\nLower_Angiography=Angiography_Mean-Angiography_MOE\nUpper_Angiography=Angiography_Mean+Angiography_MOE\nAngiography_CI=c(Lower_Angiography, Upper_Angiography)\nc(\"Angiography Confidence Interval:\", Angiography_CI)\n\n\n[1] \"Angiography Confidence Interval:\"\n[2] \"17.4907818376895\"                \n[3] \"18.5092181623105\"                \n\n#Bypass Confidence Interval\nLower_Bypass=Bypass_Mean-Bypass_MOE\nUpper_Bypass=Bypass_Mean+Bypass_MOE\nBypass_CI=c(Lower_Bypass, Upper_Bypass)\nc(\"Bypass Confidence Interval:\", Bypass_CI)\n\n\n[1] \"Bypass Confidence Interval:\" \"18.2902893200424\"           \n[3] \"19.7097106799576\"           \n\n#Comparing The Confidence Interval Widths\nAngiography_Width=Upper_Angiography-Lower_Angiography\nBypass_Width=Upper_Bypass-Lower_Bypass\nc(\"Angiography Confidence Interval Width:\", Angiography_Width)\n\n\n[1] \"Angiography Confidence Interval Width:\"\n[2] \"1.01843632462099\"                      \n\nc(\"Bypass Confidence Interval Width:\", Bypass_Width)\n\n\n[1] \"Bypass Confidence Interval Width:\"\n[2] \"1.41942135991513\"                 \n\nThe actual mean wait for angiographies is around 17.4907818376895 to 18.5092181623105 days and the actual mean wait for bypasses is 18.2902893200424 to 19.7097106799576. To find out the confidence interval for both procedures, I first found the standard error, since I already knew the sample sizes and standard deviations. From there I determined the t scores for both procedures. Even though the samples for both procedures exceeded 30, I used a t score instead of a z score because the sample standard deviations were different and the population standard deviation was unspecified. In order to figure out the correct t score, I used qt() with p set to 0.95 because each tail has an area of 0.05 when the confidence level is 90%. Once I had the t scores, I determined the margin of error and confidence intervals. To figure out which procedure has the narrower confidence interval, I subtracted the upper bound value by the lower bound value for each respective procedure. The angiography has a narrower confidence interval than the bypass.\n\n\n##QUESTION 2##\n#Sample Size\nSample=1031\n\n#Pro-College vs Anti-College Proportions\nPro_College=567/Sample\nAnti_College=1-Pro_College\n\n#Z Score, Standard Error, And Margin Of Error\nZ=1.96\nSE=sqrt((Pro_College*Anti_College)/Sample)\nMOE=Z*SE\n\n#Confidence Interval\nLower_Bound=Pro_College-MOE\nUpper_Bound=Pro_College+MOE\nConfidence_Interval <- c(Lower_Bound,Upper_Bound)\nConfidence_Interval \n\n\n[1] 0.5195833 0.5803197\n\nThe 95% confidence interval for p is 51.95833-58.03197%. Based on the fact that 567/1031 adults in the survey believed that college education is essential for success, I was able to conclude that 464/1031 adults in the survey disagreed with that mindset. From there, I used sqrt((Pro College Proportion*Anti College Proportion)/Sample Size) to find the standard error since I don’t need a standard deviation to find the standard error of a proportion. Next, I multiplied the standard error by 1.96, otherwise known as the 95% confidence level z score, to find the margin of error. The reason why I used a z score instead of a t score is because the instructions specify that I should assume that the results are reflective of the entire population. Finally, I subtracted the margin of error from the pro-college proportion to find the lower bound confidence interval limit and I added the margin of error to the pro-college proportion to find the upper bound confidence interval limit.\n\n\n##QUESTION 3##\n#Determining Standard Deviation\nRange=200-30\nSTD=Range/4\nSTD\n\n\n[1] 42.5\n\n#Determnining Margin Of Error And Z Score\nM=5\nZ=1.96\n\n#Determnining Sample Size\nZ_Div_M=Z/M\nSize=(STD*STD)*(Z_Div_M*Z_Div_M)\nSize\n\n\n[1] 277.5556\n\n#Checking Work\nM_Check=Z*(STD/sqrt(278))\nM_Check\n\n\n[1] 4.996002\n\nThe ideal sample size for an experiment with the specifications of question 3 is at least 278. Before figuring out the ideal sample size, I had to find the standard deviation and the margin of error. Based on the question 3 instructions, I could conclude that the standard deviation is 42.5 the spending range is 170 (200-30) and the population standard deviation is the spending range divided by 4. I could also conclude that the margin of error is 5 because the confidence interval length should be 10 dollars or less and the margin of error is half the confidence interval length. In order to determine the ideal sample size, I first divided the margin of error by 1.96, otherwise known as the 5% significance level z score. I used a z score instead of a t score because the population standard deviation is known. From there, I multiplied the standard deviation squared by (Margin of Error/Z Score) squared and got 277.5556, which can be rounded up to 278. To check my work, I used the M=Z*(STD/sqrt(N)).\n\n\n##QUESTION 4##\n#Mean And Standard Deviation\nPopulation_Mean=500\nWomen=9\nWoman_Mean=410\nSTD=90\nSE=STD/sqrt(Women)\n\n#T-Score\nT_Score=(Woman_Mean-Population_Mean)/SE\nT_Score\n\n\n[1] -3\n\n#A: Find Two Tail P Value\nTwo_Tail_P=2*pt(q=T_Score,df=(Women-1))\nTwo_Tail_P\n\n\n[1] 0.01707168\n\n#B: Left Tail P Value\nLeft_Tail_P=pt(q=T_Score,df=(Women-1),lower.tail=TRUE)\nLeft_Tail_P\n\n\n[1] 0.008535841\n\n#C: Right Tail P Value\nRight_Tail_P=pt(q=T_Score,df=(Women-1),lower.tail=FALSE)\nRight_Tail_P\n\n\n[1] 0.9914642\n\nFor question 4 part A, I concluded that there is a statistically significant difference between the income of women and the mean income. The null hypothesis is that the income of women is equal to that of everyone else. The alternative hypothesis is that hypothesis is that the income of women isn’t equal to that of everyone else. In order to figure out whether there is a statistically significant difference, I had to figure out the T-Score. The T-Score I got from dividing the difference between the women’s mean and the population mean by the standard error is -3. Even though I know that a T-Score of -3 is probably reflective of statistically significant differences, I still determined the two tailed p value by using 2*pt(q=T_Score,df=(Women-1)). I multiplied pt(q=T_Score,df=(Women-1)) by 2 because the pt() function is used for determining 1 tail p values and I was interested in finding the 2 tail p value. The p value I got was p=0.01707168, which is considered statistically significant when using the p<0.05 significance level. To find the left tail P value, I used pt(q=T_Score,df=(Amount of Women-1),lower.tail=TRUE) and I got p=0.008535841, which supports the alternative hypothesis of mu<500 to a highly significant extent. To find the right tail P value, I used Right_Tail_P=pt(q=T_Score,df=(Women-1),lower.tail=FALSE) and I got 0.9914642, which is indicative of mu>500 being false.\n\n\n##QUESTION 5##\n#Null Mean And Sample Size\nNull_Mean=500\nN=1000\n\n#Jones Mean And Standard Error\nJones_Mean=519.5\nJones_SE=10.0\n\n#Jones T\nJones_T=(Jones_Mean-Null_Mean)/Jones_SE\nJones_T\n\n\n[1] 1.95\n\n#Jones P\nJones_P=2*pt(q=Jones_T,df=(N-1),lower.tail=FALSE)\nJones_P\n\n\n[1] 0.05145555\n\n#Smith Mean And Standard Error\nSmith_Mean=519.7\nSmith_SE=10.0\n\n#Smith T\nSmith_T=(Smith_Mean-Null_Mean)/Jones_SE\nSmith_T\n\n\n[1] 1.97\n\n#Smith P\nSmith_P=2*pt(q=Smith_T,df=(N-1),lower.tail=FALSE)\nSmith_P\n\n\n[1] 0.04911426\n\nAccording to alpha=0.05, Smith’s results are statistically significant, but Jones’s results are not. The reason why neglecting to report the p value is so misleading when reporting results is because there is no indication of how close the results are to being statistically significant. Jones’s p value of 0.05145555 comes extremely close to being statistically significant when the significance level is set to p<=0.05 and if the significance level was set to p<=0.1, Jones’s results would undoubtedly be classified as statistically significant. If I say Jones’s results are not statistically significant with p<=0.05 and that I can’t reject the null hypothesis, nobody will know that Jones’s results come extremely close to being statistically significance and that could lead to type 2 errors.\n\n\n##QUESTION 6##\n#Dataset\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n#T-Test\nt.test(gas_taxes, mu=45, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\nEven though the sample size is only 18, there is enough evidence to conclude that the gas prices in 2005 were less than 45 cents with a 95% confidence interval. Given that the data is provided, I was able to use the t test function to figure out the answer. In this context, the null hypothesis is that the average tax per gallon is equal to 45 cents, so I set mu to mu=45. For the alternative= component, I used alternative=“less” because the alternative hypothesis is that the true mean of the gas prices is less than 45 cents. When I ran the t-test, I got a p value of 0.03827, which is considered statistically significant because a 95% confidence interval calls for a p value that is less than or equal to 0.05.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to DACSS 601",
    "description": "Welcome to DACSS 603! We hope you enjoy the class!",
    "author": [
      {
        "name": "Omer Yalcin",
        "url": "http://umass.edu/sbs/dacss"
      }
    ],
    "date": "2022-02-24",
    "categories": [
      "welcome"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:14:40-05:00",
    "input_file": {}
  }
]
