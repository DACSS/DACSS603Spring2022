[
  {
    "path": "posts/httprpubscomkpopiela881714/",
    "title": "Homework 2",
    "description": "DACSS-603",
    "author": [
      {
        "name": "Katie Popiela",
        "url": {}
      }
    ],
    "date": "2022-03-27",
    "categories": [],
    "contents": "\r\nQuestion 1\r\n(Problem 1.1 in ALR) United Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\na. Identify the predictor and the response\r\nPredictor: ppgdp\r\nResponse: fertility\r\nb. Draw the scatterplot of “fertility” on the vertical axis versus “ppgdp” on the horizontal axis and summarize the information in the graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n\r\n\r\nlibrary(alr4)\r\ndata(UN11)\r\n\r\nlibrary(ggplot2)\r\nggplot(data = UN11, aes(x=ppgdp,y=fertility)) + geom_point()\r\n\r\n\r\n\r\n\r\nThe scatterplot shows a marked decline in fertility rates as GDP increases. I will now recreate the scatterplot with a straight-line function to see if it appears to be appropriate for this presentation of the data.\r\n\r\n\r\ndata(UN11)\r\nggplot(data = UN11, aes(x=ppgdp,y=fertility)) + geom_point()+\r\n  geom_smooth(method=\"lm\",se=FALSE)\r\n\r\n\r\n\r\n\r\nAs can be seen above, a straight-line function is not appropriate; the data is not currently presented in a linear manner (the data is L-shaped). I will now try a linear regression model to see if a straight-line funtion is applicable there.\r\nc. Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\n\r\n\r\ndata(UN11)\r\nggplot(data = UN11, aes(x=log(ppgdp),y=log(fertility))) + geom_point() +\r\n  geom_smooth(method=\"lm\",se=FALSE)\r\n\r\n\r\n\r\n\r\nA simple linear regression model is much more plausible for a straight-line function.\r\nQuestion 2\r\n(Problem 9.47 in SMSS) Annual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\na. How, if at all, does the slope prediction equation change?\r\nTo account for the conversion rate from USD to GBP, the value of the response must be divided by 1.33. The slope shall also be divided by 1.33.\r\nb. How, if at all, does the correlation change?\r\nCorrelation isn’t affected by units of measurement, so it would not change in this scenario.\r\nQuestion 3\r\n(Problem 1.5 in ALR) Water runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\nlibrary(alr4)\r\ndata(water)\r\npairs(water)\r\n\r\n\r\n\r\n\r\nSince this matrix presents a lot of information, I’ll summarize:\r\n* Year doesn’t seem to be related to runoff or water levels\r\n* The following variables appear to be correlated with each other: OPBPC, OPRC, OPSLAKE. All parts of the matrix with 2 of these variables exhibit a dependence among themselves that is not present between OPBPC, OPRC, and OPSLAKE and APMAM, APSAB, APSLAKE. That being said, though, there also appears to be a correlation among APMAM, APSAB, APSLAKE.\r\n* BSAAM is more closely related to OPBPC, OPRC, and OPSLAKE than to APMAM, APSAB, APSLAKE.\r\nQuestion 4\r\n(Problem 1.6 in ALR, modified) Professor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n\r\n\r\nlibrary(alr4)\r\ndata(Rateprof)\r\npairs(Rateprof[c(\"quality\",\"clarity\",\"helpfulness\",\"easiness\",\"raterInterest\")])\r\n\r\n\r\n\r\n\r\nThere is a strong correlation among “quality”, “clarity”, and “helpfulness.” As those variables increase, so too do the professors’ ratings. This makes sense as the quality and clarity of the material, as well as the professor’s helpfulness are major factors in undergraduate learning. There appears to be some correlation among “helpfulness” and “easiness” but the data is much more dispersed. “raterInterest” seems pretty consistent in the middle of each graph, indicating that the rater is at least moderately interested in the subject matter of the courses they are rating.\r\nQuestion 5\r\n(Problem 9.34 in SMSS) For the “student.survey” data file in the smss package, conduce regression analyses relating:\r\na. y = political ideology and x = religiosity\r\nb. y = high school GPA and x = hours of TV watching\r\n(You can use ?student.survey in the R console after loading the package to see what each variable means)\r\n* Use graphical ways to portray the individual variables and their relationships.\r\n* Interpret descriptive statistics for summarizing the individual variables and their relationships.\r\n* Summarize and interpret the results of inferential analyses.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nlibrary(smss)\r\ndata(\"student.survey\")\r\ncolnames(student.survey)\r\n\r\n\r\n [1] \"subj\" \"ge\"   \"ag\"   \"hi\"   \"co\"   \"dh\"   \"dr\"   \"tv\"   \"sp\"  \r\n[10] \"ne\"   \"ah\"   \"ve\"   \"pa\"   \"pi\"   \"re\"   \"ab\"   \"aa\"   \"ld\"  \r\n\r\nThe variables I will be focusing on (as per the problem) are “re” (x) and “pi” (y) for subsection (a); and then “hi” (x) and “tv” (y) for subsection (b)\r\n\r\n\r\nlibrary(smss)\r\ndata(\"student.survey\")\r\nggplot(data=student.survey,aes(x=re,fill=pi))+\r\n  geom_bar() + labs(x=\"Religiosity\", fill =\"Political Ideology\")\r\n\r\n\r\n\r\n\r\nThe graph above is one (of several possible) visualizations of the relationship between religiosity and political ideology. I couldn’t figure out how to get more info on what exactly the variables mean, so I’m assuming “Religiosity” refers to the frequency individuals of different political ideologies go to church/temple/mosque/etc.. From left to right, the frequency goes from “never” to “every week.” As frequency increases, so too does conservatism. While not a majority by any means, it is still significant to note that those who identify as very conservative only appear in the bar labelled “every week,” whereas those who identify as very liberal are not even present on the graph to the right of “occasionally.” This, therefore, indicates that those who are heavily liberal-leaning in political ideology are far less likely to go to church/temple/mosque/etc. regularly/frequently than those who are more conservative.\r\nb. y = high school GPA and x = hours of TV watching\r\n\r\n\r\ndata(\"student.survey\")\r\nggplot(data=student.survey,aes(x=hi, y=tv)) +\r\n  geom_point() + labs(x=\"High School GPA\", y=\"Hours Watching TV\")\r\n\r\n\r\n\r\n\r\nOnce again, this graph is just one of several visualizations that could be used. I chose a scatterplot to reflect individual responses; a standard bar graph for this scenario is, in my opinion, misleading as outliers appear to be a much higher concentration of responses. Given the measurements on this graph, I am also assuming that the y-axis refers to hours of TV watched per week. While this graph does not show a linear relationship between the two variables, there is a higher concentration of responses with higher GPA’s and lower # of hours watching TV. I will conduct a simple regression model to test whether a linear realtionship exists.\r\n\r\n\r\nggplot(data = student.survey, aes(x=log(hi),y=log(tv))) + geom_point() +\r\n  labs(x=\"High School GPA\",y=\"Hours Watching TV\")\r\n\r\n\r\n\r\n\r\nEven with a linear regression model, there does not appear to be a linear relationship between these 2 variables. There is still a higher concentration of responses on the higher end of the spectrum, but there is enough variation in the responses to argue that “Hours Watching TV” does not have a correlative affect on “High School GPA.”\r\nNow I will present some descriptive/summary statistics of all 4 variables to show their statistical significance.\r\n\r\n\r\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\r\n\r\n\r\n                     pi                re           hi       \r\n very liberal         : 8   never       :15   Min.   :2.000  \r\n liberal              :24   occasionally:29   1st Qu.:3.000  \r\n slightly liberal     : 6   most weeks  : 7   Median :3.350  \r\n moderate             :10   every week  : 9   Mean   :3.308  \r\n slightly conservative: 6                     3rd Qu.:3.625  \r\n conservative         : 4                     Max.   :4.000  \r\n very conservative    : 2                                    \r\n       tv        \r\n Min.   : 0.000  \r\n 1st Qu.: 3.000  \r\n Median : 6.000  \r\n Mean   : 7.267  \r\n 3rd Qu.:10.000  \r\n Max.   :37.000  \r\n                 \r\n\r\nSince each sample has a different number of subjects/respondents, the results are somewhat skewed.The distribution of high school GPA looks relatively normal and unimodal. The distribution of “Hours Watching TV” on the other hand, has several outliers which can be seen both on the above graph and in the summary statistics via the difference between the 3rd quartile and maximum values.\r\nQuestion 6\r\n(Problem 9.50 in SMSS)\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\r\nRegression toward the mean implies that outlying/extreme values will always occur each time the test or experiment is conducted. Additionally the values found in any reproduction of the test or experiment will be the same as the previous. In this scenario, the students could have been chosen from the group of those not doing well by chance, and the “improvement” seen in the graph might just be regression toward the mean and not actual academic improvement.\r\n\r\n\r\n\r\n",
    "preview": "posts/httprpubscomkpopiela881714/distill-preview.png",
    "last_modified": "2022-03-27T15:55:59-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomrhyslong96877697/",
    "title": "Homework 2",
    "description": "Here is Rhys Long's submission for Homework 2",
    "author": [
      {
        "name": "Rhys Long",
        "url": {}
      }
    ],
    "date": "2022-03-23",
    "categories": [],
    "contents": "\n\n\nlibrary(\"alr4\")\nlibrary(\"ggplot2\")\nlibrary(\"smss\")\nlibrary(\"tidyverse\")\nlibrary(\"numbers\")\n\n\n\nQuestion 1\nFor the first question, the predictor variable is pddgdp and the response variable is fertility because the amount of fertility is influenced by the amount of ppgdp.\n\n\ndata(UN11)\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\nWhen placing ppgdp on the x axis and fertility on the y axis, the correlation is negative. In other words, as the amount of ppgdp increases, the amount of fertility decreases. In the scatter plot below, a straight line mean function does not appear to be plausible for summarizing the data because the points appear to form a curve.\n\n\nplot(x=UN11$ppgdp,y=UN11$fertility)\n\n\n\n\nWhen using the log() function for the predictor and response variables, there is still a negative correlation between ppgdp and fertility. However, unlike the plot without the log() function, a simple linear regression is appropriate for the corresponding scatter plot because the points are distributed in a more linear manner.\n\n\nplot(x=log(UN11$ppgdp),y=log(UN11$fertility))\n\n\n\n\nQuestion 2\nIf annual income is an explanatory variable and the response variable is expressed in terms of currency, the slope of the prediction equation wouldn’t change at all because both the rise and the run would be converted to pounds in this scenario.\n\n\n#Slope When The Income, X, Is In Dollars And Y Is Also Currency\nDollar_Rise=500\nDollar_Run=2500\nDollar_Slope=Dollar_Rise/Dollar_Run\nc(\"Slope When Income Is In Dollars\", Dollar_Slope)\n\n\n[1] \"Slope When Income Is In Dollars\"\n[2] \"0.2\"                            \n\n#Slope When The Income, X, Is In Pounds And Y Is Also Currency\nPound_Rise=Dollar_Rise/1.33\nPound_Run=Dollar_Run/1.33\nPound_Slope=Pound_Rise/Pound_Run\nc(\"Slope When Income Is In Pounds\", Pound_Slope)\n\n\n[1] \"Slope When Income Is In Pounds\" \"0.2\"                           \n\nIf the response variable is not expressed in terms of currency, I hypothesize that “New Slope”=“Rise Of Old Slope”/(“Run Of Old Slope”/1.33). My hypothesis differs from the hypothesis provided in the textbook. In the book, the hypothesis is “New Slope”=“Old Slope”/1.33. To determine which hypothesis is more accurate, I will first come up with my own data set using vectors and the data.frame() function.\n\n\nIncome_Dollars <- c(100000, 2800, 52000, 840, 590000, 3400, 2700, 430)\nResponse_Variable <- c(200, 23400, 7600, 864000, 520, 64200, 4920, 658300)\nData_In_Dollars <- data.frame(Income_Dollars, Response_Variable)\nData_In_Dollars\n\n\n  Income_Dollars Response_Variable\n1         100000               200\n2           2800             23400\n3          52000              7600\n4            840            864000\n5         590000               520\n6           3400             64200\n7           2700              4920\n8            430            658300\n\nAfter creating a data set with the income in dollars, I will now use the lm() function to figure out the slope and y intercept. The y intercept is listed under “(Intercept)” and the slope is listed under “Income_Dollars”.\n\n\nDollar_Regression <- lm(formula = Response_Variable~Income_Dollars, data=Data_In_Dollars)\nprint(Dollar_Regression)\n\n\n\nCall:\nlm(formula = Response_Variable ~ Income_Dollars, data = Data_In_Dollars)\n\nCoefficients:\n   (Intercept)  Income_Dollars  \n    251694.004          -0.519  \n\nBefore testing the hypotheses, I will now create a Data_In_Pounds data set, which has the same response values as Data_In_Dollars, but has the income values converted to pounds. I will also use lm() to determine what the actual slope would be if the income is converted to pounds.\n\n\nData_In_Pounds <- rename(Data_In_Dollars, Income_Pounds=Income_Dollars) %>%\n  mutate(Income_Pounds=Income_Pounds/1.33)\nData_In_Pounds\n\n\n  Income_Pounds Response_Variable\n1    75187.9699               200\n2     2105.2632             23400\n3    39097.7444              7600\n4      631.5789            864000\n5   443609.0226               520\n6     2556.3910             64200\n7     2030.0752              4920\n8      323.3083            658300\n\nPound_Regression <- lm(formula = Response_Variable~Income_Pounds, data=Data_In_Pounds)\nprint(Pound_Regression)\n\n\n\nCall:\nlm(formula = Response_Variable ~ Income_Pounds, data = Data_In_Pounds)\n\nCoefficients:\n  (Intercept)  Income_Pounds  \n    2.517e+05     -6.903e-01  \n\nNow, I will test the book’s hypothesis and my hypothesis to see which one is more accurate. The equation I’m using to test the book’s hypothesis is M2=M1/1.33 and the equation I’m using to test my hypothesis is M2=(Y-B)/(X2). In both equations, M1 is the slope of the data when the income is in dollars and M2 is the predicted slope when the income is converted to pounds. In my equation, X1 is the “run value” of M1, X2 is the predicted “run value” of M2, Y1 and Y2 are 0, B1 is the y intercept of the prediction equation when income is in dollars, and B2 is the y intercept of the of the prediction equation when income is in pounds (and is identical to B1).\n\n\n#The Book's Hypothesis\nM1=-0.519\nBook_M2=M1/1.33\n\n#My Hypothesis\nY1=0\nB1=251694.004\nX1=(Y1-B1)/M1\nY2=Y1\nB2=B1\nX2=X1/1.33\nRhys_M2=(Y2-B2)/(X2)\n\n#The Actual Answer\nActual_M2=-6.903e-1\n\n\n\nBased on the results below, it is safe to conclude that if income is converted to pounds, the new slope would have the same rise value as the old slope, but the run value would be different. It is also safe to conclude that converting the income to pounds will not change the correlation of the data because the slope would still negative and a negative slope is indicative of a negative correlation.\n\n\n#Comparing Slopes\nc(\"Book's Predicted Slope\", Book_M2)\n\n\n[1] \"Book's Predicted Slope\" \"-0.390225563909774\"    \n\nc(\"My Predicted Slope\", Rhys_M2)\n\n\n[1] \"My Predicted Slope\" \"-0.69027\"          \n\nc(\"Actual Slope\", Actual_M2)\n\n\n[1] \"Actual Slope\" \"-0.6903\"     \n\nQuestion 3\nBased on the scatter plot matrix below it is definitely safe to conclude that the runoff volumes of similar venues are correlated with each other. More specifically, the runoff volumes of APMAM, APSAB, and APSLAKE are all positively correlated with each other and the runoff volumes of OPBPC, OPRC, and OPSLAKE are all positively correlated with each other. It is also possible to conclude that the runoff volume of BSAAM is correlated with the runoff volumes of OPBPC, OPRC, and OPSLAKE, but not with the runoff volumes of APMAM, APSAB, and APSLAKE. However, it isn’t possible to conclude whether the runoff volumes of future years can be predicted because even though there appears to be a very slight positive correlation with year and runoff volumes for each venue, the correlations look too weak to be significant.\n\n\ndata(water)\npairs(water)\n\n\n\n\nQuestion 4\nTo find the relationships between the quality, helpfulness, clarity, easiness of instructor’s courses, and rater interest, I can’t just use the pairs(Rateprof) function because when I do, I won’t get an scatter plot that contains the variables I’m interested in analyzing.\n\n\ndata(Rateprof)\npairs(Rateprof)\n\n\n\n\nIn order to fix the problems shown in the scatter plot above, I will now create a new data set with only the quality, helpfulness, clarity, easiness of instructor’s course, and rater interest variables selected before using the pairs() function to form a scatter plot matrix.\n\n\nRateprof_Data <- select(Rateprof, quality:raterInterest)\npairs(Rateprof_Data)\n\n\n\n\nBased on the scatter plot above, quality ratings have a strong positive correlation with helpfulness and clarity ratings. In other words, if a professor received high quality ratings, chances are that they received high helpfulness and clarity ratings as well. There is also a strong positive correlation between helpfulness and clarity, but the correlation isn’t as strong as the first two correlations I mentioned. The correlation between easiness and quality, easiness and helpfulness, and clarity and easiness are also positive, but the correlations are somewhat weak. Out of the possible correlations depicted in the scatterplot, the correlations containing the “raterInterest” variable are the weakest, meaning that a rater’s interest in the subject matter doesn’t influence how they rate their professor.\nQuestion 5\nPart 1: Analyzing The Correlation Between Frequency of Religious Service Attendance and Political Ideology\nSince the religiosity and political ideology are both categorical variables, I will first provide bar charts depicting which political ideology and religiosity choices were most common in the survey.\n\n\n#Data Set\ndata(\"student.survey\")\nggplot(student.survey, aes(x=re)) + geom_bar() + labs(title=\"Religiosity Of Respondents\", x=\"Frequency Of Religious Service Attendance\")\n\n\n\nggplot(student.survey, aes(x=pi)) + geom_bar() + labs(title=\"Political Ideology Of Respondents\", x=\"Political Ideology\")\n\n\n\n\nI suspect that infrequent religious service attendance is correlated with being liberal and frequent religious service attendance is correlated with being conservative because both bar charts are skewed to the left. To get a clearer idea of how correlated the two variables are with each other, I am going to find the central tendencies. Given that both variables are categorical, this will require using mutate() to assign numerical scales to each variable.\n\n\nConverted_Survey <- select(student.survey, re, pi) %>%\n  mutate(re=case_when(\n    re==\"never\"~0,\n    re==\"occasionally\"~1,\n    re==\"most weeks\"~2,\n    re==\"every week\"~3,\n    )) %>%\n  mutate(pi=case_when(\n    pi==\"very liberal\"~-3,\n    pi==\"liberal\"~-2,\n    pi==\"slightly liberal\"~-1,\n    pi==\"moderate\"~0,\n    pi==\"slightly conservative\"~1,\n    pi==\"conservative\"~2,\n    pi==\"very conservative\"~3,\n  ))\nConverted_Survey\n\n\n   re pi\n1   2  2\n2   1 -2\n3   2 -2\n4   1  0\n5   0 -3\n6   1 -2\n7   1 -2\n8   1 -2\n9   1 -3\n10  0 -1\n11  1  1\n12  1 -2\n13  1 -3\n14  1  0\n15  0 -3\n16  3 -2\n17  3 -1\n18  0 -2\n19  0 -2\n20  1 -3\n21  1  0\n22  1 -2\n23  3  3\n24  0  0\n25  0 -3\n26  2 -1\n27  1 -2\n28  2  1\n29  1  0\n30  1 -2\n31  1 -2\n32  1 -2\n33  0 -2\n34  1 -2\n35  1 -3\n36  3  1\n37  0 -2\n38  0 -2\n39  2  1\n40  2  2\n41  1 -2\n42  0 -1\n43  3  2\n44  0  1\n45  1  0\n46  0 -2\n47  1 -1\n48  3  3\n49  0 -2\n50  1 -3\n51  3  1\n52  1  0\n53  1 -2\n54  1 -2\n55  0 -2\n56  3  2\n57  1  0\n58  3 -1\n59  2  0\n60  1  0\n\nNow that I have scales for each response, I will use summary to figure out the central tendencies of each variable. My central tendency findings appear to be consistent with the findings displayed in the bar charts. The median of my religious service attendance data, 1, is slightly smaller than the mean, which is 1.17. This is indicative of the data being subtly skewed towards the left. The political ideology data is also skewed to left because the mean, -0.967, is higher than the median, which is -2, but the skewness of the political ideology data is more pronounced than that of the religious service attendance data.\n\n\nsummary(Converted_Survey, re, pi)\n\n\n       re             pi        \n Min.   :0.00   Min.   :-3.000  \n 1st Qu.:0.75   1st Qu.:-2.000  \n Median :1.00   Median :-2.000  \n Mean   :1.17   Mean   :-0.967  \n 3rd Qu.:2.00   3rd Qu.: 0.000  \n Max.   :3.00   Max.   : 3.000  \n\nNow, I will use regressions to figure out how correlated religious service attendance frequency is with political ideology. Given that religious service attendance frequency could be responsible for shaping political ideologies and politics may be responsible for turning people towards or away from religion, I am going to create two scatter plots with 2 separate regression equations. In the first scatter plot, religious service attendance frequency is the explanatory variable and political ideology is the response variable. In the second scatter plot, the vice versa is true.\n\n\nScenario_1 <- lm(formula = pi~re, data=Converted_Survey)\nprint(Scenario_1)\n\n\n\nCall:\nlm(formula = pi ~ re, data = Converted_Survey)\n\nCoefficients:\n(Intercept)           re  \n    -2.0988       0.9704  \n\nggplot(Converted_Survey, aes(x=re, y=pi)) +  geom_point() + geom_smooth(method=\"lm\") + labs(title = \"Scenario 1: y=0.9704x-2.0988\", x=\"Religiosity\", y=\"Political Ideology\")\n\n\n\n\n\n\nScenario_2 <- lm(formula = re~pi, data=Converted_Survey)\nprint(Scenario_2)\n\n\n\nCall:\nlm(formula = re ~ pi, data = Converted_Survey)\n\nCoefficients:\n(Intercept)           pi  \n     1.5013       0.3461  \n\nggplot(Converted_Survey, aes(x=pi, y=re)) +  geom_point() + geom_smooth(method=\"lm\") + labs(title = \"Scenario 2: y=0.3461x+1.5013\", x=\"Political Ideology\", y=\"Religiosity\")\n\n\n\n\nBased on the scatter plots above, it is safe to assume that being conservative is positively correlated with frequent religious service attendance and frequent religious service attendance is positively correlated with being conservative.\nPart II: Analyzing The Correlation Between Time Spent Watching TV and High School GPA.\nGiven that GPA and time spent watching TV are both quantitative variables, I will start off by displaying the GPA and TV time data as histograms.\n\n\nggplot(student.survey, aes(x=hi)) + geom_histogram(bins=5) + labs(title=\"High School GPA Of Respondents\", x=\"High School GPA\")\n\n\n\nggplot(student.survey, aes(x=tv)) + geom_histogram(bins=5) + labs(title=\"Hours Respondents Spend Watching TV Per Week\", x=\"Hours of TV Watched\")\n\n\n\n\nI suspect that hours spent watching TV per week is negatively correlated with high school GPA because the high school GPA histogram is skewed to the left and the TV time histogram is skewed to the right. I also suspect that both variables contain outliers because GPAs below 3.0 are uncommon and the amount of students who watch less than 10 hours of TV per week far outweighs the amount of students who watch more than 10 hours of TV per week. Before figuring out whether there are outliers, I must figure out the first quartile value for high school GPA, the third quartile value for hours spent watching TV, and the innerquartile ranges for botth variables\n\n\nselect(student.survey, hi, tv) %>%\n  summary()\n\n\n       hi              tv        \n Min.   :2.000   Min.   : 0.000  \n 1st Qu.:3.000   1st Qu.: 3.000  \n Median :3.350   Median : 6.000  \n Mean   :3.308   Mean   : 7.267  \n 3rd Qu.:3.625   3rd Qu.:10.000  \n Max.   :4.000   Max.   :37.000  \n\nsummarize(student.survey, IQR(hi), IQR(tv))\n\n\n  IQR(hi) IQR(tv)\n1   0.625       7\n\nNow that I know the inner quartile range and quartile values for both variables, I can now determine whether there are outliers.\n\n\nTV_IQR=7\nTV_Q3=10\nUpper_TV_Outlier=TV_Q3+(TV_IQR*1.5)\nc(\"TV Time Outlier:\", Upper_TV_Outlier)\n\n\n[1] \"TV Time Outlier:\" \"20.5\"            \n\nHI_IQR=0.625\nHI_Q1=3\nLower_HI_Outlier=HI_Q1-(HI_IQR*1.5)\nc(\"High School GPA Outlier:\", Lower_HI_Outlier)\n\n\n[1] \"High School GPA Outlier:\" \"2.0625\"                  \n\nAccording to my calculations, students with GPAs below 2.0625 and students who watch TV for more than 20 hours per day are considered outliers, so I will create a new version of the dataset that doesn’t contain outliers.\n\n\nNo_Outliers <- select(student.survey, tv, hi) %>%\n  filter(tv <= 20.5) %>%\n  filter(hi >= 2.0625)\n\n\n\nEven with the outliers removed, the high school GPA data remains skewed to the left and the TV data remains skewed to the right. According to the central tendency data below, the median GPA is still higher than the mean GPA and the median amount of TV time remains lower than the mean amount of TV time.\n\n\nselect(student.survey, hi, tv) %>%\n  summary()\n\n\n       hi              tv        \n Min.   :2.000   Min.   : 0.000  \n 1st Qu.:3.000   1st Qu.: 3.000  \n Median :3.350   Median : 6.000  \n Mean   :3.308   Mean   : 7.267  \n 3rd Qu.:3.625   3rd Qu.:10.000  \n Max.   :4.000   Max.   :37.000  \n\nselect(No_Outliers, hi, tv)%>%\n  summary()\n\n\n       hi              tv       \n Min.   :2.100   Min.   : 0.00  \n 1st Qu.:3.000   1st Qu.: 2.75  \n Median :3.400   Median : 5.50  \n Mean   :3.352   Mean   : 6.00  \n 3rd Qu.:3.700   3rd Qu.: 8.50  \n Max.   :4.000   Max.   :16.00  \n\nGiven that the data remains skewed with and without outliers, I suspect that TV time and High School GPA are negatively correlated. In other words, I suspect that students who spend more time watching TV have lower grades than students who spend less time watching TV. To test whether my hypothesis is true, I will create two scatter plots. One of the scatter plots will be based on the data set that includes outliers and the other regression will be based on the data without outliers. Both scatter plots will have TV time as the explanatory variable and GPA as the response variable because it is a lot more probable that watching too much TV causes bad grades.\n\n\nggplot(student.survey, aes(x=tv, y=hi)) + geom_point() + geom_smooth(method=\"lm\") + labs(title=\"Regression With Outliers\", x=\"Hours Spent Watching TV Per Week\", y=\"High School GPA\")\n\n\n\nggplot(No_Outliers, aes(x=tv, y=hi)) + geom_point() + geom_smooth(method=\"lm\") + labs(title=\"Regression Without Outliers\", x=\"Hours Spent Watching TV Per Week\", y=\"High School GPA\")\n\n\n\n\nAs shown in the scatter plots above, the correlation between TV time and GPA being negative regardless of whether outliers are present or not. Visually, it appears that excluding outliers makes the correlation a lot weaker. However, the visual differences are misleading because the slope and y intercept of the data without outliers aren’t significantly different from the slope and y intercept of the data with outliers.\n\n\nFull_Data_Regression <- lm(formula=hi~tv, data=student.survey)\nprint(Full_Data_Regression)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nCoefficients:\n(Intercept)           tv  \n    3.44135     -0.01831  \n\nNo_Outlier_Regression <- lm(formula=hi~tv, data=No_Outliers)\nprint(No_Outlier_Regression)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = No_Outliers)\n\nCoefficients:\n(Intercept)           tv  \n    3.44853     -0.01612  \n\nQuestion 6\nThere is not sufficient evidence to conclude that the tutoring program was successful because according to the concept of regression to the mean, if you do well on one test, you won’t do as well on the next test and at the end of the day, the overall average of test scores will remain the same. Even if this isn’t entirely true, there are spurrious variables that are not being considered such as whether each student studied beforehand. I also determined that the control group got a 72.22 average on the first test and a 71.11 average on the second test using the code below.\n\n\n#First Test Everyone\nEveryone_T1_Mean=70\nEveryone_T1_Total=Everyone_T1_Mean*100\n#First Test Treatment Group\nTreatment_T1_Mean=50\nTreatment_T1_Total=Treatment_T1_Mean*10\n#First Test Control Group\nControl_T1_Total=Everyone_T1_Total-Treatment_T1_Total\nControl_T1_Mean=Control_T1_Total/90\nControl_T1_Mean\n\n\n[1] 72.22222\n\n#Second Test Everyone\nEveryone_T2_Mean=70\nEveryone_T2_Total=Everyone_T2_Mean*100\n#Second Test Treatment Group\nTreatment_T2_Mean=60\nTreatment_T2_Total=Treatment_T2_Mean*10\n#Second Test Control Group\nControl_T2_Total=Everyone_T2_Total-Treatment_T2_Total\nControl_T2_Mean=Control_T2_Total/90\nControl_T2_Mean\n\n\n[1] 71.11111\n\n\n\n\n",
    "preview": "posts/httprpubscomrhyslong96877697/distill-preview.png",
    "last_modified": "2022-03-23T19:46:47-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscommegangeorgesdacss603-hw2/",
    "title": "DACSS 603: Homework 2",
    "description": "Homework # 2 questions and answers for DACSS 603: Introduction to Quantitative Analysis",
    "author": [
      {
        "name": "Megan Georges",
        "url": {}
      }
    ],
    "date": "2022-03-23",
    "categories": [],
    "contents": "\r\nQuestion 1:\r\n(Problem 1.1 in ALR) United Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n1.1.1. Identify the predictor and the response.\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nAnswer to Question 1:\r\n\r\n\r\n# Load dataset\r\ndata(\"UN11\") \r\n\r\n# Select variables of focus\r\nUN11 <- UN11 %>%\r\n  select(c(ppgdp, fertility))  \r\n\r\n# Preview data\r\nhead(UN11)\r\n\r\n\r\n              ppgdp fertility\r\nAfghanistan   499.0     5.968\r\nAlbania      3677.2     1.525\r\nAlgeria      4473.0     2.142\r\nAngola       4321.9     5.135\r\nAnguilla    13750.1     2.000\r\nArgentina    9162.1     2.172\r\n\r\n1.1.1\r\nThe predictor variable is ppgdp (gross national product per person, in US dollars) and the response variable is fertility (birth rate per 1000 females).\r\n1.1.2\r\n\r\n\r\n# Create scatterplot\r\n# fertility on vertical axis, ppgdp on horizontal axis\r\nplot(x = UN11$ppgdp, y = UN11$fertility, xlab = 'ppgdp', ylab = 'fertility', main = 'Scatterplot for Question 1.1.2')\r\n\r\n\r\n\r\n\r\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at first (up to about $10000 ppgdp), then there appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph.\r\n1.1.3\r\n\r\n\r\n# Create scatterplot\r\n# log(fertility) on vertical axis, log(ppgdp) on horizontal axis\r\nplot(x = log(UN11$ppgdp), y = log(UN11$fertility), xlab = 'log(ppgdp)', ylab = 'log(fertility)', main = 'Scatterplot for Question 1.1.3')\r\n\r\n\r\n\r\n\r\nThe simple linear regression seems plausible for summary of this graph. The relationship between the variables (when a log-scale is applied) appears to be negative and rather consistent throughout the graph (as opposed to the graph in 1.1.2, which has a dramatic drop at first then plateaus for the majority of the plot).\r\nQuestion 2:\r\n(Problem 9.47 in SMSS) Annual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\nHow, if at all, does the slope of the prediction equation change?\r\nHow, if at all, does the correlation change?\r\nAnswer to Question 2:\r\na.\r\nThe slope of the prediction equation would change. It would be the initial version’s slope divided by 1.33 to account for the change in unit to pounds.\r\nb.\r\nThe correlation does not change, because it standardizes the slope (thus is not impacted by unit of measure).\r\nQuestion 3:\r\n(Problem 1.5 in ALR) Water runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\nAnswer to Question 3:\r\n\r\n\r\n# load and preview dataset \r\ndata(water)\r\nhead(water)\r\n\r\n\r\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\r\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\r\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\r\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\r\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\r\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\r\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\r\n\r\n\r\n\r\n# create scatterplot matrix\r\npairs(water, main = 'Scatterplot Matrix for Question 3')\r\n\r\n\r\n\r\n\r\nLooking at this scatterplot matrix, it appears that precipitation levels for the ‘A’ named lakes seem to have a positive (relatively linear) correlation (although unsure how strong) with each other and the ‘O’ named lakes seem to have one as well with each other. The year variable does not appear to have a relationship to any of the variables. Also, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes.\r\nQuestion 4:\r\n(Problem 1.6 in ALR - slightly modified) Professor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\nAnswer to Question 4:\r\n\r\n\r\n# load dataset, select variables, preview dataset\r\ndata(Rateprof)\r\n\r\nRateprof <- Rateprof %>%\r\n  select(c(quality, clarity, helpfulness, easiness, raterInterest))  \r\n\r\nhead(Rateprof)\r\n\r\n\r\n   quality  clarity helpfulness easiness raterInterest\r\n1 4.636364 4.636364    4.636364 4.818182      3.545455\r\n2 4.318182 4.090909    4.545455 4.363636      4.000000\r\n3 4.790698 4.860465    4.720930 4.604651      3.432432\r\n4 4.250000 4.041667    4.458333 2.791667      3.181818\r\n5 4.684211 4.684211    4.684211 4.473684      4.214286\r\n6 4.233333 4.200000    4.266667 4.533333      3.916667\r\n\r\n\r\n\r\n# create scatterplot matrix\r\npairs(Rateprof, main = 'Plot for Question 4')\r\n\r\n\r\n\r\n\r\nReferring to the scatterplot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables. There are a few notable outliers in the matrix, for example the data point rating higher for clarity and lower for quality than the trend of other points on the clarity/quality plot. The variables with stronger correlations to each other may suggest that there is a relationship between certain qualities in the professors from the selected university (like professors that tend to be perceived as more helpful by students also tend to have higher clarity) or this could mean that students associate these certain qualities together (thus rating similarly for helpfulness and clarity).\r\nQuestion 5:\r\n(Problem 9.34 in SMSS) For the student.survey data file in the smss package, conduct regression analyses relating:\r\ny = political ideology and x = religiosity,\r\n\r\ny = high school GPA and x = hours of TV watching.\r\n\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\nUse graphical ways to portray the individual variables and their relationship.\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\nSummarize and interpret results of inferential analyses.\r\nAnswer to Question 5:\r\n\r\n\r\n# load and preview data\r\ndata(student.survey)\r\n\r\nhead(student.survey)\r\n\r\n\r\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\r\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\r\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\r\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\r\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\r\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\r\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal\r\n            re    ab    aa    ld\r\n1   most weeks FALSE FALSE FALSE\r\n2 occasionally FALSE FALSE    NA\r\n3   most weeks FALSE FALSE    NA\r\n4 occasionally FALSE FALSE FALSE\r\n5        never FALSE FALSE FALSE\r\n6 occasionally FALSE FALSE    NA\r\n\r\na.\r\n\r\n\r\n# graph: x=religiosity, y=political ideology\r\nrepa <- student.survey %>%\r\n  select(re, pi)\r\n\r\nggplot(data = repa) +\r\n  geom_bar(mapping = aes(x = re, fill = pi)) +\r\n  labs(title = \"Relationship between Religiosity and Political Ideology\", x = \"Religiosity (how often you attend services)\", y = \"Political Ideology (pi)\") \r\n\r\n\r\n\r\n\r\n\r\n\r\n# graph: x=hours of watching tv, y=high school gpa\r\ntvhi <- student.survey %>%\r\n  select(tv, hi)\r\n\r\nggplot(data = tvhi) +\r\n  geom_point(mapping = aes(x = tv, y = hi)) +\r\n  labs(title = \"Relationship between Hours Watching TV and GPA\", x = \"Average Hours of TV watched per Week\", y = \"High School GPA\")\r\n\r\n\r\n\r\n\r\nb.\r\n\r\n\r\n# descriptive statistics: religiosity and political ideology\r\nsummary(repa)\r\n\r\n\r\n            re                         pi    \r\n never       :15   very liberal         : 8  \r\n occasionally:29   liberal              :24  \r\n most weeks  : 7   slightly liberal     : 6  \r\n every week  : 9   moderate             :10  \r\n                   slightly conservative: 6  \r\n                   conservative         : 4  \r\n                   very conservative    : 2  \r\n\r\nBoth the religiosity and political ideology variables are skewed right, with significantly higher counts for “never” and “occasional” service attendance and “liberal” and “moderate” political ideologies in respondents.\r\n\r\n\r\n# descriptive statistics: hours of tv watched and GPA\r\nsummary(tvhi)\r\n\r\n\r\n       tv               hi       \r\n Min.   : 0.000   Min.   :2.000  \r\n 1st Qu.: 3.000   1st Qu.:3.000  \r\n Median : 6.000   Median :3.350  \r\n Mean   : 7.267   Mean   :3.308  \r\n 3rd Qu.:10.000   3rd Qu.:3.625  \r\n Max.   :37.000   Max.   :4.000  \r\n\r\nThe variable average hours of tv watched has a wide range, and the large distance between the 3rd quantile and maximum suggest that there is at least one outlier (which there are multiple when viewing the scatterplot in part a.). Additionally, the median being less than the mean suggests a right skew in the data. The summary for the high school gpa suggests a relatively normal distribution, as the mean and median are similar and lie relatively in the center of the range. However, the gpa is skewed left (although the mode lies directly in the center of the range, there is a higher count of individuals with a gpa above 3.0, as reflected by the mean being above the mode).\r\nc.\r\n\r\n\r\n# inferential analysis: religiosity and political ideology\r\nlmrepa <- lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re))\r\nsummary(lmrepa)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.81243 -0.87160  0.09882  1.12840  3.09882 \r\n\r\nCoefficients:\r\n               Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \r\nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.345 on 58 degrees of freedom\r\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \r\nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\r\n\r\n# looking at Pearson's correlation\r\ncor.test(as.numeric(student.survey$re), as.numeric(student.survey$pi))\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  as.numeric(student.survey$re) and as.numeric(student.survey$pi)\r\nt = 5.4163, df = 58, p-value = 1.221e-06\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 0.3818345 0.7265650\r\nsample estimates:\r\n      cor \r\n0.5795661 \r\n\r\nDue to the political ideology and religiosity variables being categorical, we need to use the as.numeric argument in the linear model to convert the variables into numerical data. At a significance level of .01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\r\n\r\n\r\n# inferential analysis: hours of tv and high school gpa\r\nlmtvhi <- lm(data = student.survey, formula = hi ~ tv)\r\nsummary(lmtvhi)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = hi ~ tv, data = student.survey)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.2583 -0.2456  0.0417  0.3368  0.7051 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\r\ntv          -0.018305   0.008658  -2.114   0.0388 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4467 on 58 degrees of freedom\r\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \r\nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\r\n\r\n# looking at Pearson's correlation\r\ncor.test(student.survey$tv, student.survey$hi)\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  student.survey$tv and student.survey$hi\r\nt = -2.1144, df = 58, p-value = 0.03879\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n -0.48826914 -0.01457694\r\nsample estimates:\r\n       cor \r\n-0.2675115 \r\n\r\nWith a slope of -.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of .05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatterplot with hours of tv watched a GPA, since there does not appear to be a linear trend in the data.\r\nQuestion 6:\r\n(Problem 9.50 in SMSS) For a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\nAnswer to Question 6:\r\nApplying the concept of regression toward the mean to this example, the low midterm scores in the sample could be explained as being an extreme in the sample by chance. Thus, in the next sample (in this case the final exam), we can expect that those 10 students’ scores will be closer to the mean this time (which remained to be 70, an average larger than the tutored students’ midterm score). Thus, we cannot conclude that the tutoring program was the cause of increase in the 10 students’ test scores.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscommegangeorgesdacss603-hw2/distill-preview.png",
    "last_modified": "2022-03-23T19:46:54-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomtpaske878207/",
    "title": "Homework #2",
    "description": "A new article created using the Distill format.",
    "author": [
      {
        "name": "Tyler J Paske",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-03-23",
    "categories": [],
    "contents": "\r\n(Problem 1.1 in ALR)\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, (the gross national product per person in U.S. dollars), and fertility, (the birth rate per 1000 females), both from the year 2009.The data is for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries.The data was collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.Identify the predictor and the response.“x” The Predictor in this case would be the ppgdp (gross national product per person) “y” and The Response is fertility (the birth rate per 1000 female)\r\nQuestion 1.1\r\nIdentify the predictor and the response.\r\n                                                  ANSWER\r\nI started by first loading the required package alr4 to which I then was able to plot my data as shown below.\r\nCode: ##plot(x = UN11\\(ppgdp, y = UN11\\)fertility)\r\n\r\n\r\n\r\nQuestion 1.2\r\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n                                                  ANSWER\r\nAs the plotted data below demostates a natural linear regression I would conclude that yes the simple linear regression model does seem plausible for a summary of this graph.\r\nCode: ##plot(log(UN11\\(fertility)~log(UN11\\)ppgdp), ylab =“UN11\\(fertility\", xlab = \"UN11\\)ppgdp”)\r\n\r\n\r\n\r\nQuestion 1.3\r\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\n                                                ANSWER\r\nIt seems as though that the graph above as a result shows a good summary as the average or the mean of the data supports linearity except for a few outliers.\r\nQuestion 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n(a)\r\nHow, if at all, does the slope of the prediction equation change?\r\n                                               ANSWER\r\nThe prediction equation: yˆ = a + bx, provides a prediction yˆ for every value of x or in other words for every value of x, we can calculate a y value. x being the explanatory variable (British pound) and yˆ being the estimated response outcome aka the prediction. In conclusion the slope of the prediction equation changes by whatever the prediction value of “y” is.\r\n(b)\r\nHow, if at all, does the correlation change?\r\n                                               ANSWER\r\nThe correlation changes between variables as one of the variables changes in value. To test the correlation, one needs to make predictions that are testable. Depending on the prediction, the correlation can change to a negative or positive correlation. In summary the correlation changes by the prediction.\r\nQuestion 3\r\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\n\r\n                                             ANSWER\r\nYear appears to be largely unrelated to each of the other variables; All streams seem to positively correlate with each other however all streams starting with O don’t seem to correlate with streams starting with A showing that the water can be correlated to other streams but can’t be predicted from past years of where the water comes from.\r\nQuestion 4\r\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n                                           ANSWER\r\nWe start by loading the data and reviewing the matrix\r\n##data(Rateprof)\r\n##pairs(Rateprof)\r\nAs the matrix stands we need to further scrape the data to reproduce the scatterplot matrix in Figure 1.13 in the ALR book. To do this we need to further identify the data frame that we want to work with as the there too much data currently within the matrix. The data frame itself holds 17 columns, we only need the 8th – 12th. So we use the code below to give us the reproduced scatterplot matrix.\r\nCode: pairs(Rateprof[,8:12])\r\n\r\n\r\n\r\nQuestion 5\r\nFor the student.survey data file in the smss package, conduct regression analyses relating\r\n(i)\r\ny = political ideology and x = religiosity,\r\nStep 1 • Install the package & the data file\r\n\r\nlibrary(smss)\r\n\r\n\r\ndata(“student.survey”)\r\n\r\nBoth variables contain categorical data.\r\n• Graph the variables and their relationship.\r\n                                      Can't Grpah within chunk\r\nCode: >ggplot(data = student.survey, aes(x = re, y = pi)) + geom_point() + xlab(“Religiosity”) + ylab(“Political Ideology”)\r\n(ii)\r\ny = high school GPA and x = hours of TV watching.\r\n                                         Can't Grpah within chunk\r\n                                         \r\nCode: >ggplot(data = student.survey, aes(x = tv, y = hi)) + geom_point() + xlab(“Hours of TV watching”) + ylab(“High school GPA”)\r\nAs we can clearly see from the graph above there’s a big correlation between the number of hours a high school student watches tv and their GPA in high school. The graph above shows that the fewer hours a student in high school watches TV the higher his/her GPA becomes.\r\n(a)\r\nUse graphical ways to portray the individual variables and their relationship.\r\nANSWER\r\nIn terms of the relationship between political ideology and religiosity, it seems as though graphically there’s a positive relationship between the two in that the more a student practices religion the more conservative he/she becomes over the course of a week.\r\n(b)\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\nCODE: >summary(student.survey)\r\n                                   Can't Grpah within chunk\r\nANSWER\r\nAfter running a summary of the student survey data, I was able to find many descriptive statistics to summarize the individual variable. First being in political ideology where I found that 63% of all 60 observations showed to be that the students were at some degree of liberal. Additionally, I also found that 73% of students either never went to church or occasionally went to church. With only 27% of the all the students that go to church most weeks and every week we can start to get a much better sense of the relationship between the two. Should more students go to church most to every week then we would probably see a much bigger favor of students being conservative.\r\nIn terms of the relationship between High School GPA and Hours of watching TV, the descriptive statistics shown from a summary show that on overage students with a 3.3GPA watch an average a little more than 7 hours a week.\r\n(c)\r\nSummarize and interpret results of inferential analyses.\r\nANSWER\r\nTo further explore this data (number of hours a high school student watches tv and their GPA) in terms of an inferential analysis, I turn to a summary of two variable at a time. Doing so would give me insight to the T – Value and P Score. Calculated from the sample data in a hypothesis test we calculate the test statistic under the null hypothesis. We find that there’s a very low T- Value (-2.11). It shows that it has no significance for the difference between the two variables. The smaller the t – score the more the groups hold a similarity which was confirmed and articulated from the visualization.\r\nThe p-value being .04 rounded indicates that the result is statistically significant. In this case we would reject the null hypothesis and lean in favor of the alternative hypothesis. Being in favor of the alternative hypothesis we can conclude that there is some statistical significance between the two measured phenomenon.\r\nCODE: >cor.test(student.survey\\(tv, student.survey\\)hi)\r\nIn terms of the relationship between political ideology and religiosity there has to be a numerical vector in order to conduct an inferential analysis.\r\n\r\nlibrary(smss)\r\n\r\n\r\ndata(“student.survey”)\r\n\r\n\r\ncor.test(student.survey\\(re, student.survey\\)pi)\r\n\r\nError in cor.test.default(student.survey\\(re, student.survey\\)pi) : ‘x’ must be a numeric vector\r\nQuestion 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60.\r\nUse the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\n                                            ANSWER\r\nThis is not sufficient evidence because what we’re doing is simply taking the average of the average. The overall class average is 70 for two tests but the specially tutored students increase from 50 to 60. How do we know if the specially tutored students would have had the same result not tutored? What if the tutoring had no effect on the students whatsoever? What we’re discussing is regression to the mean and regression to the mean shows that should you have a bad result in say a test, there’s a high likelihood that your results will be better the next time around. However, we shouldn’t expect special tutored students to be better or worse the second time around because of random chance variables that are unforeseen. As mentioned from the video, this is why it’s so important to say use control groups in clinical trials. This way we can see if drugs work better or worse by random chance.\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomtpaske878207/distill-preview.png",
    "last_modified": "2022-03-23T19:47:00-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomowenvespa874506/",
    "title": "Simple Linear Regression",
    "description": "DACSS 603 Homework 2",
    "author": [
      {
        "name": "Rhowena Vespa",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\r\n\r\n\r\nlibrary(distill)\r\nlibrary(dplyr)\r\nlibrary(tidyverse)\r\nlibrary(knitr)\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nlibrary(smss)\r\n\r\n\r\n\r\nQuestion 1\r\n(Problem 1.1 in ALR)\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n\r\n\r\ndata(\"UN11\") # load the UN11 data\r\nUN11 <- UN11 %>%\r\n  select(c(fertility,ppgdp)) # select the two variables to use \r\ndim(UN11)\r\n\r\n\r\n[1] 199   2\r\n\r\nkable(head(UN11), format = \"markdown\", digits = 10, caption = \"**Dependence of Fertility on ppgdp**\")\r\n\r\n\r\nTable 1: Dependence of Fertility on ppgdp\r\n\r\nfertility\r\nppgdp\r\nAfghanistan\r\n5.968\r\n499.0\r\nAlbania\r\n1.525\r\n3677.2\r\nAlgeria\r\n2.142\r\n4473.0\r\nAngola\r\n5.135\r\n4321.9\r\nAnguilla\r\n2.000\r\n13750.1\r\nArgentina\r\n2.172\r\n9162.1\r\n\r\n1.1.1. Identify the predictor and the response. \r\nPredictor is ppgdp, response is fertility.\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\nSee Figure 1.1 below. From this graph, a straight line mean function DOES NOT SEEM PLAUSIBLE for a summary of this graph. It looks like there is a negative correlation between ppgdp and fertility but we will need the lm function to visualize it.\r\n\r\n\r\nggplot(UN11, aes(x = ppgdp, y = fertility)) +\r\n    geom_point(color=2) + \r\n    labs(x=\"ppgdp-Gross National Product Per Person in U.S. dollars\", y=\"fertility-birth rate per 1000 females\", title = \"FIGURE 1.1 UN ppgdp vs fertility data in 2009\")\r\n\r\n\r\n\r\n\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nSee Figure 1.2 below. Yes, a simple linear regression model looks more plausible on this graph. Using the log function produced a graph with a more linear relationship between ppgdp and fertility.\r\n\r\n\r\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\r\n    geom_point(color=2) + \r\n    geom_smooth(method = \"lm\") +\r\n    labs(x=\"ppgdp-Gross National Product Per Person in U.S. dollars\", y=\"fertility-birth rate per 1000 females\", title = \"FIGURE 1.2 UN ppgdp and fertility data in 2009\")\r\n\r\n\r\n\r\n\r\nQuestion 2\r\n(Problem 9.47 in SMSS)\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n(a) How, if at all, does the slope of the prediction equation change?\r\nThe slope will change when responses are converted to British pounds. The new slope of the prediction equation when explanatory variable is in British pounds will be LESS than original slope (in US dollars).\r\n(b) How, if at all, does the correlation change?\r\nNo, a change in units on the explanatory variables from US Dollar to British pounds will not result in a correlation change. Correlation DOES NOT depend on the variable’s units.\r\nQuestion 3\r\n(Problem 1.5 in ALR)\r\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\ndata(\"water\") # load the water data\r\ndim(water)\r\n\r\n\r\n[1] 43  8\r\n\r\nkable(head(water), format = \"markdown\", digits = 10, caption = \"**Water runoff in the Sierras**\")\r\n\r\n\r\nTable 2: Water runoff in the Sierras\r\nYear\r\nAPMAM\r\nAPSAB\r\nAPSLAKE\r\nOPBPC\r\nOPRC\r\nOPSLAKE\r\nBSAAM\r\n1948\r\n9.13\r\n3.58\r\n3.91\r\n4.10\r\n7.43\r\n6.47\r\n54235\r\n1949\r\n5.28\r\n4.82\r\n5.20\r\n7.55\r\n11.11\r\n10.26\r\n67567\r\n1950\r\n4.20\r\n3.77\r\n3.67\r\n9.52\r\n12.20\r\n11.35\r\n66161\r\n1951\r\n4.60\r\n4.46\r\n3.93\r\n11.14\r\n15.15\r\n11.13\r\n68094\r\n1952\r\n7.15\r\n4.99\r\n4.88\r\n16.34\r\n20.05\r\n22.81\r\n107080\r\n1953\r\n9.70\r\n5.65\r\n4.91\r\n8.88\r\n8.15\r\n7.41\r\n67594\r\n\r\npairs(water,col = 2, main = \"Water Runoff in Sierras Scatterplot Matrix\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nwater1 <-lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water)\r\nsummary(water1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \r\n    OPSLAKE, data = water)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-12690  -4936  -1424   4173  18542 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\r\nAPMAM         -12.77     708.89  -0.018 0.985725    \r\nAPSAB        -664.41    1522.89  -0.436 0.665237    \r\nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \r\nOPBPC          69.70     461.69   0.151 0.880839    \r\nOPRC         1916.45     641.36   2.988 0.005031 ** \r\nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 7557 on 36 degrees of freedom\r\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \r\nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\r\n\r\nANALYSIS:\r\nOn residuals, although there is a big disparity between the min and max (-12690 and 18542), which can be possible outliers, I think the data is relatively balanced because the 1Q and 3Q (-4936 and 4173) are close in values.\r\nOn coefficients, sites OPRC and OPSLAKE has statistically significant values of Pr(>|t|) < 0.05, indicating that these two locations’ precipitation measurements are significant to BASAAM stream runoff volume.\r\nMultiple R-squared 0.9248 and adjusted R-squared 0.9123 are relatively close, suggesting model in not over-fitted. An adjusted R-squared of 0.9123 indicates a good fit for the model.\r\nLastly, with a p-value: < 2.2e-16, we can conclude that this model is statistically significant. This model can be used to predict runoff so engineers, planners, and policy makers could do their jobs more efficiently.\r\nQuestion 4\r\n(Problem 1.6 in ALR - slightly modified)\r\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20).\r\n\r\n\r\ndata(Rateprof)\r\nRateprof <- Rateprof %>%\r\n  select(c(quality,helpfulness,clarity,easiness,raterInterest)) # select the five variables to use\r\ndim(Rateprof)\r\n\r\n\r\n[1] 366   5\r\n\r\nkable(head(Rateprof), format = \"markdown\", digits = 10, col.names = c('Quality','Helpfulness','Clarity', 'Easiness', 'Rater Interest'), caption = \"**Professor Ratings**\")\r\n\r\n\r\nTable 3: Professor Ratings\r\nQuality\r\nHelpfulness\r\nClarity\r\nEasiness\r\nRater Interest\r\n4.636364\r\n4.636364\r\n4.636364\r\n4.818182\r\n3.545455\r\n4.318182\r\n4.545455\r\n4.090909\r\n4.363636\r\n4.000000\r\n4.790698\r\n4.720930\r\n4.860465\r\n4.604651\r\n3.432432\r\n4.250000\r\n4.458333\r\n4.041667\r\n2.791667\r\n3.181818\r\n4.684211\r\n4.684211\r\n4.684211\r\n4.473684\r\n4.214286\r\n4.233333\r\n4.266667\r\n4.200000\r\n4.533333\r\n3.916667\r\n\r\npairs(Rateprof,col = 2, main = \"Professor Ratings Scatterplot Matrix\")\r\n\r\n\r\n\r\n\r\nProvide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\nBased on this scatterplot matrix, we can observe a very strong linear positive correlation between: quality and helpfulness, quality and clarity, helpfulness and clarity.\r\nThere is moderate positive linear correlation between easiness and quality, clarity or helpfulness.\r\nThere is moderate positive linear correlation between raterinterest and clarity, helpfulness or quality\r\nEasiness and raterinterest have a weak positive linear association.\r\n\r\n\r\nRP <-cor(Rateprof, use = \"all.obs\",method = c(\"pearson\", \"kendall\", \"spearman\"))\r\nkable ((RP), format = \"markdown\", digits = 10, col.names = c('Quality','Helpfulness','Clarity', 'Easiness', 'Rater Interest'), caption = \"**Correlation Matrix**\")\r\n\r\n\r\nTable 4: Correlation Matrix\r\n\r\nQuality\r\nHelpfulness\r\nClarity\r\nEasiness\r\nRater Interest\r\nquality\r\n1.0000000\r\n0.9810314\r\n0.9759608\r\n0.5651154\r\n0.4706688\r\nhelpfulness\r\n0.9810314\r\n1.0000000\r\n0.9208070\r\n0.5635184\r\n0.4630321\r\nclarity\r\n0.9759608\r\n0.9208070\r\n1.0000000\r\n0.5358884\r\n0.4611408\r\neasiness\r\n0.5651154\r\n0.5635184\r\n0.5358884\r\n1.0000000\r\n0.2052237\r\nraterInterest\r\n0.4706688\r\n0.4630321\r\n0.4611408\r\n0.2052237\r\n1.0000000\r\n\r\nQuestion 5\r\n(Problem 9.34 in SMSS)\r\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\n\r\n\r\ndata(\"student.survey\") # load the student.survey data\r\nstudent.survey <- student.survey %>%\r\n  select(c(re,pi,hi,tv)) # select the four variables to use \r\ndim(student.survey)\r\n\r\n\r\n[1] 60  4\r\n\r\nkable(head(student.survey), format = \"markdown\", digits = 2, col.names = c('Religiosity','Political Ideology','HS GPA', 'Hrs of watching TV'), caption = \"**Student Survey Data**\")\r\n\r\n\r\nTable 5: Student Survey Data\r\nReligiosity\r\nPolitical Ideology\r\nHS GPA\r\nHrs of watching TV\r\nmost weeks\r\nconservative\r\n2.2\r\n3\r\noccasionally\r\nliberal\r\n2.1\r\n15\r\nmost weeks\r\nliberal\r\n3.3\r\n0\r\noccasionally\r\nmoderate\r\n3.5\r\n5\r\nnever\r\nvery liberal\r\n3.1\r\n6\r\noccasionally\r\nliberal\r\n3.5\r\n4\r\n\r\nA. Graph of Individual Variables and their Relationship\r\n(i) y = political ideology and x = religiosity\r\n\r\n\r\nggplot(student.survey, aes(x=re,ymin = 0, ymax = 30, fill=pi)) +\r\n  geom_bar() +\r\n  labs(x=\"Religiosity\", y=\"Political Ideology\", \r\n  title = \"FIGURE 2. Political Ideology and Religiosity\") +\r\n  facet_wrap(vars(re,pi),strip.position = \"left\") +\r\n  theme(axis.text.x = element_text(size = 8, angle = 90)) \r\n\r\n\r\n\r\n\r\n\r\n\r\nstudent.survey %>%\r\n    count(pi,re, sort = TRUE) %>%\r\n  kable(head(10), format = \"markdown\", digits = 10, col.names = c('Political Ideology','Religiosity','Number of Students'), caption = \"**Political Ideology and Religiosity Matrix Count**\")\r\n\r\n\r\nTable 6: Political Ideology and Religiosity Matrix Count\r\n\r\nPolitical Ideology\r\nReligiosity\r\nNumber of Students\r\n1\r\nliberal\r\noccasionally\r\n14\r\n2\r\nliberal\r\nnever\r\n8\r\n3\r\nmoderate\r\noccasionally\r\n8\r\n4\r\nvery liberal\r\noccasionally\r\n5\r\n5\r\nvery liberal\r\nnever\r\n3\r\n6\r\nslightly liberal\r\nnever\r\n2\r\n7\r\nslightly liberal\r\nevery week\r\n2\r\n8\r\nslightly conservative\r\nmost weeks\r\n2\r\n9\r\nslightly conservative\r\nevery week\r\n2\r\n10\r\nconservative\r\nmost weeks\r\n2\r\n11\r\nconservative\r\nevery week\r\n2\r\n12\r\nvery conservative\r\nevery week\r\n2\r\n13\r\nliberal\r\nmost weeks\r\n1\r\n14\r\nliberal\r\nevery week\r\n1\r\n15\r\nslightly liberal\r\noccasionally\r\n1\r\n16\r\nslightly liberal\r\nmost weeks\r\n1\r\n17\r\nmoderate\r\nnever\r\n1\r\n18\r\nmoderate\r\nmost weeks\r\n1\r\n19\r\nslightly conservative\r\nnever\r\n1\r\n20\r\nslightly conservative\r\noccasionally\r\n1\r\n\r\n(ii) y = high school GPA and x = hours of TV watching.\r\n\r\n\r\nggplot(student.survey, aes(x = tv, y = hi)) +\r\n    geom_point(color=2) + \r\n    geom_smooth(method = \"lm\") +\r\n    labs(x=\"Hrs of TV\", y=\"HS GPA\", title = \"FIGURE 3. HS GPA and Hrs of TV\")\r\n\r\n\r\n\r\n\r\nB. Interpret Descriptive Statistics of Variables and their Relationship.\r\n\r\n\r\nsummary(student.survey)\r\n\r\n\r\n            re                         pi           hi       \r\n never       :15   very liberal         : 8   Min.   :2.000  \r\n occasionally:29   liberal              :24   1st Qu.:3.000  \r\n most weeks  : 7   slightly liberal     : 6   Median :3.350  \r\n every week  : 9   moderate             :10   Mean   :3.308  \r\n                   slightly conservative: 6   3rd Qu.:3.625  \r\n                   conservative         : 4   Max.   :4.000  \r\n                   very conservative    : 2                  \r\n       tv        \r\n Min.   : 0.000  \r\n 1st Qu.: 3.000  \r\n Median : 6.000  \r\n Mean   : 7.267  \r\n 3rd Qu.:10.000  \r\n Max.   :37.000  \r\n                 \r\n\r\nINTERPRETATION:\r\nReligiosity: Mode is Occasionally (count: 29) Political Ideology:Mode is liberal (count:24). Based on this, a visualization of re vs pi (see figure 2) showed that this population had mostly occasional religiosity and liberal ideology as the mode.\r\nHS GPA: Mean GPA was 3.3 and very close to median of 3.35. The min GPA was 2.0 while max of 4.0. Hrs of Watching TV: On average, HS students watch TV for 7.267 hrs but most students watch for 6 hours. The min was 0 hrs while the maximum was 37 hrs. Based on Figure 3, there is an association between HS GPA and Hrs of TV watched.\r\nC. Summarize and Interpret results of Inferential Analyses\r\n(i) LOGISTIC REGRESSION: y = political ideology and x = religiosity\r\n\r\n\r\nSS.glm.fit <- glm(re ~ pi, data = student.survey, family = binomial)\r\nsummary(SS.glm.fit)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = re ~ pi, family = binomial, data = student.survey)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-2.1460  -0.3500   0.5314   0.9005   0.9695  \r\n\r\nCoefficients:\r\n             Estimate Std. Error z value Pr(>|z|)\r\n(Intercept)    5.8337   489.4505   0.012    0.990\r\npi.L          16.2199  1753.3896   0.009    0.993\r\npi.Q           8.1491  1526.1299   0.005    0.996\r\npi.C          -0.2996  1398.7211   0.000    1.000\r\npi^4          -4.6817  1304.7376  -0.004    0.997\r\npi^5          -5.0032   915.6782  -0.005    0.996\r\npi^6          -3.3188   401.1467  -0.008    0.993\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 67.480  on 59  degrees of freedom\r\nResidual deviance: 60.684  on 53  degrees of freedom\r\nAIC: 74.684\r\n\r\nNumber of Fisher Scoring iterations: 16\r\n\r\nINTERPRETATION\r\nBased on all the p-values, we fail to reject the null hypothesis and conclude that there is no association between religiosity and political ideology.\r\n(ii) LINEAR REGRESSION: y = high school GPA and x = hours of TV watching\r\n\r\n\r\nstudent.surveyii <-lm(hi ~ tv, data = student.survey)\r\nsummary(student.surveyii)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = hi ~ tv, data = student.survey)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.2583 -0.2456  0.0417  0.3368  0.7051 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\r\ntv          -0.018305   0.008658  -2.114   0.0388 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4467 on 58 degrees of freedom\r\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \r\nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\r\n\r\n\r\n\r\nstudent.survey1 <- student.survey %>%\r\n  select(c(hi,tv))\r\ncor(student.survey1)\r\n\r\n\r\n           hi         tv\r\nhi  1.0000000 -0.2675115\r\ntv -0.2675115  1.0000000\r\n\r\nINTERPRETATION\r\nBased on the p-value 0.0388, we reject the null hypothesis. Therefore, we can conclude that there is statistically significant association between HS GPA and hours of watching TV. However, with very low R-squared (0.07156), this model is not a good fit to explain variations in the data.\r\nBased on the correlation values, we can conclude a weak negative association between HS GPA and hrs of watching TV.\r\nQuestion 6\r\n(Problem 9.50 in SMSS)\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\r\nEXPLANATION:\r\nChoose 10 lowest scoring students (mean=50) and their scores will regress upward AFTER a tutoring program, NOT BECAUSE OF. These students who did poorly were unlucky. It is highly probable that these students would have increased their final exam scores (mean =60), with or without the tutoring program. The effect of the tutoring program on the test scores is INCONCLUSIVE. The same rationale can be used regarding highest scoring students during midterms. Chances are the same high performing students who scored higher during midterms will then score closer to the mean (mean=70) during their final exams. Both the lowest scoring students and the highest scoring students had their scores REGRESS TOWARDS THE MEAN of 70 in the final exam.\r\nTo determine if the tutoring program is effective in improving test scores, the design of the study should choose the sample student population RANDOMLY, and conduct the test in a CONTROLLED environment. Otherwise, we must consider regression to the mean as the rationale for the increase in test scores of the lowest performing group.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomowenvespa874506/distill-preview.png",
    "last_modified": "2022-03-12T19:36:09-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httprpubscomemersonflemi875398/",
    "title": "EFleming DACSS 603 HW II",
    "description": "The following document contains my work for DACSS 603 HW II.",
    "author": [
      {
        "name": "EFleming",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nQuestion 1\n(Problem 1.1 in ALR)\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\nAnswer\nThe predictor is ppgdp and the response is fertility. Fertility depends on ppgdp. This question is a bit tricky and had me confused at first as it is easy to make the mistake of making fertility the predictor influencing “y.” However, based on my logic, I would argue it is the inverse as my answer states.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\nAnswer\n\n\nattach(UN11)\nplot(fertility~ppgdp, data=UN11)\n\n\n\n\n*The relationship does not seem linear. There is an inverse and therefore negative relationship. A straight line function would not be plausible in this case. We cannot draw a solid straight line without making changes to the variables themselves as we can with logs (which we will use next).\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nAnswer\n\n\nattach(UN11)\nplot(log(fertility)~log(ppgdp), data=UN11)\n\n\n\n\n*Yes, the log transformation created two variables that are linearly related. The log helps create a better fit for our data. With the logs, we transform the variables. The line generated using the lines is much better and more smooth when we implement the logs. The log transformation creates two variables that are linearly related. We can use simple linear regression to estimate the effect of log(ppgdp on log(fertility).\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\nHow, if at all, does the slope of the prediction equation change?\nAnswer\nWe want to take the income in dollars and make it pounds. In other words $1330 = 1.33 x 1000 for example. I\ny= B0 + B1 Inc($) + E y= B0 + B1(Inc(pounds x 1.33)) + E\nTherefore it can be written as: y= B0 + 1.33B1Inc(pounds) + E ^^^We get a new slope which is 1.33 times the old slope\nHow, if at all, does the correlation change?\nAnswer\nThe correlation does not change at all.\nQuestion 3\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\nWe want to see how correlated each of the sites are to one another in order to see if this can help us predict Steam Runoff.\nAnswer\n\n\nplot(BSAAM~APMAM, data = water) \n\n\n\n\n\n\nplot(BSAAM~APSAB,data = water)\n\n\n\n\n\n\nplot(BSAAM~APSLAKE,data = water)\n\n\n\n\n\n\nplot(BSAAM~OPBPC, data = water)\n\n\n\n\n\n\nplot(BSAAM~OPRC, data = water)\n\n\n\n\n\n\nplot(BSAAM~OPSLAKE, data = water)\n\n\n\n\nThere is a relatively strong and positive linear relationship between stream run off and precipitation measures taken at “OPBPC,” “OPRC,” and “OPSLake.” There is no relationship between the others. Here we are running a bi-variate comparison for BSAAM–as this particular site helps us determine runoff when used in conjunction with other individual sites.\nQuestion 4\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\nAnswer\n\n\ndata(\"Rateprof\")\npairs(Rateprof[,8:12], pch=19)\n\n\n\n\n“Quality~Helpfulness,” “Quality~Clarity” and “Helpfulness~Clarity” paris have strong positive correlations and all other pairs have a small positive correlation except (“raterInterest” ~ “Easiness”) which seems to have almost zero correlation. Here, we use the pairs() function to compare the correlation between variables in a bi-variate manner without having to incorporate multiple linear regression (which can get a bit messy very quickly with too many variables). We can recreate the nice table given to us in the text book very simply by using the code above.\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n(a) Use graphical ways to portray the individual variables and their relationship.\nAnswer (i)\nFor the correlation between political ideology and religiosity, we use the xtabs() function which allows us to compare the relationship between 2 categorical variables.\nWe also use a cor.test and run both as.numeric as they are ordinal. We can interpret this as people that are more religious tend to be more conservative in their political views. We have a statistically significant correlation with a small\n\n\ndata(\"student.survey\")\nxtabs(~re + pi, student.survey)\n\n\n              pi\nre             very liberal liberal slightly liberal moderate\n  never                   3       8                2        1\n  occasionally            5      14                1        8\n  most weeks              0       1                1        1\n  every week              0       1                2        0\n              pi\nre             slightly conservative conservative very conservative\n  never                            1            0                 0\n  occasionally                     1            0                 0\n  most weeks                       2            2                 0\n  every week                       2            2                 2\n\ncor.test(as.numeric(student.survey$re), as.numeric(student.survey$pi))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  as.numeric(student.survey$re) and as.numeric(student.survey$pi)\nt = 5.4163, df = 58, p-value = 1.221e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3818345 0.7265650\nsample estimates:\n      cor \n0.5795661 \n\n\n\nrmean = aggregate(as.numeric(student.survey$re), by = list(student.survey$pi), mean)\n\n\n\n^^The code above gives average values for religiosity for each political view on the liberal to conservative scale.\n\n\nplot(rmean)\n\n\n\n\n^^The graphical representation a basic representation of how things look with regards to political ideology and religiosity. If we want to take it a step further, we can use the “error.bar.by” function included in the “psych” package. This plot will be provided below.\n\n\nerror.bars.by(as.numeric(student.survey$pi),student.survey$re, ylab = \"Conservativeness\", v.labels=c(\"never\", \"occasionally\", \"most weeks\", \"every week\"))\n\n\n\n\nThe lines connect the means using this plot. This graphically explains those that attend church more often have higher levels of conservativeness. At the very least, there is a moderate to high correlation.\nAnswer (ii)\n\n\ndata(\"student.survey\")\nplot(~tv + hi, student.survey, col = \"red\")\n\n\n\n\nFor the correlation between high school GPA and hours spent watching TV, we use the plot function as both “tv” & “hi” are numerical values making it possible to plot their relationship.\n\n\ndata(\"student.survey\")\nplot(~tv + hi, student.survey, col = \"red\")\n\n\n\n\n(b) Interpret descriptive statistics for summarizing the individual variables and their relationship.\nAnswer (i)\nHere, we are comparing the political ideology and how often people attend religious services. As we can see, a much larger percentage of people to the left of moderate attend church occasionally or do not attend church at all. Moderates have a very similar relationship with regards to more liberal individuals and their church attending activity. As for conservatives, there is a larger percentage of those that attend church “most week” or “every week.” However, one issue is the fact that at first glance, there are more entries for people that are moderates or liberals than for those that are conservative. Therefore, this somewhat skews the interpretation of the data.\nAnswer (ii)\nThere does appear to be at least somewhat of a correlation between GPA and hours spent watching TV. However, because we cannot draw a smooth line without most likely implementing logs to help smooth things out. Therefore, we cannot say a considerable correlation exists. Instead, we have a weaker negative correlation that exists.\n\n\ndata(\"student.survey\")\nlm(tv~hi, data = student.survey)\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nCoefficients:\n(Intercept)           hi  \n     20.200       -3.909  \n\n(c) Summarize and interpret results of inferential analyses.\nAnswer (i)\n\n\ndata(\"student.survey\")\nxtab = table(student.survey$re, student.survey$pi)\nchisq.test(xtab)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  xtab\nX-squared = 42.288, df = 18, p-value = 0.001008\n\nHere, we are able to accept our alternative hypothesis and reject our null hypothesis because the p-value is very small. This means that events are dependent on one another. For two ordinal variables, it is somewhat tricky with regards to testing how correlated to one another they really are. We could technically simply relabel the ordinal data as numerical data (as we did) to compare this data more readily.\nAnswer (ii)\n\n\ndata(\"student.survey\")\ncor.test(student.survey$tv, student.survey$hi)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  student.survey$tv and student.survey$hi\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nBecause these two variables are numerical, we can run a correlation test where Pearson’s is being used by default. Here, we can reject the null hypothesis and accept our alternative hypothesis because the p-value is below .05. Here we have a very weak negative correlation for “r.” This was indicated graphically above. We also have a t-value slightly greater than -2 which gives us more evidence of the correlation being exhibited.\nQuestion 6\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video:\nAnswer\nConceptually, correlation does not prove causation! There are a myriad of other factors that could act as confounding variables which could have affected higher scores in the tutored class. This conclusion can quickly fall into the category of omitted variable bias. In order to avoid confirmation bias, we should be looking for any other variables that could have possibly had an impact on the class that was tutored. The problem here is that we are not controlling for any of the other factors.\n\n\n\n",
    "preview": "posts/httprpubscomemersonflemi875398/distill-preview.png",
    "last_modified": "2022-03-12T19:36:34-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomektracy875966/",
    "title": "HOMEWORK 2",
    "description": "DACSS 603, Spring 2022",
    "author": [
      {
        "name": "Erin Tracy",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\r\nQUESTION 1\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n1.1.1. Identify the predictor and the response.\r\nPredictor (x): PPGDP\r\nResponse (y): Fertility\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph.\r\n\r\n\r\nlibrary(alr4)\r\n?UN11\r\n\r\nplot(x=UN11$ppgdp, y=UN11$fertility)\r\n\r\n\r\n\r\n\r\nThe scatterplot seems to suggest that the higher the PPGDP, the lower the number of children per woman.\r\nDoes a straight-line mean function seem to be plausible for a summary of this graph?\r\n\r\n\r\nggplot(data=UN11, aes(x=ppgdp, y=fertility))+\r\n  geom_point()+\r\n  geom_smooth(method=\"lm\", se=FALSE)\r\n\r\n\r\n\r\n\r\nYes, straight line does suggest there is function.\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\n\r\n\r\nUN11_log_ppgdp<- log(UN11$ppgdp)\r\nUN11_log_fertility<- log(UN11$fertility)\r\n\r\nplot(x=UN11_log_ppgdp, y=UN11_log_fertility)\r\n\r\n\r\n\r\n\r\nThe scatterplot using logs still suggests that number of children per woman decreases as PPGDP increases,but this new model represents the data more cleanly.\r\nQUESTION 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n(a) How, if at all, does the slope of the prediction equation change?\r\n\r\n\r\nusdollar<- (1:10)\r\npound<- seq(1.33,13.3, length.out = 10)\r\n\r\nslope<-(usdollar/pound)\r\nslope\r\n\r\n\r\n [1] 0.7518797 0.7518797 0.7518797 0.7518797 0.7518797 0.7518797\r\n [7] 0.7518797 0.7518797 0.7518797 0.7518797\r\n\r\nI didn’t initially know how to solve this, so I put together sample of values for dollars and pounds. I calculated the slope and can see that it will not change.\r\n(b) How, if at all, does the correlation change?\r\n\r\n\r\ncor.test(usdollar,pound)\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  usdollar and pound\r\nt = 189812531, df = 8, p-value < 2.2e-16\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 1 1\r\nsample estimates:\r\ncor \r\n  1 \r\n\r\nThe correlation test of the sample data shows that the correlation is 1- perfect correlation. It makes sense that the correlation will never change.\r\nQUESTION 3\r\nWaterrunoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\n?water\r\n\r\nsummary(water)\r\n\r\n\r\n      Year          APMAM            APSAB           APSLAKE     \r\n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \r\n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \r\n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \r\n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \r\n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \r\n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \r\n     OPBPC             OPRC           OPSLAKE           BSAAM       \r\n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \r\n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \r\n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \r\n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \r\n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \r\n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \r\n\r\nhead(water)\r\n\r\n\r\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\r\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\r\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\r\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\r\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\r\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\r\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\r\n\r\npairs(~APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data=water)\r\n\r\n\r\n\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$APMAM)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$APSAB)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$APSLAKE)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$OPBPC)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$OPRC)\r\n\r\n\r\n\r\nplot(y=water$BSAAM,x=water$OPSLAKE)\r\n\r\n\r\n\r\n\r\nI successfully created a scatterplot matrix above, but I had trouble reading the data so I also ran each of the scatterplots separately. There seems to be very strong correlation for OPBPC, OPRC and OPSLAKE between snowfall in inches and stream runoff. For the other sites, more snowfall does seem to indicate more stream runoff, but the correlation is not as great.\r\nQUESTION 4\r\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n\r\n\r\nsummary(Rateprof)\r\n\r\n\r\n    gender       numYears        numRaters       numCourses    \r\n female:159   Min.   : 1.000   Min.   :10.00   Min.   : 1.000  \r\n male  :207   1st Qu.: 6.000   1st Qu.:15.00   1st Qu.: 3.000  \r\n              Median :10.000   Median :24.00   Median : 4.000  \r\n              Mean   : 8.347   Mean   :28.58   Mean   : 4.251  \r\n              3rd Qu.:11.000   3rd Qu.:37.00   3rd Qu.: 5.000  \r\n              Max.   :11.000   Max.   :86.00   Max.   :12.000  \r\n                                                               \r\n pepper       discipline          dept        quality     \r\n no :320   Hum     :134   English   : 49   Min.   :1.409  \r\n yes: 46   SocSci  : 66   Math      : 34   1st Qu.:2.936  \r\n           STEM    :103   Biology   : 20   Median :3.612  \r\n           Pre-prof: 63   Chemistry : 20   Mean   :3.575  \r\n                          Psychology: 20   3rd Qu.:4.250  \r\n                          Spanish   : 20   Max.   :4.981  \r\n                          (Other)   :203                  \r\n  helpfulness       clarity         easiness     raterInterest  \r\n Min.   :1.364   Min.   :1.333   Min.   :1.391   Min.   :1.098  \r\n 1st Qu.:3.069   1st Qu.:2.871   1st Qu.:2.548   1st Qu.:2.934  \r\n Median :3.662   Median :3.600   Median :3.148   Median :3.305  \r\n Mean   :3.631   Mean   :3.525   Mean   :3.135   Mean   :3.310  \r\n 3rd Qu.:4.351   3rd Qu.:4.214   3rd Qu.:3.692   3rd Qu.:3.692  \r\n Max.   :5.000   Max.   :5.000   Max.   :4.900   Max.   :4.909  \r\n                                                                \r\n   sdQuality       sdHelpfulness      sdClarity        sdEasiness    \r\n Min.   :0.09623   Min.   :0.0000   Min.   :0.0000   Min.   :0.3162  \r\n 1st Qu.:0.87508   1st Qu.:0.9902   1st Qu.:0.9085   1st Qu.:0.9045  \r\n Median :1.15037   Median :1.2860   Median :1.1712   Median :1.0247  \r\n Mean   :1.05610   Mean   :1.1719   Mean   :1.0970   Mean   :1.0196  \r\n 3rd Qu.:1.28730   3rd Qu.:1.4365   3rd Qu.:1.3328   3rd Qu.:1.1485  \r\n Max.   :1.67739   Max.   :1.8091   Max.   :1.8091   Max.   :1.6293  \r\n                                                                     \r\n sdRaterInterest \r\n Min.   :0.3015  \r\n 1st Qu.:1.0848  \r\n Median :1.2167  \r\n Mean   :1.1965  \r\n 3rd Qu.:1.3326  \r\n Max.   :1.7246  \r\n                 \r\n\r\nhead(Rateprof)\r\n\r\n\r\n  gender numYears numRaters numCourses pepper discipline\r\n1   male        7        11          5     no        Hum\r\n2   male        6        11          5     no        Hum\r\n3   male       10        43          2     no        Hum\r\n4   male       11        24          5     no        Hum\r\n5   male       11        19          7     no        Hum\r\n6   male       10        15          9     no        Hum\r\n               dept  quality helpfulness  clarity easiness\r\n1           English 4.636364    4.636364 4.636364 4.818182\r\n2 Religious Studies 4.318182    4.545455 4.090909 4.363636\r\n3               Art 4.790698    4.720930 4.860465 4.604651\r\n4           English 4.250000    4.458333 4.041667 2.791667\r\n5           Spanish 4.684211    4.684211 4.684211 4.473684\r\n6           Spanish 4.233333    4.266667 4.200000 4.533333\r\n  raterInterest sdQuality sdHelpfulness sdClarity sdEasiness\r\n1      3.545455 0.5518564     0.6741999 0.5045250  0.4045199\r\n2      4.000000 0.9020179     0.9341987 0.9438798  0.5045250\r\n3      3.432432 0.4529343     0.6663898 0.4129681  0.5407021\r\n4      3.181818 0.9325048     0.9315329 0.9990938  0.5882300\r\n5      4.214286 0.6500112     0.8200699 0.5823927  0.6117753\r\n6      3.916667 0.8632717     1.0327956 0.7745967  0.6399405\r\n  sdRaterInterest\r\n1       1.1281521\r\n2       1.0744356\r\n3       1.2369438\r\n4       1.3322506\r\n5       0.9749613\r\n6       0.6685579\r\n\r\n?Rateprof\r\n\r\npairs(~quality + clarity + helpfulness + easiness + raterInterest, data=Rateprof)\r\n\r\n\r\n\r\n\r\nPlease note I used watched this video helped me create this scatterplot matrix https://www.youtube.com/watch?v=AY9PYzJtCNA\r\nIt looks like quality, clarity and helpfulness are related. It looks like professors who excel at one of these things, excel at all three. Perhaps the quality of the professor is very good when the professor exercises great clarity and helpfulness. It does not look like there is any relationship between a professor being easy and quality, clarity or helpfulness. It also looks like there is not a relationship between rate of interest and quality, clarity of helpfulness of a professor.\r\nQUESTION 5\r\nFor the student.survey data file in the smss package\r\n#install.packages(“smss”) #install.packages(“alr4”) #install.packages(“car”) #install.packages(“effects”) #install.packages(“carData”) #install.packages(“r package”, repos = “http://cran.us.r-project.org”)\r\n\r\n\r\ninstall.packages(\"r package\", repos = \"http://cran.us.r-project.org\")\r\n\r\n\r\nlibrary(smss)\r\n\r\ndata(student.survey)\r\n\r\nsummary(student.survey)\r\n\r\n\r\n      subj       ge           ag              hi       \r\n Min.   : 1.00   f:31   Min.   :22.00   Min.   :2.000  \r\n 1st Qu.:15.75   m:29   1st Qu.:24.00   1st Qu.:3.000  \r\n Median :30.50          Median :26.50   Median :3.350  \r\n Mean   :30.50          Mean   :29.17   Mean   :3.308  \r\n 3rd Qu.:45.25          3rd Qu.:31.00   3rd Qu.:3.625  \r\n Max.   :60.00          Max.   :71.00   Max.   :4.000  \r\n                                                       \r\n       co              dh             dr               tv        \r\n Min.   :2.600   Min.   :   0   Min.   : 0.200   Min.   : 0.000  \r\n 1st Qu.:3.175   1st Qu.: 205   1st Qu.: 1.450   1st Qu.: 3.000  \r\n Median :3.500   Median : 640   Median : 2.000   Median : 6.000  \r\n Mean   :3.453   Mean   :1232   Mean   : 3.818   Mean   : 7.267  \r\n 3rd Qu.:3.725   3rd Qu.:1350   3rd Qu.: 5.000   3rd Qu.:10.000  \r\n Max.   :4.000   Max.   :8000   Max.   :20.000   Max.   :37.000  \r\n                                                                 \r\n       sp               ne               ah             ve         \r\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Mode :logical  \r\n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 0.000   FALSE:60       \r\n Median : 5.000   Median : 3.000   Median : 0.500                  \r\n Mean   : 5.483   Mean   : 4.083   Mean   : 1.433                  \r\n 3rd Qu.: 7.000   3rd Qu.: 5.250   3rd Qu.: 2.000                  \r\n Max.   :16.000   Max.   :14.000   Max.   :11.000                  \r\n                                                                   \r\n pa                         pi                re         ab         \r\n d:21   very liberal         : 8   never       :15   Mode :logical  \r\n i:24   liberal              :24   occasionally:29   FALSE:60       \r\n r:15   slightly liberal     : 6   most weeks  : 7                  \r\n        moderate             :10   every week  : 9                  \r\n        slightly conservative: 6                                    \r\n        conservative         : 4                                    \r\n        very conservative    : 2                                    \r\n     aa              ld         \r\n Mode :logical   Mode :logical  \r\n FALSE:59        FALSE:44       \r\n NA's :1         NA's :16       \r\n                                \r\n                                \r\n                                \r\n                                \r\n\r\nhead(student.survey)\r\n\r\n\r\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\r\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\r\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\r\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\r\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\r\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\r\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal\r\n            re    ab    aa    ld\r\n1   most weeks FALSE FALSE FALSE\r\n2 occasionally FALSE FALSE    NA\r\n3   most weeks FALSE FALSE    NA\r\n4 occasionally FALSE FALSE FALSE\r\n5        never FALSE FALSE FALSE\r\n6 occasionally FALSE FALSE    NA\r\n\r\n?student.survey\r\n\r\n\r\n\r\nconduct regression analyses relating (i) y = political ideology and x = religiosity,\r\nInitially I received a lot of errors when I tried to plot this data. So I used various ways to remove na data, opting for na.omit Then I realized that the data was not numerical, so I recoded the data. “Very Conservative” to “Very Liberal” became 1 through 7 for “Political Ideology”. “Never” to “Every Week” became 0-3 for “how often you attend religious services.”\r\nThen I could plot the data.\r\n#install.packages(“dplyr”)\r\n\r\n\r\ninstall.packages(\"r package\", repos = \"http://cran.us.r-project.org\")\r\n\r\ndata(\"student.survey\")\r\n\r\nis.na(student.survey)\r\n\r\n\r\n       subj    ge    ag    hi    co    dh    dr    tv    sp    ne\r\n [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [6,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [7,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [8,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [9,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[10,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[11,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[12,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[13,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[14,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[15,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[16,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[17,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[18,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[19,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[20,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[21,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[22,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[23,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[24,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[25,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[26,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[27,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[28,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[29,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[30,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[31,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[32,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[33,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[34,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[35,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[36,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[37,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[38,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[39,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[40,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[41,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[42,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[43,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[44,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[45,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[46,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[47,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[48,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[49,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[50,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[51,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[52,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[53,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[54,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[55,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[56,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[57,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[58,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[59,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[60,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n         ah    ve    pa    pi    re    ab    aa    ld\r\n [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [6,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n [7,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [8,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n [9,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[10,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[11,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[12,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[13,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[14,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[15,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[16,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[17,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\r\n[18,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[19,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[20,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[21,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[22,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[23,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[24,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[25,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[26,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[27,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[28,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[29,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[30,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[31,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[32,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[33,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[34,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[35,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[36,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[37,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[38,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[39,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[40,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[41,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[42,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[43,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[44,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[45,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[46,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[47,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[48,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[49,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[50,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[51,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[52,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[53,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[54,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[55,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[56,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[57,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\r\n[58,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[59,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[60,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n\r\nstudent.survey<- na.omit(student.survey)\r\nsummary(student.survey)\r\n\r\n\r\n      subj       ge           ag              hi       \r\n Min.   : 1.00   f:22   Min.   :22.00   Min.   :2.200  \r\n 1st Qu.:17.00   m:21   1st Qu.:24.00   1st Qu.:3.000  \r\n Median :31.00          Median :27.00   Median :3.300  \r\n Mean   :31.19          Mean   :29.23   Mean   :3.305  \r\n 3rd Qu.:46.50          3rd Qu.:31.00   3rd Qu.:3.650  \r\n Max.   :60.00          Max.   :71.00   Max.   :4.000  \r\n                                                       \r\n       co              dh             dr               tv        \r\n Min.   :2.600   Min.   :   0   Min.   : 0.200   Min.   : 0.000  \r\n 1st Qu.:3.200   1st Qu.: 180   1st Qu.: 1.500   1st Qu.: 2.000  \r\n Median :3.500   Median : 630   Median : 2.000   Median : 5.000  \r\n Mean   :3.493   Mean   :1333   Mean   : 4.186   Mean   : 6.756  \r\n 3rd Qu.:3.800   3rd Qu.:1650   3rd Qu.: 5.000   3rd Qu.: 8.000  \r\n Max.   :4.000   Max.   :8000   Max.   :20.000   Max.   :37.000  \r\n                                                                 \r\n       sp               ne               ah             ve         \r\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Mode :logical  \r\n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 0.000   FALSE:43       \r\n Median : 5.000   Median : 3.000   Median : 1.000                  \r\n Mean   : 6.023   Mean   : 3.953   Mean   : 1.465                  \r\n 3rd Qu.: 7.500   3rd Qu.: 5.000   3rd Qu.: 2.000                  \r\n Max.   :16.000   Max.   :14.000   Max.   :11.000                  \r\n                                                                   \r\n pa                         pi                re         ab         \r\n d:11   very liberal         : 6   never       :12   Mode :logical  \r\n i:19   liberal              :14   occasionally:18   FALSE:43       \r\n r:13   slightly liberal     : 4   most weeks  : 5                  \r\n        moderate             : 8   every week  : 8                  \r\n        slightly conservative: 6                                    \r\n        conservative         : 3                                    \r\n        very conservative    : 2                                    \r\n     aa              ld         \r\n Mode :logical   Mode :logical  \r\n FALSE:43        FALSE:43       \r\n                                \r\n                                \r\n                                \r\n                                \r\n                                \r\n\r\nhead(student.survey)\r\n\r\n\r\n   subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa               pi\r\n1     1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r     conservative\r\n4     4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i         moderate\r\n5     5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i     very liberal\r\n7     7  m 24 3.6 3.7    0  0.2  5 12  4  2 FALSE  i          liberal\r\n8     8  f 31 3.0 3.0 5000  1.5  5  3  3  1 FALSE  i          liberal\r\n10   10  m 28 4.0 3.1  900  2.0  1  1  2  1 FALSE  i slightly liberal\r\n             re    ab    aa    ld\r\n1    most weeks FALSE FALSE FALSE\r\n4  occasionally FALSE FALSE FALSE\r\n5         never FALSE FALSE FALSE\r\n7  occasionally FALSE FALSE FALSE\r\n8  occasionally FALSE FALSE FALSE\r\n10        never FALSE FALSE FALSE\r\n\r\nlibrary(dplyr)\r\n\r\n\r\nstudent.survey$pi<- recode(student.survey$pi,\r\n                  \"1\" = \"very conservative\",\r\n                  \"2\" = \"conservative\",\r\n                  \"3\" = \"slightly conservative\",\r\n                  \"4\" = \"moderate\",\r\n                  \"5\" = \"slightly liberal\",\r\n                  \"6\" = \"liberal\",\r\n                  \"7\" = \"very liberal\")\r\n\r\nstudent.survey$re<- recode(student.survey$re,\r\n                  \"0\" = \"never\",\r\n                  \"1\" = \"occasionally\",\r\n                  \"2\" = \"most weeks\",\r\n                  \"3\" = \"every week\")\r\n\r\nplot(x = student.survey$re, y = student.survey$pi)\r\n\r\n\r\n\r\n\r\nIt was not clear exactly what this model meant, so I tried another way.\r\n(a) Use graphical ways to portray the individual variables and their relationship.\r\n\r\n\r\nggplot(data=student.survey, aes(x=re, y=pi))+\r\n  geom_point()\r\n\r\n\r\n\r\n\r\n(b) Interpret descriptive statistics for summarizing the individual variables and their relationship.\r\nThis model is unusual, but it does seem to indicate that at as the frequency of attending religious services grew, so did the number associated with political ideology.\r\n(c) Summarize and interpret results of inferential analyses.\r\nPeople who attend church often are more likely to leave conservative, than people who attend church never or only occasionally.\r\nconduct regression analyses relating (ii) y = high school GPA and x = hours of TV watching.\r\n\r\n\r\nplot(x = student.survey$tv, y = student.survey$hi)\r\n\r\n\r\n\r\n\r\n(a) Use graphical ways to portray the individual variables and their relationship.\r\n\r\n\r\nggplot(data=student.survey, aes(x=tv, y=hi))+\r\n  geom_point()+\r\n  geom_smooth(method=\"lm\", se=FALSE)\r\n\r\n\r\n\r\n\r\n(b) Interpret descriptive statistics for summarizing the individual variables and their relationship.\r\nI do not see a significant relationship here.\r\n(c) Summarize and interpret results of inferential analyses.\r\nWhile a few outliers- who watch a lot of TV seem to have below average High School GPAs, most people in this study fall between a 3.0 and a 4.0 GPA and average number of hours of tv does not seem to affect GPA.\r\nQUESTION 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\r\nMaybe the 10 students who performed the poorest on the midterm would have had an increase from 50 to 60 for the final, regardless of the tutoring. Perhaps they would have been so aware and concerned that they had performed poorly on the midterm, that they would have made an extra effort to do better on the next (and final) exam. While the students that did better or even just average for the class- would not have felt the same drive to try harder.\r\nhttps://www.youtube.com/watch?v=1tSqSMOyNFE\r\nThere is always a level of chance. “You should not expect them to be as unlucky when you test them a second time…their scores should improve just based on random chance.”\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomektracy875966/distill-preview.png",
    "last_modified": "2022-03-12T19:36:39-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomclairebattagliahomework-2-603/",
    "title": "Homework 2",
    "description": "Simple Linear Regression",
    "author": [
      {
        "name": "Claire Battaglia",
        "url": "https://rpubs.com/clairebattaglia"
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\n\nContents\nQuestion 1\nAnswer\nSolution\n\nQuestion 2\nAnswer\nSolution\n\nQuestion 3\nAnswer\nSolution\n\nQuestion 4\nSolution\nDiscussion\n\nQuestion 5\nSolution\n\nQuestion 6\nSolution\n\n\nQuestion 1\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\nIdentify the predictor and the response.\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nAnswer\nThe predictor, or explanatory, variable is gross national product per person (ppgdp) and the response variable is fertility.\nNo. The relationship between the two variables appears to be exponential instead of linear so a simple linear regression model is not a good fit.\nYes. A simple linear regression model is plausible when we log-transform the data.\nSolution\nLet’s start by inspecting the data. I’ll load the data set and create a new dataframe with just the variables I’m interested in. I’ll then take a quick look to make sure that there are no missing values.\n\n\nShow code\n\n# load data\ndata(\"UN11\")\n\n# create new dataframe\nun11 <- UN11 %>%\n  select(c(fertility, ppgdp))\n\n# get summary\nsummary(un11)\n\n\n   fertility         ppgdp         \n Min.   :1.134   Min.   :   114.8  \n 1st Qu.:1.754   1st Qu.:  1283.0  \n Median :2.262   Median :  4684.5  \n Mean   :2.761   Mean   : 13012.0  \n 3rd Qu.:3.545   3rd Qu.: 15520.5  \n Max.   :6.925   Max.   :105095.4  \n\nNow I’ll create a scatterplot.\n\n\nShow code\n\n# create scatterplot\nggplot(un11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  labs(title = \"Fertility v. Gross National Product Per Person\", x = \"Gross National Product Per Person\", y = \"Fertility\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\n\n\n\nWe can see that a linear model is not a good fit. This makes intuitive sense, as a linear model would predict a negative birth rate that continues to grow in absolute value beyond a certain value for ppgdp, which isn’t logical. It instead seems that fertility varies pretty wildly among countries with a ppgdp less than 125,000 or so but that for countries with a ppgdp above that threshold, fertility varies much less and eventually seems to settle around 2.\nBecause the relationship isn’t linear, a log transformation might be useful.\n\n\nShow code\n\nggplot(un11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  labs(title = \"Log Transformation of Fertility v. Gross National Product Per Person\", x = \"Log of Gross National Product Per Person\", y = \"Log of Fertility\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\n\n\n\nA linear model of the log transformation appears to be a much better fit.\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\nHow, if at all, does the slope of the prediction equation change?\nHow, if at all, does the correlation change?\nAnswer\nThe slope does change. Converting the units of the response variable at the exchange rate from 2016 (1 GBP = 1.33 USD, 1 USD = .75 GBP), \\[new\\;slope=\\frac{old\\;slope}{1.33}\\] It’s worth noting that if we were to convert the units of the explanatory variable instead of the response variable, we would calculate the new slope by performing the inverse mathematical operation of the unit change.\nThe correlation does not change.\nSolution\nWhile the slope (\\(b\\)) of a prediction equation and the correlation (\\(r\\)) between the two variables are linked conceptually and mathematically, they are distinct from one another and convey different information.\nThe slope is a measure of the direction of the association between the two variables. For every unit increase in \\(x\\), \\(y\\) will either increase or decrease (if the two variables are actually associated with one another). The sign of the slope will tell us which and the value itself will tell us by how much.\nThe slope cannot tell us the strength of that association, however. This is because the slope is dependent on the specific units of the variables. If the data are converted to a different unit of measurement, the slope will change, while logically we know that the strength of the association between the two variables hasn’t changed.\nAs a standardized form of the slope, the correlation is not dependent on the specific units of the variables and is thus able to tell us the strength of the association between the two variables.\nLet’s look at a quick example (all data are entirely made up and completely meaningless).\n\n\nShow code\n\nx <- 1:20 # set values for x\na <- 1 # set y intercept\nb <- 2 # set slope\nyUSD = a + b*x # prediction equation with y in USD\n\n# create dataframe\ndataQ2 <- data.frame(x, yUSD)\n\n# convert y from USD to GBP\ndataQ2 <- dataQ2 %>%\n  mutate(yGBP = yUSD/1.33)\n\n# view\ndataQ2\n\n\n    x yUSD      yGBP\n1   1    3  2.255639\n2   2    5  3.759398\n3   3    7  5.263158\n4   4    9  6.766917\n5   5   11  8.270677\n6   6   13  9.774436\n7   7   15 11.278195\n8   8   17 12.781955\n9   9   19 14.285714\n10 10   21 15.789474\n11 11   23 17.293233\n12 12   25 18.796992\n13 13   27 20.300752\n14 14   29 21.804511\n15 15   31 23.308271\n16 16   33 24.812030\n17 17   35 26.315789\n18 18   37 27.819549\n19 19   39 29.323308\n20 20   41 30.827068\n\nI now have 20 values for \\(x\\) in USD, 20 corresponding \\(y\\) values in USD, and a second set of \\(y\\) values that are the original \\(y\\) values converted into GBP at the given exchange rate. I can now regress each set of \\(y\\) values onto \\(x\\) and compare the slopes and correlation coefficients.\n\n\nShow code\n\n# regress yUSD onto x\nfitUSD <- lm(yUSD ~ x, data = dataQ2)\n\n# regress yGBP onto x\nfitGBP <- lm(yGBP ~ x, data = dataQ2)\n\n# show both regression outputs\nstargazer(fitUSD, fitGBP,\n          type = \"text\",\n          header = FALSE,\n          single.row = TRUE,\n          no.space = TRUE,\n          column.sep.width = \"3pt\",\n          font.size = \"small\")\n\n\n\n=================================================================================================================================\n                                                                      Dependent variable:                                        \n                              ---------------------------------------------------------------------------------------------------\n                                                    yUSD                                              yGBP                       \n                                                     (1)                                               (2)                       \n---------------------------------------------------------------------------------------------------------------------------------\nx                                             2.000*** (0.000)                                  1.504*** (0.000)                 \nConstant                                      1.000*** (0.000)                                  0.752*** (0.000)                 \n---------------------------------------------------------------------------------------------------------------------------------\nObservations                                         20                                                20                        \nR2                                                  1.000                                             1.000                      \nAdjusted R2                                         1.000                                             1.000                      \nResidual Std. Error (df = 18)                       0.000                                             0.000                      \nF Statistic (df = 1; 18)      79,494,261,481,456,398,357,598,065,131,520.000*** 71,025,695,591,639,092,023,323,881,635,840.000***\n=================================================================================================================================\nNote:                                                                                                 *p<0.1; **p<0.05; ***p<0.01\n\nWhen \\(y\\sim\\sf{x}\\) and both variables are in USD, \\(b=2\\) and \\(r=1\\)\nThis means that when \\(x\\) increases by a single unit, \\(y\\) increases by 2 units.\nWhen I convert \\(y\\) from USD into GBP and once again run the regression \\(y\\sim\\sf{x}\\), \\(b=1.504\\) and \\(r=1\\)\nThis means that when \\(x\\) increases by a single unit, \\(y\\) increases by 1.504 units.\n\\[1.504=\\frac{2}{1.33}\\]\n\nNote that while the output doesn’t include the correlation coefficient, we know that \\(r=1\\) because \\(R^2=1\\).\nBecause I created the data myself, I know that the relationship between \\(x\\) and \\(y\\) is the same when \\(y\\) is in USD as when \\(y\\) is in GBP, yet the slopes are different. This demonstrates that the slope is dependent on the specific unit of measurements of the variables while the correlation coefficient is not. The two are used in concert to understand the relationship between two variables.\nQuestion 3\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\nAnswer\nYear does not appear to be correlated with any variable.\nAPMAM, APSAB, and APSLAKE appear to be moderately positively correlated with one another.\nOPBPC, OPRC, and OPSLAKE appear to be strongly positively correlated with one another.\nThere appears to be a moderate positive correlation between BSAAM and APMAM, APSAB, and APSLAKE, and a strong positive correlation between BSAAM and OPBPC, OPRC, and OPSLAKE.\nSolution\nAs always, I’ll start by inspecting the data.\n\n\nShow code\n\n# load data\ndata(\"water\")\n\n# create object\nwater <- water\n\n# preview\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\nThe predictor variables are the precipitation measurements taken at each of the six sites: APMAM, APSAB, APSLAKE, OPBPC, OPRC, OPSLAKE, measured in inches.\n\nMeasurements are included in the package documentation, available here.\nThe response variable is the stream runoff at the site BSAAM, measured in acre-feet.\n\n\nShow code\n\n# create scatterplot matrix\npairs(water)\n\n\n\n\nYikes. Let’s first orient ourselves so that we can make sense of this.\nThe \\(x\\)-axis of any given plot is the variable named in that column. In the above matrix, each plot in the first column has the \\(x\\)-axis year.\nThe \\(y\\)-axis of any given plot is the variable named in that row. In the above matrix, each plot in the first row has the \\(y\\)-axis year.\nBy looking at the first column and row, then, we can see the variable year plotted against every other variable, on the \\(x\\) and \\(y\\)-axis, respectively.\nWith that in mind, let’s assess.\nYear does not appear to be correlated with any variable, which is to be expected. While collecting these data over time could reveal larger trends (e.g. climate change), the year itself isn’t predictive of runoff.\nAPMAM, APSAB, and APSLAKE appear to be moderately positively correlated with one another.\nOPBPC, OPRC, and OPSLAKE appear to be strongly positively correlated with one another.\nBSAAM appears to be moderately positively correlated with APMAM, APSAB, and APSLAKE.\nBSAAM appears to be strongly positively correlated with OPBPC, OPRC, and OPSLAKE.\nAnother way to visualize the data is with the ggcorr function. Instead of showing us a scatterplot matrix, it calculates the correlation between each set of variables and then displays the strength of the association as color-coded categories. This is an easy to confirm what we gleaned from the scatterplot matrix.\n\n\nShow code\n\nggcorr(water)\n\n\n\n\nWe can see that no variables are negatively correlated with one another (blue), that year is either not correlated or very weakly positively correlated with every variable (white to pale pink), that APMAM, APSAB, and APSLAKE are moderately positively correlated with one another (light red to red), that OPBPC, OPRC, and OPSLAKE are strongly positively correlated with one another (red), and, finally, that BSAAM is strongly positively correlated with OPBPC, OPRC, and OPSLAKE (red) and slightly less so with APMAM, APSAB, and APSLAKE (light red).\nQuestion 4\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\nSolution\nAs always, I’ll start by inspecting the data.\n\n\nShow code\n\n# load data\ndata(\"Rateprof\")\n\n# create object\nrateProf <- Rateprof\n\n# preview\nhead(rateProf)\n\n\n  gender numYears numRaters numCourses pepper discipline\n1   male        7        11          5     no        Hum\n2   male        6        11          5     no        Hum\n3   male       10        43          2     no        Hum\n4   male       11        24          5     no        Hum\n5   male       11        19          7     no        Hum\n6   male       10        15          9     no        Hum\n               dept  quality helpfulness  clarity easiness\n1           English 4.636364    4.636364 4.636364 4.818182\n2 Religious Studies 4.318182    4.545455 4.090909 4.363636\n3               Art 4.790698    4.720930 4.860465 4.604651\n4           English 4.250000    4.458333 4.041667 2.791667\n5           Spanish 4.684211    4.684211 4.684211 4.473684\n6           Spanish 4.233333    4.266667 4.200000 4.533333\n  raterInterest sdQuality sdHelpfulness sdClarity sdEasiness\n1      3.545455 0.5518564     0.6741999 0.5045250  0.4045199\n2      4.000000 0.9020179     0.9341987 0.9438798  0.5045250\n3      3.432432 0.4529343     0.6663898 0.4129681  0.5407021\n4      3.181818 0.9325048     0.9315329 0.9990938  0.5882300\n5      4.214286 0.6500112     0.8200699 0.5823927  0.6117753\n6      3.916667 0.8632717     1.0327956 0.7745967  0.6399405\n  sdRaterInterest\n1       1.1281521\n2       1.0744356\n3       1.2369438\n4       1.3322506\n5       0.9749613\n6       0.6685579\n\nNow I’ll create a basic scatterplot matrix.\n\n\nShow code\n\n# create scatterplot matrix\npairs(~ quality + helpfulness + clarity + easiness + raterInterest, data = rateProf)\n\n\n\n\nDiscussion\nQuality appears to have:\na strong positive correlation with helpfulness and clarity,\na moderate positive correlation with easiness,\na weak positive correlation with raterInterest.\nHelpfulness appears to have:\na strong positive correlation with quality and clarity,\na moderate positive correlation with easiness,\na weak positive correlation with raterInterest.\nClarity appears to have:\na strong positive correlation with quality and helpfulness,\na moderate positive correlation with easiness,\na weak positive correlation with raterInterest.\nEasiness appears to have:\na moderate positive correlation with quality, helpfulness, and clarity,\na weak positive correlation with raterInterest.\nRaterInterest appears to have a weak positive correlation with every other variable.\nBeyond a simple statement of the association between each of these variables, however, it’s impossible to say what substantive, or contextual, significance these data have. In an attempt to understand the data more clearly, I created an account with Rate My Professors and took a look around. Some things I noticed are:\nThe questions that yield the data represented in the above matrix all utilize a Likert scale and there is no guidance as to what constitutes a “1” or a “5” on any given question. For example, the question that yields the quality rating is simply “Rate your professor” and the scale is from “1 - Awful” to “5 - Awesome.” Without knowing what makes a professor awful or awesome to particular respondent, the rating itself is arguably meaningless.\nInformation about the respondent (the student) is paramount to placing the ratings in context. For example, the question that yields the easiness rating is “How difficult was this professor?” If the rating were for the professor of this class (DACSS 603), for example, a rating of “5 - Very Difficult” could mean something completely different if it’s coming from an undergraduate student with a humanities background versus a graduate student with a statistics background. While the question is technically about the professor, not the subject material, those two are too easily conflated. Without knowing this information about the respondent, this rating is of limited value.\nThere are some challenges to measurement validity. For example, is a high rating in clarity due to the professor actually being clear when conveying information or is it due to the student having a strong enough background in the subject matter to understand the material well, regardless of how clearly it’s presented? Similarly, we know that people ask better questions when they’re better versed in a subject so it’s conceivable that a high helpfulness rating is actually reflective of the respondent being able to ask good enough questions to get good answers.\nThe whole point of Rate My Professors is for students to be able to read reviews of a professor and/or class before taking a class. Thus if it’s being utilized as intended, students are, by design, going into a course with others’ evaluation of the professor and/or class in mind, which could conceivably influence their attitude/behavior in the class, which could affect their experience, which would ultimately affect their own rating.\nUltimately, I don’t think these data can tell us much about a given professor. The real potential of these data is in understanding how people rate their professors, which is not the same thing as how the professors actually are.\nIt could be interesting to subset the data in different ways: by the gender of the professor (do we rate professors of different gender differently?), the gender of the respondent (do students of different gender rate professors differently?), the discipline (do we rate humanities professors differently than STEM professors?), the age/year of the respondent (do undergraduate freshmen tend to rate differently than graduate students?), etc. As the questions are currently designed, however, they don’t consistently capture valid information about the professor or the underlying factors that influence how people might rate their professors.\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating\ny = political ideology and x = religiosity,\ny = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\nUse graphical ways to portray the individual variables and their relationship.\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\nSummarize and interpret results of inferential analyses.\nSolution\nAs always, I’ll start by inspecting the data.\n\n\nShow code\n\n# load data\ndata(\"student.survey\")\n\n# create dataframe\nstudS <- student.survey %>%\n  select(c(\"pi\", \"re\", \"hi\", \"tv\"))\n\n# get summary\nsummary(studS)\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nWhile I am using the summary function here primarily to determine which types of variables we’re working with and to make sure there are no missing values, it actually also returns some of the descriptive statistics I’m interested in. I’ll look at those more closely in each section.\nPolitical Ideology and Religiosity\nPolitical ideology and religiosity are both categorical variables, meaning that respondents can select their response option from a specified set of values that are categorical in nature. Because there is an underlying order to each range of response options, both are ordinal.\nThis means that I’ll primarily be looking at the frequency and relative frequency of each response option. Given that the distance between response options is not quantifiable and almost certainly unequal, measures like the mean and standard deviation are not appropriate here.\nI’ll start by looking at political ideology. A simple bar chart is a great way to visualize frequency of each response.\n\n\nShow code\n\n# create bar chart\nggplot(studS, aes(x = pi)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Political Ideology of STA 6126 Students\", x = \"Political ideology\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nWe can clearly see that the mode is “liberal,” as well as the relative frequency of all of the response options (although not the exact values).\nWe could also view the frequency distribution in a simple table. In the table below, the first value in each cell is the number of students who responded with that option and the second value is the proportion. Looking at the first cell, we can see that 8 students, or 13.3%, identify as “very liberal.”\n\n\nShow code\n\n# create freq table\nCrossTable(studS$pi,  max.width = 3)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  60 \n\n \n                      |          very liberal |               liberal |      slightly liberal | \n                      |-----------------------|-----------------------|-----------------------|\n                      |                     8 |                    24 |                     6 | \n                      |                 0.133 |                 0.400 |                 0.100 | \n                      |-----------------------|-----------------------|-----------------------|\n\n                      |              moderate | slightly conservative |          conservative | \n                      |-----------------------|-----------------------|-----------------------|\n                      |                    10 |                     6 |                     4 | \n                      |                 0.167 |                 0.100 |                 0.067 | \n                      |-----------------------|-----------------------|-----------------------|\n\n                      |     very conservative | \n                      |-----------------------|\n                      |                     2 | \n                      |                 0.033 | \n                      |-----------------------|\n\n\n\n \n\nIf we were willing to sacrifice some precision to simplify the data we could collapse the categories into “conservative,” “moderate,” and “liberal.”\n\n\nShow code\n\n# collapse categories\npiCollapse <- studS %>%\n  mutate(pi = fct_collapse(pi,\n                         liberal = c(\"very liberal\", \"liberal\", \"slightly liberal\"),\n                         conservative = c(\"very conservative\", \"conservative\", \"slightly conservative\"),\n                         moderate = c(\"moderate\")))\n\n# create bar plot\nggplot(piCollapse, aes(x = pi)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Political Ideology of STA 6126 Students\", x = \"Political ideology\", y = NULL) +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\nAnother way to approach these data is by looking at the students’ distance from the ideological center. That is, how are the students distributed around the center of the ideological spectrum?\n\n\nShow code\n\n# collapse categories\npiDistance <- studS %>%\n  mutate(pi = fct_collapse(pi,\n                           \"furthest left or right of center\" = c(\"very liberal\", \"very conservative\"),\n                           \"a bit further left or right of center\" = c(\"liberal\", \"conservative\"),\n                           \"slightly left or right of center\" = c(\"slightly liberal\", \"slightly conservative\"),\n                           \"center (neither left nor right)\" = c(\"moderate\")))\n\n# create bar plot\nggplot(piDistance, aes(x = pi)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Distance from Ideological Center\", x = \"Distance from center\", y = NULL) +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\nIt appears that the greatest number of students consider themselves to be more than just slightly left or right of center but not very far left or right of center.\nI’m sure this is an entire area of study in itself so I’d want to explore the literature before standing by this interpretation but for now I’ll say we could potentially look at the data this way.\nNext I’ll look at religiosity. Again, a simple bar chart is a great place to start.\n\n\nShow code\n\n# create bar chart\nggplot(studS, aes(x = re)) +\n  geom_bar(fill = \"#830042\") +\n  labs(title = \"Religiosity of STA 6126 Students\", x = \"How often do you attend religious services?\", y = NULL) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nOnce again this allows us to easily identify the mode and relative frequency of each response option. “Occasionally” is clearly the most common response; “most weeks” the least common.\nWe can again look at the frequency distribution in a simple table.\n\n\nShow code\n\n# create freq table\nCrossTable(studS$re,  max.width = 4)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  60 \n\n \n             |        never | occasionally |   most weeks |   every week | \n             |--------------|--------------|--------------|--------------|\n             |           15 |           29 |            7 |            9 | \n             |        0.250 |        0.483 |        0.117 |        0.150 | \n             |--------------|--------------|--------------|--------------|\n\n\n\n \n\nIn order to understand the relationship between political ideology and religiosity, we can construct a contingency table and/or a scatterplot.\n\n\nShow code\n\n# create table pi and re\ntablePiRe <- table(studS$pi, studS$re)\n\n# view\ntablePiRe\n\n\n                       \n                        never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nShow code\n\n# create scatterplot\nggplot(studS, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_jitter() +\n  labs(title = \"Political Ideology v. Religiosity\", x = \"Religiosity\", y = \"Political ideology\") +\n  theme_minimal()\n\n\n\n\nThe two do seem to be associated with one another. Less religious students are more likely to be more liberal and more religious students are more likely to be more conservative.\nBecause both variables are ordinal, we can also calculate gamma (\\(\\gamma\\)) to understand the association between them. Gamma is the standardized difference between pairs that are concordant (both measures are either high or low) and discordant (one measure is high and the other is low).\n\n\nShow code\n\n# calculate gamma\nGoodmanKruskalGamma(tablePiRe,\n                    conf.level = 0.95)\n\n\n    gamma    lwr.ci    upr.ci \n0.5747711 0.3646211 0.7849211 \n\nThis output gives us the 95% confidence interval for gamma. Here, \\(\\gamma=.575\\). The positive sign indicates a positive correlation and the confidence interval tells us gamma will most likely lie between .365 and .785—that is, there is a moderate to fairly strong correlation between the two variables.\nBecause of the way the categories are numbered (1 = very liberal, 7 = very conservative), gamma has been calculated such that we articulate the association:\nReligiosity is positively associated with conservatism. As religiosity increases, conservatism increases.\nWe could also say that religiosity is negatively associated with liberalism but that is not what’s being reflected in our gamma calculation.\nThere are a few ways to approach understanding association between ordinal variables. I’ve chosen to look at gamma because the sample size seems too small to justify using \\(\\chi^2\\). Another option is to treat the data numerically and then run a simple linear regression. Given the potentially extreme variability in distance between the categories (for religiosity in particular), I’m not sure we could justify doing that here.\nThat being said, here’s what it would look like.\n\n\nShow code\n\n# convert categories to numeric values\npireNum <- studS %>%\n  select(c(\"pi\", \"re\")) %>%\n  sapply(unclass)\n\n# convert to dataframe\npireNum <- data.frame(pireNum)\n\n# regress ideology onto religiosity\nfitPiRe <- lm(pi ~ re, data = pireNum)\n\n# get summary\nsummary(fitPiRe)\n\n\n\nCall:\nlm(formula = pi ~ re, data = pireNum)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre            0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\nHere again we do see a positive correlation, which confirms our above conclusion. The \\(P\\)-value meets the criteria for statistical significance at the .01 level, indicating that we can be confident that the slope (\\(b\\)) is not zero.\nPut another way, we can reject the null hypothesis that the two variables are independent of one another.\nHigh School GPA and TV Watching\nBoth high school GPA and hours of TV watched per week are quantitative variables. This means that in addition to looking at the frequency and relative frequency of each value, we’ll also be able to look at the mean and standard deviation of each data set.\nI’ll first look at high school GPA. Because this is a small data set, a stem-and-leaf plot might be a useful way to take a quick look.\n\nWe could also use a histogram.\n\n\nShow code\n\n# create stem and leaf plot\nstem(studS$hi, scale = 3)\n\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  20 | 0\n  21 | 0\n  22 | 0\n  23 | 0\n  24 | \n  25 | \n  26 | \n  27 | 0\n  28 | 0\n  29 | \n  30 | 00000000000000\n  31 | 0\n  32 | 000\n  33 | 000000\n  34 | 00000\n  35 | 0000000\n  36 | 000\n  37 | 0000\n  38 | 000000\n  39 | \n  40 | 00000\n\nWe can see that the lowest grade earned is a 2.0 (1 student) and that the highest grade earned is a 4.0 (5 students), making the range 2.\n3.0 appears to be the most frequently earned grade (the mode) and the largest proportion of grades are clustered between 3.0 and 3.8 so I expect both the mean and median to fall somewhere between those values.\nBeyond the mode, range, and some sense of the distribution of the grades, there are a couple of other descriptive statistics we can look at.\n\n\nShow code\n\n# get summary stats\nselect(studS, hi) %>%\n  describe.by()\n\n\n   vars  n mean   sd median trimmed  mad min max range  skew kurtosis\nX1    1 60 3.31 0.46   3.35    3.35 0.52   2   4     2 -0.76     0.57\n     se\nX1 0.06\n\nThe median is 3.35, the mean is 3.31, and the standard deviation is .46. We can see that the data are slightly skewed because the median and mean are different values. The mean is pulled down by those few very low GPAs.\nNow I’ll look at the average number of hours of TV watched per week. We can use a simple histogram to take a look at the frequency distribution.\n\n\nShow code\n\n# create hist\nggplot(studS, aes(x = tv)) +\n  geom_histogram(fill = \"#830042\") +\n  labs(title = \"TV-Watching Habits of STA 6126 Students\", x = \"Average hours watched per week\", y = NULL) +\n  theme_minimal()\n\n\n\n\nThis gives us a sense of the frequency distribution and we can again get some summary statistics.\n\n\nShow code\n\n# get summary stats\nselect(studS, tv) %>%\n  describe.by()\n\n\n   vars  n mean   sd median trimmed  mad min max range skew kurtosis\nX1    1 60 7.27 6.72      6    6.24 5.93   0  37    37 2.14     6.06\n     se\nX1 0.87\n\nWe can see from the histogram that most of the students watch less than 15 hours per week but that a few watch more than 25 hours per week. The median is 6 and the mean is 7.27, reflecting the skew created by those few very high values. The range (37) is surprisingly large and it’s hard to even imagine watching 37 hours of TV per week. It’s conceivable that there are two types of TV-watching: active watching (you sit down to watch something) and passive watching (it’s on in the background while you do something else). The large range makes me wonder if some respondents were reporting only active watching while perhaps others were reporting both active and passive watching.\nBefore looking at these two variables together, it’s worth noting that we’re looking at each student’s GPA in high school and TV-watching habits in graduate school. It’s worth noting because I think it’s tempting to conceptualize TV-watching habits as the explanatory variable and GPA as the outcome variable (something like “watching more/less TV is associated with a higher/lower GPA”) and subsequently explore causal inference but given the temporal order in this case (GPA precedes TV-watching) and long time between the two (a minimum of 4 years), it seems highly unlikely that any association between these two variables could have any substantive meaning.\nThat being said, a scatterplot is a great way to visualize association.\n\n\nShow code\n\n# create scatterplot\nggplot(studS, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_jitter() +\n  labs(title = \"High School GPA v. Hours of TV Watched per Week in Graduate School\", x = \"Hours of TV watched per week\", y = \"High School GPA\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\n\n\n\nThere appears to be a weak negative correlation between high school GPA and hours of TV watched per week.\n\n\nShow code\n\n# cor test\ncor.test(studS$tv, studS$hi)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  studS$tv and studS$hi\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nA correlation coefficient (\\(r\\)) of -.268 confirms that there is a weak negative correlation. Those students with higher GPAs in high school tend to watch less TV in graduate school.\n\n\nShow code\n\n# fit model\nfitHiTv <- lm(hi ~ tv, data = studS)\n\n# get summary\nsummary(fitHiTv)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = studS)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\nOur regression analysis indicates that for every unit increase in GPA, there is a .018 decrease in hours of TV watched in graduate school.\nAgain, though, given the above discussion of these two variables, I don’t believe there is any substantive or contextual meaning to the observed association.\nQuestion 6\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\nSolution\nRegression towards the mean is a statistical phenomenon in which extreme values (those furthest away from the mean) tend to be less extreme (closer to the mean) when measured again.\nIt occurs when two variables are correlated but there is also some element of chance that explains the outcome variable. Observing a non-random sample makes it more likely that we will observe this phenomenon.\nIn the context of this quasi-experiment, each student’s test scores vary around some true level of knowledge, which we don’t know. The difference between the student’s true level of knowledge and the student’s exam score is the error.\n\nTo call this a quasi-experiment feels generous but for lack of a more precise term, that’s what I’m going to call it.\nThis error can occur in either direction and be caused by many things. That is, a student could score higher or lower than her true level of knowledge because of any number of conditions—conditions that occur to some degree by chance. Because those conditions occur to some degree by chance, it’s improbable that the same students will experience the same conditions during both the first observation (the midterm exam) and the second observation (the final exam). If those students do not experience those same conditions, we would expect to see the students who scored higher than their true level of knowledge score a little lower and the students who scored lower than their true level of knowledge score a little higher. This will occur regardless of treatment.\nIt’s important to note that regression towards the mean does not imply that every student who scored extremely high or low did so purely by chance and that their scores will be different when observed a second time. We’re looking at the mean score of the ten lowest-scorers. If even just a few of those students scored lower than their true level of knowledge due to some chance condition—perhaps one student was sick on exam day, one missed the bus and had to run to school, and a third’s parents had just announced their divorce—that wasn’t present at the time of the second observation (the final exam), those students would score higher, which would increase the mean of the entire group of lowest-scorers. Thus we cannot say whether there was any change in the students’ true level of knowledge.\nIt would be helpful to also look at the mean score of the ten highest-scorers on the midterm. Regression towards the mean tells us that we would likely see the mean score of those ten students decrease from the midterm to the final.\nFrom a research design perspective, this quasi-experiment is problematic in other ways that prevent us from drawing any causal inference about the effectiveness of the treatment.\nThe sample is small.\nThere is no control group. There is a group of students who did not receive the treatment but because the treatment and no-treatment groups were not randomly assigned, we know that they differed from one another in at least one way (their midterm exam score). Whether they differed in other ways was not explored so at this time we have no way of knowing whether they differed in systematic ways that could also explain their scores.\nIt’s likely that one observation (the midterm exam) is insufficient for determining a student’s true level of knowledge. In order to overcome the regression challenge, we would need more observations of the same students over time. This would help us understand how much of each student’s score can be attributed to some true level of knowledge and how much is due to some chance condition.\nWe can finally conclude that the phenomenon of regression towards the mean could easily explain the increase in the mean score of the ten lowest-scorers. In addition, there are numerous other challenges to the validity of this quasi-experiment. As such, we cannot claim that the treatment was effective. Indeed, we can’t even claim that there was any actual change in the students’ level of knowledge.\n\n\n\n",
    "preview": "posts/httpsrpubscomclairebattagliahomework-2-603/distill-preview.png",
    "last_modified": "2022-03-12T19:36:42-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomalexanderhong86876317/",
    "title": "DACSS 603 HW#2",
    "description": "Second homework for DACSS 603.",
    "author": [
      {
        "name": "Alexander Hong",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n\r\n\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nhw1 <- UN11\r\n\r\nhw1_plot <- ggplot(hw1, aes(ppgdp, fertility)) + geom_point()\r\nhw1_plot_log <- ggplot(hw1, aes(log(ppgdp), log(fertility)) ) + geom_point()\r\n\r\n\r\n\r\n1.1.1. Identify the predictor and the response. - The response is ppgdp (on ppgdp) and the predictor is fertility (dependence of).\r\n\r\n\r\n\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n\r\n\r\n\r\nNo. If anything, the points seem to be distributed in a negative exponential fashion.\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nThe natural logarithmic transformation has enabled a simple linear regression model to be plausible.\r\nQuestion 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\n\r\n\r\nsome_pred <- c(7, 9, 11, 12, 13, 14)\r\nsome_inc_us <- c(45000, 58000, 65400, 80000, 94600, 103580)\r\nsome_inc_uk <- some_inc_us * (1 / 1.33)\r\n\r\nhw2 <- data.frame(some_pred, some_inc_us, some_inc_uk)\r\nhw2_table <- knitr::kable(hw2)\r\n\r\nhw2a <- summary(lm(some_pred ~ some_inc_us, data = hw2))\r\nhw2b <- summary(lm(some_pred ~ some_inc_uk, data = hw2))\r\n\r\n\r\n\r\nFirst, I made a hypothetical dataset that has some response variable (some_pred), along with hypothetical salaries (some_inc_us), as well as converting the salaries to pounds (some_inc_uk)\r\nsome_pred\r\nsome_inc_us\r\nsome_inc_uk\r\n7\r\n45000\r\n33834.59\r\n9\r\n58000\r\n43609.02\r\n11\r\n65400\r\n49172.93\r\n12\r\n80000\r\n60150.38\r\n13\r\n94600\r\n71127.82\r\n14\r\n103580\r\n77879.70\r\nHow, if at all, does the slope of the prediction equation change\r\nThe slope (coefficient) for using dollars as a predictor 1.1334084^{-4}\r\nis different from\r\nusing pounds as a predictor 1.5074332^{-4}\r\nHow, if at all, does the correlation change? It seems as if the correlation does not change\r\nR Squared value, using dollar salaries to predict some_pred : 0.9465294\r\nR Squared value, using UK pound salaries to predict some_pred : 0.9465294\r\nQuestion 3\r\nWater runoff in the Sierras (ALR Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\r\n\r\n\r\nlibrary(alr4)\r\n\r\nhw3 <- pairs(water[,2:7])\r\n\r\n\r\n\r\n\r\nIt seems that all the AP___ predictors are heavily linearly related with other AP___ predictors. The same can be said with OP___ predictors being heavily linearly related with other OP___ predictors.\r\nOtherwise, the linear relationship from an AP___ predictor to an OP___ predictor is not as prevalent.\r\nQuestion 4\r\nProfessor ratings (ALR Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\n\r\n\r\nlibrary(alr4)\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\nhw4 <- Rateprof\r\nhw4_plot_var <- select(hw4, quality, clarity, helpfulness, easiness, raterInterest)\r\nhw4_pairs <- pairs(hw4_plot_var)\r\n\r\n\r\n\r\n\r\nThe quality, clarity, and helpfulness ratings seem to be highly, positively correlated with each other and demonstrate clear linear relationships with each other.\r\nThe easiness ratings also seem to be positively correlated with helpfulness, clarity, and quality. However the linear relationship is not as as strong compared to the relationships that quality, clarity, and helpfulness share with each other.\r\nRater interest\r\nQuestion 5\r\nFor the student.survey SMSS data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\nUse graphical ways to portray the individual variables and their relationship.\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\nSummarize and interpret results of inferential analyses.\r\n\r\n\r\nlibrary(smss)\r\nlibrary(ggplot2)\r\n\r\n## To convert the responses to numerical values, based on scaling\r\ndata(student.survey)\r\nhw5 <- student.survey\r\n\r\nre_mod <- unclass(hw5$re)\r\npi_mod <- unclass(hw5$pi)\r\n\r\nre_scale <- c(1:4)\r\npi_scale <- c(1:7)\r\nre_levels <- levels(hw5$re)\r\npi_levels <- levels(hw5$pi)\r\n\r\n# Outputs the scales and what it is associated with\r\nre_levels_df <- data.frame(re_scale, re_levels)\r\npi_levels_df <- data.frame(pi_scale, pi_levels)\r\n\r\n# Model fitting\r\ni5 <- lm(pi_mod ~ re_mod, hw5)\r\ni5_summ <- summary(i5)\r\ni5_plot <- ggplot(hw5, aes(re_mod, pi_mod)) + geom_point()\r\ni5_resumm <- summary(re_mod)\r\ni5_pisumm <- summary(pi_mod)\r\n\r\na5 <- data.frame(as.factor(i5_pisumm), as.factor(i5_resumm))\r\nnames(a5) <- c('PI', 'RE')\r\n\r\n# Correlation Test\r\ni5_cortest <- cor.test(pi_mod, re_mod)\r\n\r\n# Information fot TV Watching vs High School GPA\r\nii5 <- lm(hi ~ tv, hw5)\r\nii5_plot <- ggplot(hw5, aes(tv, hi)) + geom_point()\r\n\r\naa5 <- data.frame(as.factor(summary(hw5$tv)), as.factor(summary(hw5$hi)) )\r\nnames(aa5) <- c('TV', 'HI')\r\n\r\nii5_cortest <- cor.test(hw5$hi, hw5$tv)\r\n\r\n\r\n\r\nOutline of the response choices for Religious Services and Political Ideology\r\nre_scale\r\nre_levels\r\n1\r\nnever\r\n2\r\noccasionally\r\n3\r\nmost weeks\r\n4\r\nevery week\r\npi_scale\r\npi_levels\r\n1\r\nvery liberal\r\n2\r\nliberal\r\n3\r\nslightly liberal\r\n4\r\nmoderate\r\n5\r\nslightly conservative\r\n6\r\nconservative\r\n7\r\nvery conservative\r\n\r\na. Ideology vs Religious Services Responses Plot\r\n\r\n\r\n\r\nb. Summary Statistics for the Religious Services and Political Ideology responses\r\n\r\nPI\r\nRE\r\nMin.\r\n1\r\n1\r\n1st Qu.\r\n2\r\n1.75\r\nMedian\r\n2\r\n2\r\nMean\r\n3.03333333333333\r\n2.16666666666667\r\n3rd Qu.\r\n4\r\n3\r\nMax.\r\n7\r\n4\r\nThe median for the responses are that students go to religious services occasionally, and that the political ideology is liberal. The mean however, suggests that students still occasionally go to religious services occasionally, but that their political ideology is slightly liveral.\r\nc. Inferential Analyses\r\nCorrelation Test\r\nTest Statistic : 5.4162562\r\nP Value : 1.22113^{-6}\r\nAs far as the correlation test goes, it can be deemed that the correlation between the Political Ideology and Religious Services responses is statistically significant.\r\nHigh School GPA vs TV Watching Analysis\r\na. Plot outlining the relationship between GPA and the amount of TV watched\r\n\r\n\r\n\r\nb. Summary Statistics for High School GPA and Amount of TV Hours Watched\r\n\r\nTV\r\nHI\r\nMin.\r\n0\r\n2\r\n1st Qu.\r\n3\r\n3\r\nMedian\r\n6\r\n3.35\r\nMean\r\n7.26666666666667\r\n3.30833333333333\r\n3rd Qu.\r\n10\r\n3.625\r\nMax.\r\n37\r\n4\r\nThe lowest amount of TV watched is 0 hours, while the most amount of TV watched is 37 hours! On average, the amount of TV watched amongst this group is 7.3 hours, with the median being 6 hours.\r\nThe lowest GPA reported by the respondents is 2, with the highest being a 4. On average, the respondents have a 3.31 GPA with a median of a 3.35 GPA.\r\nc. Inferential Analyses\r\nCorrelation Test\r\nTest Statistic : -2.1143654\r\nP Value : 0.0387935\r\nThe correlation test shows that the correlation between hours of TV watched and GPA is statistically significant. Furthermore, the amount of TV watched has a negative relationship with GPA, reinforcing the cliche idea that student’s grades will suffer as more time spent watching on TV.\r\nQuestion 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\n\r\n\r\nhw6 <- read.csv(\"~/DACSS/DACSS603/hw6.csv\")\r\nhw6_ttest <- t.test(hw6$final, hw6$final_h1)\r\n\r\n\r\n\r\nI constructed a fake dataset, with one column displaying test scores that had the 10 people scoring 50, and another column scoring 60 on the next test, and not adjusting other test scores to average out to 70. After conducting a two sided T test with this scenario with a test statistic of -1 and a p-value of 0.319 , this test in itself suggests that there is not enough evidence to prove that the improvement was statistically significant.\r\nFurthermore, regression to the mean suggests that the most extreme of occurrences is not going to happen again and go back to the mean. In this testing case, the students that scored 50 will most likely score better the 2nd or 3rd time around.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomalexanderhong86876317/distill-preview.png",
    "last_modified": "2022-03-12T19:36:47-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomegeeslindacss-603-hw2/",
    "title": "HW2_DACSS603",
    "description": "DACSS 603 Homework 2: Linear Regression",
    "author": [
      {
        "name": "Eliza Geeslin",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nQuestion 1\n(Problem 1.1 in ALR)\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\nThe predictor is ppgdp and the response is fertility. So, in the scatterplot we will see that fertility is on the y-axis while ppgdp is on the x-axis.\n1.1.2. Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point(color = \"#4FB06D\") +\n  labs(y = \"Fertility (birth rate per 1000 females)\", x = \"PPGDP ($)\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nWe see that fertility decreases as ppgdp increases to a certain point, and then fertility appears to flatten out (around 1-2 births per female). Based on this graph it seems like a straight-line mean function is not the best summary of this graph.\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point(color = \"#4FB06D\") +\n  labs(y = \"Fertility (birth rate per 1000 females)\", x = \"PPGDP ($)\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nA simple linear regression does seem like a more plausible model for this graph. The mean function appears to be linear, and it is believable that there could be consistent varience across the points (although this is not completely certain). As we would expect, we see that there are a few outliers with either very high or very low fertility for their ppgdp.\nQuestion 2\n(Problem 9.47 in SMSS)\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n(a) How, if at all, does the slope of the prediction equation change?\nAssuming that the data itself is the same and the study was not re-done with based on income in Britain (just conversion), the new slope will be equal to the old slope divided by 1.33 (each British pound represents a smaller interval).\n(b) How, if at all, does the correlation change?\nThe correlation will not change (i.e. the relationship is the same regardless of the unit of income used to represent it).\nQuestion 3\n(Problem 1.5 in ALR)\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\n\n\n# use pairs() function\n\npairs(~ Year + APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE + BSAAM, \n      data = water,\n      col = \"#0068B1\",\n      pch = 20)\n\n\n\n\nSummary of relationship between variables based on this matrix:\nYear does not appear to be related to the the other variables (runoff or water levels).\nThe OPBPC, OPRC, and OPSLAKE sites seem to be correlated with each other. The plots that include two of these variables seem to show a dependence that is stronger than any dependence they have with the APSLAKE, APSAB, and APMAM sites.\nThe APSLAKE, APSAB, and APMAM sites also seem to be correlated with each other.\nBSAAM (the stream runoff) is more correlated with the OPBPC, OPRC, and OPSLAKE group. Perhaps this stream runoff drains to the OPBPC, OPRC, and OPSLAKE sites more than the others.\nIt seems like there is probably another stream that runsoff into the APSLAKE, APSAB, and APMAM sites that we are not accounting for here since they are all closely related to each other but not as much to BSAAM.\nQuestion 4\n(Problem 1.6 in ALR - slightly modified)\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\n\n\n# find the names of the variables\n# names(Rateprof)\n\n#use pairs() to graph matrix\npairs(~ quality + clarity + helpfulness + easiness + raterInterest, \n      data = Rateprof,\n      xlim = c(1,5),\n      ylim = c(1,5),\n      xaxs = \"i\", \n      yaxs = \"i\",\n      col = \"#FF5C35\",\n      pch = 20)\n\n\n\n\nSummary of relationship between variables based on this matrix:\nThere is a strong correlation between the quality, clarity, and helpfulness variables. This indicates that professors who rate highly in one of these categories may also rate highly in the other two.\nThere are weaker relationships between easiness and raterInterest and the rest of the variables.\nQuestion 5\n(Problem 9.34 in SMSS)\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\nUse graphical ways to portray the individual variables and their relationship.\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\nSummarize and interpret results of inferential analyses.\n\n\n# downloading the data\ndata(\"student.survey\")\n\n# taking a look at each variable.\n# ?student.survey\n\n#select the variables we need. \nstudent.survey <- student.survey %>%\n  select(c(pi, re, hi, tv))\n\nstr(student.survey)\n\n\n'data.frame':   60 obs. of  4 variables:\n $ pi: Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re: Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ hi: num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ tv: num  3 15 0 5 6 4 5 5 7 1 ...\n\nhead(student.survey)\n\n\n            pi           re  hi tv\n1 conservative   most weeks 2.2  3\n2      liberal occasionally 2.1 15\n3      liberal   most weeks 3.3  0\n4     moderate occasionally 3.5  5\n5 very liberal        never 3.1  6\n6      liberal occasionally 3.5  4\n\n\n\n# graph to portray individual variables and their relationship\n\nplot(pi ~ re, data = student.survey)\n\n\n\n\nUsing the datatable() and descriptive graph above, we see that for political ideology we see that the scale consists of very conservative, conservative, slightly conservative, moderate, slightly liberal, liberal, very liberal. The religiosity variable is how often the respondent attends religious services, and the options are: never, occasionally, most weeks, and every week.\n\n\n# graph to portray individual variables and their relationship\n\nplot(hi ~ tv, data = student.survey)\n\n\n\n\nHigh School GPA is on a 4 point scale (from 1 to 4) and TV is hours of watching TV.\nNext, in order to summarize these variables using descriptive statistics, I am going to assign numerical values to the political ideology and religiosity values as follows:\nvery conservative = 7, conservative = 6, slightly conservative = 5, moderate = 4, slightly liberal = 3, liberal = 2, very liberal = 1\nnever = 1, occasionally = 2, most weeks = 3, and every week = 4\n\n\n# re-assign values as integers and recreate table so I can double check\nstudent.survey <- student.survey %>%\n  add_column(pi_num = (as.integer(as.factor(student.survey$pi))), .after = 1)\n\nstudent.survey <- student.survey %>% \n  add_column(re_num = (as.integer(as.factor(student.survey$re))), .after = 3)\n             \nhead(student.survey)\n\n\n            pi pi_num           re re_num  hi tv\n1 conservative      6   most weeks      3 2.2  3\n2      liberal      2 occasionally      2 2.1 15\n3      liberal      2   most weeks      3 3.3  0\n4     moderate      4 occasionally      2 3.5  5\n5 very liberal      1        never      1 3.1  6\n6      liberal      2 occasionally      2 3.5  4\n\n\n\n#select only numerical values\n\nstudent.survey <- student.survey %>%\n  select(c(pi_num, re_num, hi, tv))\n\n#summarize data\nsummary(student.survey)\n\n\n     pi_num          re_num            hi              tv        \n Min.   :1.000   Min.   :1.000   Min.   :2.000   Min.   : 0.000  \n 1st Qu.:2.000   1st Qu.:1.750   1st Qu.:3.000   1st Qu.: 3.000  \n Median :2.000   Median :2.000   Median :3.350   Median : 6.000  \n Mean   :3.033   Mean   :2.167   Mean   :3.308   Mean   : 7.267  \n 3rd Qu.:4.000   3rd Qu.:3.000   3rd Qu.:3.625   3rd Qu.:10.000  \n Max.   :7.000   Max.   :4.000   Max.   :4.000   Max.   :37.000  \n\n\n\n# use stat.desc() to summarize data\nstat.desc(student.survey)\n\n\n                  pi_num      re_num           hi          tv\nnbr.val       60.0000000  60.0000000  60.00000000  60.0000000\nnbr.null       0.0000000   0.0000000   0.00000000   5.0000000\nnbr.na         0.0000000   0.0000000   0.00000000   0.0000000\nmin            1.0000000   1.0000000   2.00000000   0.0000000\nmax            7.0000000   4.0000000   4.00000000  37.0000000\nrange          6.0000000   3.0000000   2.00000000  37.0000000\nsum          182.0000000 130.0000000 198.50000000 436.0000000\nmedian         2.0000000   2.0000000   3.35000000   6.0000000\nmean           3.0333333   2.1666667   3.30833333   7.2666667\nSE.mean        0.2112201   0.1261482   0.05934157   0.8672043\nCI.mean.0.95   0.4226505   0.2524220   0.11874221   1.7352718\nvar            2.6768362   0.9548023   0.21128531  45.1225989\nstd.dev        1.6361040   0.9771398   0.45965782   6.7173357\ncoef.var       0.5393749   0.4509876   0.13893939   0.9244040\n\nNow we have a lot of information:\npolitical ideology - the medium ideology was a 2 which is liberal while the mean ideology was a ~3 which is slightly liberal. Using the stat.desc() means we also know the standard deviation is 1.64 and the 95% confidence interval of the mean is 3.03 ± .24.\nreligiosity - the median is 2 while the mean is ~2 which is occasionally attending religious services. The standard deviation is .98 and the 95% confidence interval of the mean is 2 ± .25.\nhigh school GPA - the median GPA was a 3.35 while the mean was 3.31; the maximum GPA was a 4.0 and the minimum was a 2.0.\nhours watching TV - the median hours watched was 6 hours and the mean was 7.27 hours. The minimum hours watched was 0 while the maximum was 37 hours.\n\n\n# plot the relationship between religiosity and political ideology - scatterplot\nggplot(data = student.survey, aes(x = re_num, y = pi_num)) +\n  geom_point(color = \"#5C62D6\") +\n  labs(x = \"Religiosity (frequency of attending services)\", y = \"Political Ideology\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nBased on this it looks like there is weak evidence of a correlation between religiosity (as measured by attendence) and political ideology from this graph. Slightly conservative, moderate, slightly liberal and liberal respondents vary from never, occasionally, most weeks, and every week service attendance. However, we do see that on either end (very liberal and conservative/very conservative) there is some relationship between ideology and religiosity.\n\n\n# plot the relationship between HS GPA and hours of TV watched - scatterplot\nggplot(data = student.survey, aes(y = hi, x = tv)) +\n  geom_point(color = \"#ED2D40\") +\n  labs(x = \"TV watched per week (Avg Hrs.)\", y = \"High School GPA\") +\n  theme(panel.background = element_rect(fill = \"#F6F9FC\"), \n        axis.text.x = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#192733\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#192733\", size=11),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#192733\", size=11))\n\n\n\n\nThis is a repeat of the summary graph above (plot()) but we do glean some interesting information. In general, most students are watching less than 15 hours of TV on average per week. Most respondents to this survey had a GPA of 3.0 or greater. It seems like there is not a strong relationship between the average number of hours of TV watched per week and the student’s GPA.\nFinally, I used cor.test() to test the correlation between each of these pairs. HO is that the correlation coefficient = 0, while HA is that the correlation coefficient =/= 0.\n\n\n# correlation test between political ideology and religiosity using cor.test()\n\ncor.test(student.survey$pi_num, student.survey$re_num)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  student.survey$pi_num and student.survey$re_num\nt = 5.4163, df = 58, p-value = 1.221e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3818345 0.7265650\nsample estimates:\n      cor \n0.5795661 \n\nFor political ideology and religiosity (how often you attend religious services) there is a p-value of less than the 0.05 significance level. Since the p-value is smaller than 0.05, we can reject the null hypothesis (that there is no relationship). So, we can assume that these two do have a significant relationship to each other. The estimated correlation/correlation coefficient is 0.560 so the correlation shows a modest positive correlation.\n\n\n# correlation test between GPA and TV using cor.test()\n\ncor.test(student.survey$hi, student.survey$tv)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  student.survey$hi and student.survey$tv\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nFor the relationship between high school GPA and the average hours of TV watched per week there is a p-value of 0.039 which is less than the 0.05 significance level. Therefore, we can assume that these two do have a significant relationship to each other. Since the p-value is .039 we can reject the null hypothesis. However, the relationship is not as significant as the one we looked at above. Additionally, the estimated correlation is -0.268, so there is a small negative correlation.\nQuestion 6\n(Problem 9.50 in SMSS)\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\nWe don’t have enough evidence (enough data) to imply that the tutoring program was responsible for the increased scores. Regression towards the mean is the idea that if one sample of a random variable is extreme, the next sampling of the same random variable is likely to be closer to its mean and extreme events are likely to be followed by more typical ones. In the example in the question, the group that was selected to receive tutoring was the “extreme” group (performing the lowest on the midterm exam). On the next exam (the final), the average for the class was 70 and the 10 students in the extreme group’s average regressed toward the mean. Therefore, we do not know if the increase was due to the tutoring.\n\n\n\n",
    "preview": "posts/httpsrpubscomegeeslindacss-603-hw2/distill-preview.png",
    "last_modified": "2022-03-12T19:36:51-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomjflatteryhomework2/",
    "title": "Homework 2",
    "description": "Homework 2",
    "author": [
      {
        "name": "Justin Flattery",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\n\nlibrary('smss')\nlibrary('alr4')\n\n\n\n1\n1.1.1\n\n\n\nPredictor variable is ppdg and response variable is fertility\n1.1.2\n\n\nplot(x = UN11$ppgdp, y = UN11$fertility)\n\n\n\n\nA straight line mean function does not seem to be plausible for a summary of this graph. It appears to be a non linear relationship between the x and y axis.\n1.1.3\n\n\nplot(x = log(UN11$ppgdp), y = log(UN11$fertility))\n\n\n\n\nYes! There seems to be a linear relationship when using the log of these values. Therefore It appears a simple linear regression would be plausible for this graph\n2\na\nThe slope of the prediction will change by 1/1.6 (=0.625) since previously it was predicting slope based on every 1 unit changein the explanatory variable. Now the rate of that one unit is decreasing to equal the same change in the response variable\nb\nThe overall correlation will not change in the regression analysis since the units of the explanatory variable are all moving at an equal rate.\n3\n\n\ndata('water')\n\n\n\n\n\npairs(~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE + BSAAM, data = water)\n\n\n\n\nUsing the information from these plots, we can see a linear relationship appears plausible to explain the OP sites and Bishop runoff, however the relationship for AP sites and runoff seems to be more randomly related.\n4\n\n\ndata('Rateprof')\n\npairs(~quality+helpfulness+clarity+easiness+raterInterest,data=Rateprof,\n      main=\"Professor Rating Scatterplot Matrix\")\n\n\n\n\nAccording to this scatterplot there is a positive linear relationship between the variables of quality, helpfulness and clarity. However the variables of easiness and raterInterest do not appear correlated or have a linear relationship with any of the other variables analyzed\n5\n\n\ndata('student.survey')\n\n\n\n(a)\n\n\nplot(x = student.survey$re, y = student.survey$pi)\n\n\n\n\n\n\nplot(x = student.survey$tv, y = student.survey$hi)\n\n\n\n\nb)\ni)\nFor political ideology, there is a scale from very conservative to moderate to liberal Religiosity is measured by how often you attend a religious service It appears according to the plot in a that the more religious service attended, the more conservative individuals were.\nii)\nFor high school GPA, ranked on scale from 1-4 hours of watching TV is measured by average number of hours per week that individuals watched TV It appears according to the plot in a that the more tv watched, the lower individuals GPA were.\nc)\ni)\n\n\nsummary(plot(x = student.survey$re, y = student.survey$pi)) \n\n\n\nNumber of cases in table: 60 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 42.29, df = 18, p-value = 0.001008\n    Chi-squared approximation may be incorrect\n\nSince this is a categorical comparison, regression can not necessarily be run (without substituing dummy variables) However, we can see the p value for this relationship is <0.01 and therefore it appears there is a significant relationship\nii)\nFirst, running a regression analysis\n\n\nlm(re ~ pi, data = student.survey)\n\n\n\nCall:\nlm(formula = re ~ pi, data = student.survey)\n\nCoefficients:\n(Intercept)         pi.L         pi.Q         pi.C         pi^4  \n     2.6071       2.0552       0.4501       0.1361      -0.2283  \n       pi^5         pi^6  \n    -0.3046       0.4565  \n\n\n\nsummary(lm(tv ~ hi, data = student.survey))\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.600 -3.790 -1.167  2.408 27.746 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   20.200      6.175   3.271   0.0018 **\nhi            -3.909      1.849  -2.114   0.0388 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.528 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\nI see there is a significant relationship with a low p value, however a low r2 value. However, viewing our plot, it appears a logarithmic relationship may be a better fit for this data\n\n\nsummary(lm((tv) ~ log(hi), data = student.survey))\n\n\n\nCall:\nlm(formula = (tv) ~ log(hi), data = student.survey)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.846 -4.035 -1.172  2.508 27.933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   20.933      6.717   3.116  0.00285 **\nlog(hi)      -11.525      5.619  -2.051  0.04481 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.542 on 58 degrees of freedom\nMultiple R-squared:  0.06761,   Adjusted R-squared:  0.05154 \nF-statistic: 4.206 on 1 and 58 DF,  p-value: 0.04481\n\nNow we see a higher r^2 value an inverse relationship between the hours of tv watched and log of an individuals GPA.The equation relating these two is Log(GPA) = -11.525 * hours of tv watched + 21\n6\nSince these students were the lowest performers in the previous test, they are more likely to have improved test scores closer to the mean in the next test. The findings are in line with the concept of regression toward the mean. For example, if we took the top 10 highest perfoming students and provided them tutoring, their test scores are actually expected to get closer to the mean in the next sample (i.e go down!). We could not conclude therefore that tutoring had a negative affect, similarly we do not have enough evidence with the information provided to state the tutoring program is successful.\n\n\n\n",
    "preview": "posts/httpsrpubscomjflatteryhomework2/distill-preview.png",
    "last_modified": "2022-03-12T19:36:55-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomofyalcinhw2-sol/",
    "title": "Homework 2 | Solutions",
    "description": "Solutions to Homework 2",
    "author": [
      {
        "name": "Omer Yalcin",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nQuestion 1\n\n\ndata(UN11)\n\n\n\n1.1.1\nThe predictor is ppgdp, i.e. GDP per capita. The response is fertility, the birth rate per 1000 women.\n1.1.2.\n\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\nA straight line is not appropriate, because the relationship has an L-shaped structure.\n1.1.2\n\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\nYes, now a simple linear regression model is more plausible. We can imagine a negative-sloped straight line going through those points.\nQuestion 2\n(a)\nThe conversion from USD to British pound will mean the numerical value of the response will be divided by 1.33. To offset that, the slope will also become divided by 1.33.\n(b)\nCorrelation will not change because it is a standardized measure that is not influenced by the unit of measurement.\nBoth outcomes can easily be shown via simulation.\nQuestion 3\n\n\ndata(water)\npairs(water)\n\n\n\n\nYear appears to be largely unrelated to each of the other variables\nthe three variables starting with “O” seem to be correlated with each other, meaning that all the plot including two of these variables exhibit a dependence between the variables that is stronger than the dependence between the “O” variables and other variables. The three variables starting with “A” also seem to be another correlated group\nBSAAM is more closely related to the “O” variables than the “A” variables\nQuestion 4\n\n\ndata(Rateprof)\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\n\n\n\nThe very strong pair-wise correlation among quality, clarity, and helpfulness is very striking. easiness is also correlated fairly highly with the other three. raterInterest is also moderately correlated, but raters almost always say they are at least moderately interested in the subject. Overall, the results might show that people don’t necessarily distinguish all these dimensions very well in their minds—or that professors that do one in one dimension tend to do well on the others too.\nQuestion 5\na\nOne way of visually representing the relationship between religiosity and political ideology is as follows (and there are other ways). As we go towards bars to the right (more religiousity), we see lighter colors pop up (more conservatism)\n\n\ndata(student.survey)\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\n\n\n\nThe relationship between high school GPA and hours of watching TV can be shown with a good old scatter plot.\n\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\n\n\n\nb\nWe can get some descriptive statistics with:\n\n\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nFor the categorical variables, we have somewhat of a skewed sample: not all categories have a similar number of people. While the high school GPA distribution looks like a normal-looking unimodal distribution, the hours of TV variable has quite a few outliers, apparent from the difference between 3rd quartile and the maximum value While the high school GPA distribution looks like a normal-looking unimodal distribution, the TV variable has some outliers, apparent from the difference between 3rd quartile and the maximum value.\nc\nDealing with ordinal variables in linear regression is a difficult problem. We’ll just go ahead and assume that we can just convert them to numeric and use them. This would be done for political ideology and religiosity. High school GPA and hours of TV are already continuous.\n\n\nm1 <- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\nm2 <- lm(hi ~ tv, data = student.survey)\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\n\n\n==========================================================\n                                  Dependent variable:     \n                              ----------------------------\n                               Pol. Ideology     HS GPA   \n                                    (1)            (2)    \n----------------------------------------------------------\nReligiosity                       0.970***                \n                                  (0.179)                 \n                                                          \nHours of TV                                     -0.018**  \n                                                 (0.009)  \n                                                          \nConstant                          0.931**       3.441***  \n                                  (0.425)        (0.085)  \n                                                          \n----------------------------------------------------------\nObservations                         60            60     \nR2                                 0.336          0.072   \nAdjusted R2                        0.324          0.056   \nResidual Std. Error (df = 58)      1.345          0.447   \nF Statistic (df = 1; 58)         29.336***       4.471**  \n==========================================================\nNote:                          *p<0.1; **p<0.05; ***p<0.01\n\nReligiosity is positively and statistically significantly (at the 0.01 significance level) associated with conservatism.\nHours of TV is negatively and statistically significantly (at the 0.05 significance level) associated with High School GPA. Watching an average of 1 more hour of TV per week is associated with a 0.018 decline in High School GPA.\nQuestion 6\nRegression toward the mean implies that extreme values will always occur by chance and in a next iteration of the test / experiment, those observations’ expected value will still be whatever the previous mean was. In this sense, they’re expected to regress toward the mean.\nIn this example, the tutored students might have been chosen from those who performed very poorly by chance, and the improvement might just be an artifact of regression toward the mean.\n\n\n\n",
    "preview": "posts/httpsrpubscomofyalcinhw2-sol/distill-preview.png",
    "last_modified": "2022-03-12T19:37:00-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomchester876831/",
    "title": "Homework Two",
    "description": "DACSS 603",
    "author": [
      {
        "name": "Cynthia Hester",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\n\nContents\nQuestion 1\nSolution\n\nQuestion 2\nSolution\n\nQuestion 3\nSolution\n\nQuestion 4\nSolution\n\nQuestion 5\nSolution\n\nQuestion 6\nSolution\n\n\nQuestion 1\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nSolution\n1.1.1. Identify the predictor and the response.\nThe predictor is ppgdp. This is because the problem is studying the dependence of fertility on ppgdp (gross national product per person) which is independent/explanatory.\nThe response variable is fertility. This is because fertility is the dependent variable in relation to ppgdp.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph? \nFirst, we import the data from the United Nations (Datafile:UN11) contained in data accompanying the ALR text book data set.\n\n\nhide\n\ndata(\"UN11\")                      #load UN11 data, which contains\nUnited_Nations_11<-UN11           #\"UN11\" renamed for easier readability\nhead(United_Nations_11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\nI now separate the two variables, fertility and ppgdp from the UN11 dataset for inspection\n\n\nhide\n\n#separates fertility and ppgdp from dataset\n\nUnited_Nations_11 <- United_Nations_11 %>%\n  select(c(fertility,ppgdp))\nhead(United_Nations_11,5)               # First five rows of variables fertility and ppgdp verified\n\n\n            fertility   ppgdp\nAfghanistan     5.968   499.0\nAlbania         1.525  3677.2\nAlgeria         2.142  4473.0\nAngola          5.135  4321.9\nAnguilla        2.000 13750.1\n\nHere I use a table to represent the variables extracted from the UN11 data\n\n\nhide\n\nkable(head(United_Nations_11),format = \"markdown\",digits = 3,colnames = c('fertility','ppgdp'),\n      caption = \"United Nations Fertility and Gross National Product Per Person in USD\")\n\n\nTable 1: United Nations Fertility and Gross National Product Per Person in USD\n\nfertility\nppgdp\nAfghanistan\n5.968\n499.0\nAlbania\n1.525\n3677.2\nAlgeria\n2.142\n4473.0\nAngola\n5.135\n4321.9\nAnguilla\n2.000\n13750.1\nArgentina\n2.172\n9162.1\n\nHere I look at a summary of the two variables as well as remove any missing data\n\n\nhide\n\nis.na(United_Nations_11) %>% #removes missing values from variables we\n  str()                      #provides a concise overview of the data set\n\n\n logi [1:199, 1:2] FALSE FALSE FALSE FALSE FALSE FALSE ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:199] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ : chr [1:2] \"fertility\" \"ppgdp\"\n\nI now look at a numerical summary of the two variables to get a more granular overview of the data.\n\n\nhide\n\n  summary(United_Nations_11)                 #numerical summary\n\n\n   fertility         ppgdp         \n Min.   :1.134   Min.   :   114.8  \n 1st Qu.:1.754   1st Qu.:  1283.0  \n Median :2.262   Median :  4684.5  \n Mean   :2.761   Mean   : 13012.0  \n 3rd Qu.:3.545   3rd Qu.: 15520.5  \n Max.   :6.925   Max.   :105095.4  \n\nVariables fertility and ppgdp renamed for better understandability\n\n\nhide\n\nUnited_Nations_rename<-UN11 %>%\n  rename(Gross_National_Product_Per_Person_USD=ppgdp)%>%\n  rename(Fertility_Birthrate_per_1000_females_from_2009=fertility)\n\n\n\nWe now look at the scatterplot of the UN11 (renamed\nUnited_Nations_11) data using fertility on the vertical axis versus ppgdp on the horizontal axis.\n\n\nhide\n\nUnited_Nations_11<-UN11\nggplot(data = UN11, aes(x = ppgdp ,y = fertility))+\n  geom_point(color=5)+\n  labs(title = \"Fertility vs United Nations Gross National Product Per Person USD\")\n\n\n\n\nAnalysis\nThis linear regression scatter plot does not appear to be an effective summary of the data. The mean is not linear and the variance is not constant. This could be partly attributed to the crowding of the x-axis and y-axis. Therefore, using natural logarithms for the x-axis and y-axis would be warranted to determine if there is any indication of the plausibility of a straight-line mean function.\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nScatterplot reflecting Natural Log of variables fertility vs. ppgdp\n\n\nhide\n\nUnited_Nations_11<-UN11\nggplot(data = UN11,aes(x = log(ppgdp),y = log(fertility)))+\n  geom_point(color=5)+\n  labs(title = \"Natural_Log of Fertility vs UN Gross National Product Per Person USD\")\n\n\n\n\nScatterplot reflecting Natural Log of variables fertility vs. ppgdp with linear regression\n\n\nhide\n\nUnited_Nations_11<-UN11\nggplot(data = UN11,aes(x = log(ppgdp),y = log(fertility)))+\n  geom_point(color=5)+\n  geom_smooth(method =\"lm\")+\n  labs(title = \"Natural_Log of Fertility vs UN Gross National Product Per Person USD\")\n\n\n\n\nAnalysis\nImplementation of the natural logarithmic scale for the UN11 scatterplot appears to indicate an effective representation of a linear regression. Compared to the previous scattorplot, this plot appears to be linear, and the variance seems to be plausible. Therefore, the relationship between fertility and gross domestic product(ppgdp )is linear.\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\nSolution\nPart A\nHow, if at all, does the slope of the prediction equation change?\nWhen responses are converted to British pounds sterling the slope changes,thus the slope increases by 1.33 times the original. This is because there is an inverse relationship between the slope,and the explanatory variables.\nPart B\nHow, if at all, does the correlation change?\nThere will be no change in the correlation. This is because the strength and pattern (correlation) cannot be affected by change in units.\nQuestion 3\nWater runoff in the Sierras (Data file: water) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM.\nDraw the scatterplot matrix for these data and summarize the information available from these plots.\nSolution\nFirst, we load the (Data files: water) and check the structure of the data\n\n\nhide\n\ndata(\"water\")                       #importing water data set\nhead(water,5)                       #looking at first 5 rows of data set\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n\n\n\nhide\n\nstr(water)                          #concise look at data frame\n\n\n'data.frame':   43 obs. of  8 variables:\n $ Year   : int  1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 ...\n $ APMAM  : num  9.13 5.28 4.2 4.6 7.15 9.7 5.02 6.7 10.5 9.1 ...\n $ APSAB  : num  3.58 4.82 3.77 4.46 4.99 5.65 1.45 7.44 5.85 6.13 ...\n $ APSLAKE: num  3.91 5.2 3.67 3.93 4.88 4.91 1.77 6.51 3.38 4.08 ...\n $ OPBPC  : num  4.1 7.55 9.52 11.14 16.34 ...\n $ OPRC   : num  7.43 11.11 12.2 15.15 20.05 ...\n $ OPSLAKE: num  6.47 10.26 11.35 11.13 22.81 ...\n $ BSAAM  : int  54235 67567 66161 68094 107080 67594 65356 67909 92715 70024 ...\n\nhide\n\nsummary(water)                      #provides numerical overview of data\n\n\n      Year          APMAM            APSAB           APSLAKE     \n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \n     OPBPC             OPRC           OPSLAKE           BSAAM       \n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \n\nFor better readability and understandability of the data, a table is created\n\n\nhide\n\nlibrary(DT)\ndatatable(head(water,25), options = list(\n  columnDefs = list(list(className = 'dt-center',targets = 7)),\n  rownames = T,\n  pageLength = 5,\n  autowidth = T,\n  lengthMenu =c(5,10,15,20)\n))\n\n\n\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"],[1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972],[9.13,5.28,4.2,4.6,7.15,9.7,5.02,6.7,10.5,9.1,8.75,8.1,3.75,10.15,6.15,12.75,7.35,11.25,4.05,12.65,4.65,5.35,4.05,5.9,9.45],[3.58,4.82,3.77,4.46,4.99,5.65,1.45,7.44,5.85,6.13,5.23,3.77,1.47,5.09,3.52,8.17,4.33,6.56,1.9,6.62,3.84,3.62,1.98,5.72,4.82],[3.91,5.2,3.67,3.93,4.88,4.91,1.77,6.51,3.38,4.08,5.9,4.56,1.78,4.86,3.3,10.16,4.85,7.6,2,7.14,3.34,4.62,2.94,5.42,6.79],[4.1,7.55,9.52,11.14,16.34,8.88,13.57,9.28,21.2,9.55,15.25,9.05,4.57,8.9,16.9,16.75,5.25,8.4,10.85,23.25,7.1,43.37,8.95,8.45,7.9],[7.43,11.11,12.2,15.15,20.05,8.15,12.45,9.65,18.55,9.2,14.8,6.85,6.1,7.15,14.75,11.55,7.45,13.2,8.25,17,6.8,24.85,11.25,10.9,7.6],[6.47,10.26,11.35,11.13,22.81,7.41,13.32,9.8,17.42,8.25,17.48,9.56,7.65,9,17.68,15.53,8.2,13.29,12.56,23.66,8.28,33.07,11,10.82,8.06],[54235,67567,66161,68094,107080,67594,65356,67909,92715,70024,99216,55786,46153,47947,76877,88443,54634,78806,56542,116244,60857,146345,73726,65530,60772]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Year<\\/th>\\n      <th>APMAM<\\/th>\\n      <th>APSAB<\\/th>\\n      <th>APSLAKE<\\/th>\\n      <th>OPBPC<\\/th>\\n      <th>OPRC<\\/th>\\n      <th>OPSLAKE<\\/th>\\n      <th>BSAAM<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-center\",\"targets\":7},{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,8]},{\"orderable\":false,\"targets\":0}],\"rownames\":true,\"pageLength\":5,\"autowidth\":true,\"lengthMenu\":[5,10,15,20],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nRemoving any missing data\n\n\nhide\n\nis.na(water) %>%     #removes missing values from variables we are working with\n  summary()          #summary of data set, provides numerical insight\n\n\n    Year           APMAM           APSAB          APSLAKE       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:43        FALSE:43        FALSE:43        FALSE:43       \n   OPBPC            OPRC          OPSLAKE          BSAAM        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:43        FALSE:43        FALSE:43        FALSE:43       \n\nDraw the scatterplot matrix for these data and summarize the information available from these plots.\nWe now draw a scatterplot matrix for the water data for better understandably\n\n\nhide\n\n#Since data has already been imported we can rename the water variable\nwater_supply<-water\nhead(water_supply,5)            #first five rows of dataset\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n\nScatterplot matrix for water data\n\n\nhide\n\npairs(water_supply,main = \"Sierra Southern California Water Supply Runoff\",\n      pch = 21, bg = \"green\")\n\n\n\n\nFor more granular insight, we run a simple linear regression on the water data set\n\n\nhide\n\n#Simple linear regression of water data set\n\nlm_water_supply<-lm(BSAAM~APMAM+APSAB+APSLAKE+OPBPC+OPRC+OPSLAKE,data = water)\nsummary(lm_water_supply)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\nAnalysis:\nStarting with variable Year we see that it does not seem to be appear to be particularly related to any of the other variables.APSLAKE,APSAB and APMAM appear to be correlated, and it is observed in relation to the runoff variable BSAAM they do not seem to appear to have a strong correlation.\nIt appears that OPSLAKE and OPRC are strongly correlated as well being positively linear. This suggests statistical significance of the p-value in the linear regression Pr(>|t|)< 0.05.The BSAAM variable appears to be strongly correlated with them as well.Since these variables appear to be correlated with each other, it may cause a multicollinearity problem. Furthermore, with a p-value: < 2.2e-16 the model appears to be statistically significant. The residuals display of the regression a wide disparity from a Minimum of -12690 to a Maximum of 18542. This may suggest outliers in the model.\nFinally, the multiple R-squared: value of 0.9248 and the Adjusted R-squared: 0.9123  values are close to 1 which suggests there is minimal overfitting.\nQuestion 4\nProfessor ratings (Data file: Rateprof) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\nSolution\nFirst, the data (Data file: Rateprof) is imported from the ALR4 dataset\n\n\nhide\n\ndata(\"Rateprof\")\nhead(Rateprof,5)                           #first 5 rows of data\n\n\n  gender numYears numRaters numCourses pepper discipline\n1   male        7        11          5     no        Hum\n2   male        6        11          5     no        Hum\n3   male       10        43          2     no        Hum\n4   male       11        24          5     no        Hum\n5   male       11        19          7     no        Hum\n               dept  quality helpfulness  clarity easiness\n1           English 4.636364    4.636364 4.636364 4.818182\n2 Religious Studies 4.318182    4.545455 4.090909 4.363636\n3               Art 4.790698    4.720930 4.860465 4.604651\n4           English 4.250000    4.458333 4.041667 2.791667\n5           Spanish 4.684211    4.684211 4.684211 4.473684\n  raterInterest sdQuality sdHelpfulness sdClarity sdEasiness\n1      3.545455 0.5518564     0.6741999 0.5045250  0.4045199\n2      4.000000 0.9020179     0.9341987 0.9438798  0.5045250\n3      3.432432 0.4529343     0.6663898 0.4129681  0.5407021\n4      3.181818 0.9325048     0.9315329 0.9990938  0.5882300\n5      4.214286 0.6500112     0.8200699 0.5823927  0.6117753\n  sdRaterInterest\n1       1.1281521\n2       1.0744356\n3       1.2369438\n4       1.3322506\n5       0.9749613\n\nMissing values are removed and then a summary of the data is run,to gain insight into the structure of the data.\n\n\nhide\n\nis.na(Rateprof) %>%        #removes missing values from variables we\nsummary()                  #insight into numerical data\n\n\n   gender         numYears       numRaters       numCourses     \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n   pepper        discipline         dept          quality       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n helpfulness      clarity         easiness       raterInterest  \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n sdQuality       sdHelpfulness   sdClarity       sdEasiness     \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:366       FALSE:366       FALSE:366       FALSE:366      \n sdRaterInterest\n Mode :logical  \n FALSE:366      \n\nhide\n\nstr(Rateprof)\n\n\n'data.frame':   366 obs. of  17 variables:\n $ gender         : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ numYears       : int  7 6 10 11 11 10 7 11 11 7 ...\n $ numRaters      : int  11 11 43 24 19 15 17 16 12 18 ...\n $ numCourses     : int  5 5 2 5 7 9 3 3 4 4 ...\n $ pepper         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ discipline     : Factor w/ 4 levels \"Hum\",\"SocSci\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dept           : Factor w/ 48 levels \"Accounting\",\"Anthropology\",..: 17 42 3 17 45 45 45 17 34 17 ...\n $ quality        : num  4.64 4.32 4.79 4.25 4.68 ...\n $ helpfulness    : num  4.64 4.55 4.72 4.46 4.68 ...\n $ clarity        : num  4.64 4.09 4.86 4.04 4.68 ...\n $ easiness       : num  4.82 4.36 4.6 2.79 4.47 ...\n $ raterInterest  : num  3.55 4 3.43 3.18 4.21 ...\n $ sdQuality      : num  0.552 0.902 0.453 0.933 0.65 ...\n $ sdHelpfulness  : num  0.674 0.934 0.666 0.932 0.82 ...\n $ sdClarity      : num  0.505 0.944 0.413 0.999 0.582 ...\n $ sdEasiness     : num  0.405 0.505 0.541 0.588 0.612 ...\n $ sdRaterInterest: num  1.128 1.074 1.237 1.332 0.975 ...\n\nFive variables we are focused on extracted from Rateprof dataset.\n\n\nhide\n\n#data set renamed for subset\nrate_my_prof<-Rateprof\ncolnames(rate_my_prof)            #column names in dataset\n\n\n [1] \"gender\"          \"numYears\"        \"numRaters\"      \n [4] \"numCourses\"      \"pepper\"          \"discipline\"     \n [7] \"dept\"            \"quality\"         \"helpfulness\"    \n[10] \"clarity\"         \"easiness\"        \"raterInterest\"  \n[13] \"sdQuality\"       \"sdHelpfulness\"   \"sdClarity\"      \n[16] \"sdEasiness\"      \"sdRaterInterest\"\n\nFive variable subset of RateProf dataset\n\n\nhide\n\n#subset of RateProf data\n\nrate_my_prof<-select(Rateprof,c('quality','helpfulness','clarity','easiness','raterInterest'))\nhead(rate_my_prof)     #first 5 rows of data set\n\n\n   quality helpfulness  clarity easiness raterInterest\n1 4.636364    4.636364 4.636364 4.818182      3.545455\n2 4.318182    4.545455 4.090909 4.363636      4.000000\n3 4.790698    4.720930 4.860465 4.604651      3.432432\n4 4.250000    4.458333 4.041667 2.791667      3.181818\n5 4.684211    4.684211 4.684211 4.473684      4.214286\n6 4.233333    4.266667 4.200000 4.533333      3.916667\n\nTable of subset of data for better understandability\n\n\nhide\n\nkable(head(rate_my_prof),format = \"markdown\",digits = 5,\ncolnames = c('Quality','Helpfulness','Clarity','Easiness','RaterInterest'),caption\n= \"Rate My Professor\")\n\n\nTable 2: Rate My Professor\nquality\nhelpfulness\nclarity\neasiness\nraterInterest\n4.63636\n4.63636\n4.63636\n4.81818\n3.54545\n4.31818\n4.54545\n4.09091\n4.36364\n4.00000\n4.79070\n4.72093\n4.86047\n4.60465\n3.43243\n4.25000\n4.45833\n4.04167\n2.79167\n3.18182\n4.68421\n4.68421\n4.68421\n4.47368\n4.21429\n4.23333\n4.26667\n4.20000\n4.53333\n3.91667\n\nScatterplot Matrix of five RateProf variables\n\n\nhide\n\npairs(rate_my_prof,\n      col = \"green3\",\n      pch = 20,\n      main = \"Rate my Professor Matrix ScatterPlot \")\n\n\n\n\nProvide a brief description of the relationships between the five ratings\nInterpretation:\nWe see that if there is an intersection of any two variables then there is linear correlation of varying degrees of strength. Furthermore,if the correlation is not as linear then the correlation is weak.\nWe see that the relationship between some pairs of variables indicate better positive linear correlations than others.\nQuality-Clarity indicates a very strong positive linear correlation\nQuality-Happiness indicates a very strong positive linear correlation\nQuality-Easiness indicates a weak positive linear correlation\nQuality-RaterInterest indicates a weak positive linear correlation\nHelpfulness-Easiness indicates a weak linear correlation\nHelpfulness-RaterInterest indicates a weak linear correlation\nClarity-Helpfulness indicates a positive correlation\nClarity-RaterInterest indicates a weak positive correlation\nClarity-Easiness indicates a weak positive correlation\nEasiness and RaterInterest indicates a very weak positive correlation\nQuestion 5\n(Problem 9.34 in SMSS)\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\nUse graphical ways to portray the individual variables and their relationship.\n\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\n\nSummarize and interpret results of inferential analyses.\n\nSolution\nI first, import and inspect the student.survey dataset from SMSS text.\n\n\nhide\n\ndata(\"student.survey\")                   #import dataset\nstudent_survey_data<-student.survey      #renamed for better understandability\nhead(student_survey_data,5)              #inspects first five rows of data\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\n            re    ab    aa    ld\n1   most weeks FALSE FALSE FALSE\n2 occasionally FALSE FALSE    NA\n3   most weeks FALSE FALSE    NA\n4 occasionally FALSE FALSE FALSE\n5        never FALSE FALSE FALSE\n\nTo gain better understanding of the data I look at a summery, strings,and column names of the data set\n\n\nhide\n\ncolnames(student_survey_data)                 #column names of dataset\n\n\n [1] \"subj\" \"ge\"   \"ag\"   \"hi\"   \"co\"   \"dh\"   \"dr\"   \"tv\"   \"sp\"  \n[10] \"ne\"   \"ah\"   \"ve\"   \"pa\"   \"pi\"   \"re\"   \"ab\"   \"aa\"   \"ld\"  \n\nhide\n\nsummary(student_survey_data)                  #numeric structure of data\n\n\n      subj       ge           ag              hi       \n Min.   : 1.00   f:31   Min.   :22.00   Min.   :2.000  \n 1st Qu.:15.75   m:29   1st Qu.:24.00   1st Qu.:3.000  \n Median :30.50          Median :26.50   Median :3.350  \n Mean   :30.50          Mean   :29.17   Mean   :3.308  \n 3rd Qu.:45.25          3rd Qu.:31.00   3rd Qu.:3.625  \n Max.   :60.00          Max.   :71.00   Max.   :4.000  \n                                                       \n       co              dh             dr               tv        \n Min.   :2.600   Min.   :   0   Min.   : 0.200   Min.   : 0.000  \n 1st Qu.:3.175   1st Qu.: 205   1st Qu.: 1.450   1st Qu.: 3.000  \n Median :3.500   Median : 640   Median : 2.000   Median : 6.000  \n Mean   :3.453   Mean   :1232   Mean   : 3.818   Mean   : 7.267  \n 3rd Qu.:3.725   3rd Qu.:1350   3rd Qu.: 5.000   3rd Qu.:10.000  \n Max.   :4.000   Max.   :8000   Max.   :20.000   Max.   :37.000  \n                                                                 \n       sp               ne               ah             ve         \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Mode :logical  \n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 0.000   FALSE:60       \n Median : 5.000   Median : 3.000   Median : 0.500                  \n Mean   : 5.483   Mean   : 4.083   Mean   : 1.433                  \n 3rd Qu.: 7.000   3rd Qu.: 5.250   3rd Qu.: 2.000                  \n Max.   :16.000   Max.   :14.000   Max.   :11.000                  \n                                                                   \n pa                         pi                re         ab         \n d:21   very liberal         : 8   never       :15   Mode :logical  \n i:24   liberal              :24   occasionally:29   FALSE:60       \n r:15   slightly liberal     : 6   most weeks  : 7                  \n        moderate             :10   every week  : 9                  \n        slightly conservative: 6                                    \n        conservative         : 4                                    \n        very conservative    : 2                                    \n     aa              ld         \n Mode :logical   Mode :logical  \n FALSE:59        FALSE:44       \n NA's :1         NA's :16       \n                                \n                                \n                                \n                                \n\nhide\n\nstr(student_survey_data)\n\n\n'data.frame':   60 obs. of  18 variables:\n $ subj: int  1 2 3 4 5 6 7 8 9 10 ...\n $ ge  : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 2 2 1 2 2 ...\n $ ag  : int  32 23 27 35 23 39 24 31 34 28 ...\n $ hi  : num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ co  : num  3.5 3.5 3 3.2 3.5 3.5 3.7 3 3 3.1 ...\n $ dh  : int  0 1200 1300 1500 1600 350 0 5000 5000 900 ...\n $ dr  : num  5 0.3 1.5 8 10 3 0.2 1.5 2 2 ...\n $ tv  : num  3 15 0 5 6 4 5 5 7 1 ...\n $ sp  : int  5 7 4 5 6 5 12 3 5 1 ...\n $ ne  : int  0 5 3 6 3 7 4 3 3 2 ...\n $ ah  : int  0 6 0 3 0 0 2 1 0 1 ...\n $ ve  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ pa  : Factor w/ 3 levels \"d\",\"i\",\"r\": 3 1 1 2 2 1 2 2 2 2 ...\n $ pi  : Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re  : Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ ab  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ aa  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ ld  : logi  FALSE NA NA FALSE FALSE NA ...\n\nSubset of data extracted from student survey dataset we are inspecting\n\n\nhide\n\nstudent_survey_data<-student_survey_data %>% \n  select(c(pi,re,hi,tv))               #data subset for plots\n  head(student_survey_data,5)\n\n\n            pi           re  hi tv\n1 conservative   most weeks 2.2  3\n2      liberal occasionally 2.1 15\n3      liberal   most weeks 3.3  0\n4     moderate occasionally 3.5  5\n5 very liberal        never 3.1  6\n\nMissing values are removed and then a summary of the data is run to gain insight and inspect structure of the data.\n\n\nhide\n\nis.na(student_survey_data) %>%         #removes missing values from data \nsummary(student_survey_data)\n\n\n     pi              re              hi              tv         \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:60        FALSE:60        FALSE:60        FALSE:60       \n\nFor easier readability the data is placed in a table.\n\n\nhide\n\nkable(head(student_survey_data),format = \"markdown\",digits = 3,\ncol.names = c('Political_Ideology','Religiosity','HighSchoolGPA','HoursTVWatched'),caption\n= \"Student_Survey_Data\")\n\n\nTable 3: Student_Survey_Data\nPolitical_Ideology\nReligiosity\nHighSchoolGPA\nHoursTVWatched\nconservative\nmost weeks\n2.2\n3\nliberal\noccasionally\n2.1\n15\nliberal\nmost weeks\n3.3\n0\nmoderate\noccasionally\n3.5\n5\nvery liberal\nnever\n3.1\n6\nliberal\noccasionally\n3.5\n4\n\nPart A\nUse graphical ways to portray the individual variables and their relationship.\ni) Now that we know the structure of the data we can represent it visually with a plot. The first plot represents variables: y = political ideology(pi) vs x = religiosity(re).\n\n\nhide\n\nstudent_survey_plot<-plot(pi~re,data = student.survey,\n        #survey plot using plot function\n                          main = \"Political Ideology vs. Religiosity\")\n\n\n\n\nI use xtabs() function to gain further insight into the data since it is categorical.\n\n\nhide\n\ndata(\"student.survey\")\nxtabs(~pi+re,student.survey)\n\n\n                       re\npi                      never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nii)\nThe second plot is represented visually with the following variables: y = high school GPA(hi) and x = hours of TV watching(tv). Since the variables are numeric it is more straight forward for interpretation.\n\n\nhide\n\nstudent_survey_plot<-plot(hi~tv,data = student.survey,\n                          xlab=\"Hours of TV Watching\",\n                          ylab=\"High school GPA\",\n                          col = \"green\",\n                          main = \"High School GPA vs. Hours of TV \nWatching\")                #survey plot using plot function\n\n\n\n\nAnalysis:\nInspection of the Political Ideology(pi) vs. Religiosity(re) plot yielded very little meaningful information in its current form, as a categorical subset of the data. I used the xtabs() function to gain further insight in to the data since it is categorical. Whereas, the Hours of TV Watching(tv) and High school GPA(hi) data are numeric and did yield some insight. I will further explore any correlations in Part B.\nPart B\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\ni)\nI rename all 4 variables of the subset for better understandability\n\n\nhide\n\nstudent_survey_rename<-student.survey %>% \n  rename(Political_Ideology=pi) %>% \n  rename(Religiosity=re) %>% \n  rename(Hours_of_TV=tv) %>% \n  rename(HighSchool_GPA=hi)\n\n\n\nI then use my favorite tool, the table to gain further insight into the political ideology and religiosity variables.\n\n\nhide\n\ndata(\"student.survey\")\nxtabs(~Political_Ideology+Religiosity,student_survey_rename)\n\n\n                       Religiosity\nPolitical_Ideology      never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nSummary of 4 variables pi,re,hi,tv\n\n\nhide\n\nsummary(student_survey_data)\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nInterpretation:\nThe relationship between the two categorical variables Political Ideology and Religiosity , where Political Ideology is the dependent variable and Religiosity independent, yields the following insight based on the xtabs and summary analysis. A mode of 24 for liberal Political Ideology associated to a mode of 29 for the liberal sample Religiosity occasionally. This result coincides with the xtabs table indicating a mode of 14 of the liberal sample who attend a religious service occasionally.\nii)\nHere I use a linear model to determine if there is a correlation between Highschool GPA and Hours Watching TV\n\n\nhide\n\nggscatter(student.survey,x=\"tv\",y=\"hi\",\n          add = \"reg.line\",conf.int = TRUE,\n          xlab = \"Hours Watching TV\",ylab = \"HighSchool_GPA\",title = \"HighSchool_GPA vs Hours Watching TV\")\n\n\n\n\nI can now gain further insight into the variables high school gpa(hi) and hours watching tv(tv)\n\n\nhide\n\nskim(student_survey_data)      #provides concise,descriptive insight into the data\n\n\nTable 4: Data summary\nName\nstudent_survey_data\nNumber of rows\n60\nNumber of columns\n4\n_______________________\n\nColumn type frequency:\n\nfactor\n2\nnumeric\n2\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\npi\n0\n1\nTRUE\n7\nlib: 24, mod: 10, ver: 8, sli: 6\nre\n0\n1\nTRUE\n4\nocc: 29, nev: 15, eve: 9, mos: 7\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nhi\n0\n1\n3.31\n0.46\n2\n3\n3.35\n3.62\n4\n▂▁▇▇▆\ntv\n0\n1\n7.27\n6.72\n0\n3\n6.00\n10.00\n37\n▇▃▁▁▁\n\nhide\n\nstr(student_survey_data)\n\n\n'data.frame':   60 obs. of  4 variables:\n $ pi: Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re: Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ hi: num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ tv: num  3 15 0 5 6 4 5 5 7 1 ...\n\nhide\n\nsummary(student_survey_data)\n\n\n                     pi                re           hi       \n very liberal         : 8   never       :15   Min.   :2.000  \n liberal              :24   occasionally:29   1st Qu.:3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350  \n moderate             :10   every week  : 9   Mean   :3.308  \n slightly conservative: 6                     3rd Qu.:3.625  \n conservative         : 4                     Max.   :4.000  \n very conservative    : 2                                    \n       tv        \n Min.   : 0.000  \n 1st Qu.: 3.000  \n Median : 6.000  \n Mean   : 7.267  \n 3rd Qu.:10.000  \n Max.   :37.000  \n                 \n\nInterpretation:\nStudents Highschool GPA has a mean of 3.31gpa and median of 3.35gpa. The GPA’s are within a range of 2.00 for minimum and 4.00 for maximum. It has a standard deviation of 0.46 which indicates the data are clustered around the mean. This is verified in the graph.\nStudents Hours Watching TV has a mean of 7.3 hours, median of 6 hours. The hours watched range from a minimum of 0 to a maximum value of 37 hours per week.\nPART C\nSummarize and interpret results of inferential analyses.\ni)\nTo gain better insight into the political ideology and religiosity variables, I use the function cor.test() (as discussed in class).The cor.test() function will provide an association or correlation between paired samples political ideology(pi) and religiosity variables(re).\n\n\nhide\n\n# correlation test\ncor.test(as.numeric(student_survey_data$pi),as.numeric(student_survey_data$re))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  as.numeric(student_survey_data$pi) and as.numeric(student_survey_data$re)\nt = 5.4163, df = 58, p-value = 1.221e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3818345 0.7265650\nsample estimates:\n      cor \n0.5795661 \n\nInterpretation:\nTest_Statistic = 5.4163 The p-value = 1.221e-06 is less p-value is <0.05 we see the correlation between  political ideology and religiosity variables** is statistically significant since the p-value is < 0.05 we therefore reject the null hypothesis.\nTo gain better insight into the “HighSchoolGPA vs Hours Watching TV” I use a cor.test as discussed in class\n\n\nhide\n\ncor.test(as.numeric(student_survey_data$hi),as.numeric(student_survey_data$tv))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  as.numeric(student_survey_data$hi) and as.numeric(student_survey_data$tv)\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\nii)\nPearson correlation plot “HighSchoolGPA vs Hours Watching TV”\n\n\nhide\n\nggscatter(student.survey,x=\"tv\",y=\"hi\",\n          add = \"reg.line\",conf.int = TRUE,\n          cor.coef=TRUE,cor.method = \"pearson\",\n          xlab = \"Hours Watching TV\",ylab = \"HighSchool_GPA\",title = \"HighSchool_GPA vs Hours Watching TV\")\n\n\n\n\nInterpretation:\nTest statistic = -2.1144\nBased on the cor.test we see the correlation between gpa and television watching is statistically significant since the p-value is < 0.05 at p-value: 0.039, we therefore reject the null hypothesis. Graphically we also observe there is a moderately weak negative correlation between gpa and television watching.\nQuestion 6\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful.\nSolution\nRegression toward the mean is an idea that refers to the fact that if one sample of a random variable is extreme, the next sampling of the same random variable is likely to be closer to its mean.\nIn this case, the original sample mean of the specially tutored students was 50 which is very low compared to the overall class sample mean of 70; therefore the next sampling of the same set of the specially tutored students is likely to produce a value of a sample mean closer to the overall mean of 70; and this is purely due to the statistical phenomenon of regression towards mean and not due to any special effect of the special tutoring program ; even without the special tutoring program, this second sample mean would most likely be closer to 70 than the first sample mean.\n\n\n\n",
    "preview": "posts/httpsrpubscomchester876831/distill-preview.png",
    "last_modified": "2022-03-12T19:37:04-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomcdaviau872505/",
    "title": "HW #1",
    "description": "First homework assignment for DACSS 603",
    "author": [
      {
        "name": "Chris Daviau",
        "url": {}
      }
    ],
    "date": "2022-03-06",
    "categories": [],
    "contents": "\nProblem Set 1\n1.\nQuestion:\nThe time between the date a patient was recommended for heart surgery\nand the surgery date for cardiac patients in Ontario was collected by\nthe Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health\nand Long-Term Care, Ontario, Canada, 2006). The sample mean and sample\nstandard deviation for wait times (in days) of patients for two cardiac\nprocedures are given in the accompanying table. Assume that the sample\nis representative of the Ontario population. Construct the 90%\nconfidence interval to estimate the actual mean wait time for each of\nthe two procedures. Is the confidence interval narrower for angiography\nor bypass surgery?\n\n[1] 18.21042 19.78958\n[1] 17.42495 18.57505\n[1] \"The confidence interval for angiography is narrower compared to bypass surgeries\"\n\n2.\nQuestion:\nA survey of 1031 adult Americans was carried out by the National Center\nfor Public Policy. Assume that the sample is representative of adult\nAmericans. Among those surveyed, 567 believed that college education is\nessential for success. Find the point estimate, p, of the proportion of\nall adult Americans who believe that a college education is essential\nfor success. Construct and interpret a 95% confidence interval for\np.\n\n\np <- 567/1031\nn <- 1031\nx <- 567\n\n# We are 95% certain that p falls between 0.519 and 0.580\nprop.test(x,n,p)\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  x out of n, null probability p\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n3.\nQuestion:\nSuppose that the financial aid office of UMass Amherst seeks to estimate\nthe mean cost of textbooks per quarter for students. The estimate will\nbe useful if it is within 5 dollars of the true population mean\n(i.e. they want the confidence interval to have a length of 10 dollars\nor less). The financial aid office is pretty sure that the amount spent\non books varies widely, with most values between 30 and 200 dollars.\nThey think that the population standard deviation is about a quarter of\nthis range. Assuming the significance level to be 5%, what should be the\nsize of the sample?\n\n\n# Std. dev. \n  text_sd <- (200-30)/4\n\n# Z-score for .05 siginificance level\n  z <- 1.96\n  \n# Referring to the confidence interval formula, it can be broken down into point estimate +/- margin of error\n# In this case, UMass specifically wants our CI length to be 10 dollars or less, so accounting for both tails:\n  e <- 10/2\n\n# Since z(se) = e, and se = s/sqrt(n), and we know all other variables except n,\n# We can create a formula to solve for n:\n  n <- (text_sd^2 * z^2)/e^2\n\n# The size of the sample should be 278\nprint(n)\n\n\n[1] 277.5556\n\n4.\nQuestion:\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union\nagreement, the mean income for all senior-level workers in a large\nservice company equals 500 dollars per week. A representative of a\nwomen’s group decides to analyze whether the mean income μ for female\nemployees matches this norm. For a random sample of nine female\nemployees, ȳ = $410 and s = 90.\n4a.\nTest whether the mean income of female employees differs from $500 per\nweek. Include assumptions, hypotheses, test statistic, and P-value.\nInterpret the result.\n\n\n# T Statistic\n(410-500)/(90/sqrt(9))\n\n\n[1] -3\n\n# Determine p-value, multiply by 2 to account for both tails\n# The p-value obtained indicates the mean is significantly different from $500\npt(-3, 8)*2\n\n\n[1] 0.01707168\n\n4b. Report the\nP-value for Ha : μ < 500. Interpret.\n\n\n# This time we only account for one side since it's \"less than 500\"\n# The p-value indicates the mean is significantly less than $500\npt(-3, 8)\n\n\n[1] 0.008535841\n\n4c.\nReport and interpret the P-value for H a: μ > 500. (Hint: The\nP-values for the two possible one-sided tests must sum to 1.)\n\n\n# We only want the right side since it's \"greater than 500\", which is why lower.tail=FALSE\n# The p-value indicates that the mean is not significantly greater than $500\npt(-3, 8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n5.\nQuestion:\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith\nseparately conduct studies to test H0: μ = 500 against Ha : μ = 500,\neach with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ =\n519.7,with se = 10.0.\n5a.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and\nP-value = 0.049 for Smith.\n\n\nJ_t_score <- (519.5-500)/10\n\nJ_p_value <- pt(J_t_score, 999, lower.tail = FALSE)*2\n\nS_t_score <- (519.7-500)/10\n\nS_p_value <- pt(S_t_score, 999, lower.tail = FALSE)*2\n\n# Jones\nprint(J_t_score)\n\n\n[1] 1.95\n\nprint(J_p_value)\n\n\n[1] 0.05145555\n\n# Smith\nprint(S_t_score)\n\n\n[1] 1.97\n\nprint(S_p_value)\n\n\n[1] 0.04911426\n\n5b.\nUsing α = 0.05, for each study indicate whether the result is\n“statistically significant.”\nSmith’s results are statistically significant since the p-value is\nless than .05, but Jones’ results are not since the p-value is greater\nthan .05.\n5c.\nUsing this example, explain the misleading aspects of reporting the\nresult of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0”\nversus “Do not reject H0 ,” without reporting the actual P-value.\nIt’s misleading to report a p-value is significant without providing\nthe actual value because, as shown above, the p-value could be\nsignificant, but only by a small margin and doesn’t tell the whole\nstory\n6.\nQuestion:\nAre the taxes on gasoline very high in the United States? According to\nthe American Petroleum Institute, the per gallon federal tax that was\nlevied on gasoline was 18.4 cents per gallon. However, state and local\ntaxes vary over the same period. The sample data of gasoline taxes for\n18 large cities is given below in the variable called gas_taxes. Is\nthere enough evidence to conclude at a 95% confidence level that the\naverage tax per gallon of gas in the US in 2005 was less than 45 cents?\nExplain.\n\n\n\n\n\n# There is not enough evidence to conclude that the average tax per gallon of gas was less than 45 cents\n# The conf. interval contains the value 45, meaning the average very well could be 45\nt.test(gas_taxes, conf.level = .95)$conf.int\n\n\n[1] 36.23386 45.49169\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-06T19:21:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomkpopiela873483/",
    "title": "Homework 1",
    "description": "DACSS-603",
    "author": [
      {
        "name": "Katie Popiela",
        "url": {}
      }
    ],
    "date": "2022-03-06",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nTo calculate the confidence intervals of bypass and angiography procedures we must use the following formula: x +/- z(s/sqrt(n)). X represents the sample mean, z represents the z-value, s represents the sample standard deviation, and n represents sample size.\r\nBypass:\r\nIn this part of the problem, variables x, z, s, and n hold the following values:\r\nx = 19,\r\nz = 1.645 (for 90% confidence interval),\r\ns = 10,\r\nn = 539.\r\n\r\n\r\n19+1.645*(10/(sqrt(539)))\r\n\r\n\r\n[1] 19.70855\r\n\r\n\r\n\r\n19-1.645*(10/(sqrt(539))) \r\n\r\n\r\n[1] 18.29145\r\n\r\nConfidence Interval = 18.29145, 19,70855\r\nAngiography\r\nIn this part of the problem, variables x, z, s, and n hold the following values:\r\nx = 18,\r\nz = 1.645,\r\ns = 9,\r\nn = 847\r\n\r\n\r\n18+1.645*(9/(sqrt(847)))\r\n\r\n\r\n[1] 18.50871\r\n\r\n\r\n\r\n18-1.645*(9/(sqrt(847)))\r\n\r\n\r\n[1] 17.49129\r\n\r\nConfidence Interval = 17.49129, 18.50871\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\nTo find the point estimate p, with a 95% confidence interval we must use the following formula:\r\np +/- z*(sqrt(p(1 p)/n))\r\nin which p represents the sample proportion, z represents the chosen z-value, and n represents sample size. The values these variables take are listed in the code chunk below.\r\n\r\n\r\np <-0.54995  \r\nz <-1.96  \r\nn <-1031  \r\np+1.96*(sqrt(p*(1-p)/n)) \r\n\r\n\r\n[1] 0.5803182\r\n\r\n\r\n\r\np-1.96*(sqrt(p*(1-p)/n))\r\n\r\n\r\n[1] 0.5195818\r\n\r\nConfidence Interval = 0.5195818, 0.5803182\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nWe must first start with the confidence interval formula:\r\nx +/- z*(s/sqrt(n)) = 5 in which\r\nx = sample mean, z = chosen z-value, s = sample standard deviation, and n = sample size. 5 accounts for the $5 range the cost estimate must be within.\r\nBut because we are looking for variable n (sample size), we have to reorganize the equation;\r\nz*(s/5)^2 = n\r\n\r\n\r\nf <-function(n, z = 1.96, s = 42.5) {\r\n  res <- z*s/sqrt(n)\r\n  return(res)\r\n}\r\n\r\nvec <- vapply(1:300, FUN = f, FUN.VALUE = 5.0)\r\nwhich(vec < 5) [1]\r\n\r\n\r\n[1] 278\r\n\r\nOnce we transition the formula into code, we must then create a vector in order to see the lowest value the sample size can be to achieve a mean textbook cost within $5 of the true population mean.\r\nThe result indicates that the sample must contain at least 278 people to achieve an estimate within $5 of the true population mean.\r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\na. Test whether the mean income of female employees differs from $500/week. Include assumptions, hypotheses, test statistic, and p-value. Interpret the result.b. Report the p-value for \\(H_{a}\\) : μ < 500. Interpret.c. Report and interpret the P-value for \\(H_{a}\\) : μ > 500.(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nIn order to test whether or not the mean income for female employees differs from $500/week, we must first condect a one-sample, two-sided significance test.\r\nWe can also assume the following:\r\n1. The sample is random and the population has a normal distribution\r\n2. The mean income for all senior-level workers = $500/week\r\n3. From the random sample of 9 female employees, the mean income = $410/week\r\n4. Standard deviation = 90\r\n5. Null Hypothesis: \\(H_{0}\\): μ = 500\r\n6. Alternative Hypothesis: \\(H_{a}\\): μ ≠ 500\r\n\r\n\r\n(410 - 500)/(90/sqrt(9))\r\n\r\n\r\n[1] -3\r\n\r\nThe test statistic/t-test value is -3.\r\nNow onto the P-value\r\n\r\n\r\nrandom_sample_n <- 9  \r\ndf_n <- (random_sample_n - 1)  \r\nt_test <- (410 - 500)/(90/sqrt(9))  \r\np_value <- pt(t_test, df_n)*2  \r\nprint(p_value)\r\n\r\n\r\n[1] 0.01707168\r\n\r\nInterpretation: We know that the p-value is 0.01707. Assuming α = 0.05, we can see that 0.01707 < 0.05, meaning we can reject the null hypothesis. As a result, we have sufficient evidence to assert that the mean income for female employees differs from the general mean of $500/week.\r\nThe next part of the question asks us to report the p-value for Ha: μ < 500, and then interpret.\r\nHypotheses\\(H_{0}\\) : μ = $500/week\\(H_{a}\\) : μ = <$500/week\r\nP-Value = p(t < t_test)p(t<-3)\r\nP-value for \\(H_{a}\\): μ > 500 (left tail test) using the formula pt(q,df,lower.tail=TRUE,log.p=FALSE)\r\n\r\n\r\nq <- -3\r\nrandom_sample_n <- 9\r\ndf_n <- (random_sample_n-1)\r\nleft_p_value <- pt(q,df_n,lower.tail=TRUE,log.p=FALSE)\r\nprint(left_p_value)\r\n\r\n\r\n[1] 0.008535841\r\n\r\nP= 0.0085, which can be rounded to 0.01. This indicates that there is strong evidence against the mean weekly income being $500 or more.\r\nNext we must calculate the P-value for \\(H_{0}\\) : μ < 500 (right tail test)\r\n\r\n\r\nq <- -3\r\nrandom_sample_n <- 9\r\ndf_n <- (random_sample_n-1)\r\nright_p_value <- pt(q,df_n,lower.tail = FALSE,log.p=FALSE)\r\nprint(right_p_value)\r\n\r\n\r\n[1] 0.9914642\r\n\r\nThe P-value for \\(H_{0}\\) : μ < 500 is 0.99. This indicates strong evidence in favor of the null hypothesis, going against the claim that mean μ > 500. To ensure these findings are in fact correct, we have to confirm that the sum of left_p_value and right_p_value = 1.\r\n\r\n\r\nleft_p_value <- 0.01\r\nright_p_value <- 0.99\r\ntotal_sum_lr <- left_p_value + right_p_value\r\nprint(total_sum_lr)\r\n\r\n\r\n[1] 1\r\n\r\nAs is shown above, the sum of the left and right tails is 1.\r\nQuestion 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7,with se = 10.0.\r\na. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nJones\r\nWe will start with a t-test, using the formula: t = (\\(\\overline{y}\\) - μ)/10.0\r\n\\(\\overline{y}\\) = 519.5 μ = 500 se = 10.0\r\n\r\n\r\nt_test <- (519.5-500)/10.0\r\nprint((519.5-500)/10.0)\r\n\r\n\r\n[1] 1.95\r\n\r\nWe have thus shown that for Jones, t = 1.95. Now, onto the p-value.\r\n\r\n\r\nn <- 1000\r\ndf_j <- (n - 1)  \r\nt_test <- (519.5-500)/10.0  \r\np_value <- pt(t_test, df_j,lower.tail = FALSE,log.p = FALSE)*2  \r\nprint(p_value)\r\n\r\n\r\n[1] 0.05145555\r\n\r\nThe presented p-value of 0.051 is in fact correct, as the math shows. The next step is doing the same calculations for Smith, in which t = 1.97 and P-value = 0.049\r\nSmith\r\nT-test:\r\n\\(\\overline{y}\\) = 519.7 μ = 500 se = 10.0\r\n\r\n\r\nt_test <- (519.7 - 500)/10.0\r\nprint(t_test)\r\n\r\n\r\n[1] 1.97\r\n\r\nOnce again the presented value for t has been confirmed as correct (t=1.97). Now that we have this information, we can calculate and hopefully confirm the p-value as well.\r\nP-value:\r\n\r\n\r\nn <- 1000\r\ndf_s <- (n - 1)  \r\nt_test <- (519.7-500)/10.0  \r\np_value <- pt(t_test, df_s,lower.tail = FALSE,log.p = FALSE)*2  \r\nprint(p_value)\r\n\r\n\r\n[1] 0.04911426\r\n\r\nSmith’s p-value is correct at 0.049.\r\nb. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\r\njones_p_value = 0.051\r\nsmith_p_value = 0.049\r\n\\(\\alpha\\) = 0.05\r\nIn order for a p-value to be statistically significant, it must be greater than 0.05. Smith’s p-value is 0.049 which, while close, is still less than 0.05. Jones’s p-value, however, is statistically significant at 0.051.\r\nc. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nBoth studies yielded extremely similar results, but the difference is great enough that only Jones’s work was statistically significant.However, given the closeness in result values between the two, we can see that both have moderate evidence against \\(H_{0}\\).\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAssumptions:\r\ngas_taxes_sample <- 18 df_gt <- gas_taxes_sample-1 95% confidence interval Significance level: \\(\\alpha\\) = 0.05 (based on confidence interval)\r\nTo start, we must calculate the t-score to find the upper and lower intervals of gas_taxes_sample\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\ngas_taxes_sample <- 18\r\ndf_gt <- gas_taxes_sample - 1\r\nmean_gt <- mean(gas_taxes)\r\ntscore_gt <- qt(p=0.05,df=df_gt,lower.tail=FALSE)\r\ngas_sd <- sd(gas_taxes)\r\nme_gas_taxes <- qt(0.05,df = df_gt)*gas_sd/sqrt(18)\r\n\r\nlower_int_gt<-(mean_gt-me_gas_taxes)\r\nprint(lower_int_gt)\r\n\r\n\r\n[1] 44.67946\r\n\r\nThe lower bound of gas_taxes = 44.67946\r\nNow to find the upper bound:\r\n\r\n\r\nupper_int_gt <- (mean_gt + me_gas_taxes)\r\nprint(upper_int_gt)\r\n\r\n\r\n[1] 37.0461\r\n\r\nThe upper bound is 37.0461.Therefore, the confidence interval (at 95%) is [37.0461, 44.6794].\r\nThe average tax/gallon of gas is less than $0.45, so it is within the upper and lower bounds of the confidence interval. However, we will test an alternate outcome via a t-test\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nmean(gas_taxes)\r\n\r\n\r\n[1] 40.86278\r\n\r\n\r\n\r\nt.test(gas_taxes,conf.level=0.95)\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 9.555e-13\r\nalternative hypothesis: true mean is not equal to 0\r\n95 percent confidence interval:\r\n 36.23386 45.49169\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nIn this scenario, the confidence interval is [36.2338, 45.4916]. Therefore there isn’t enough evidence to conclude that at a 95% confidence level the average tax per gallon of gas was less than $0.45 in the US in 2005 since $0.45 is within the confidence interval (which contains tax rates greater than $0.45).\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-06T19:21:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomofyalcinhw1-sol/",
    "title": "Homework 1 - Solutions",
    "description": "Solutions for homework 1.",
    "author": [
      {
        "name": "Omer Yalcin",
        "url": {}
      }
    ],
    "date": "2022-03-06",
    "categories": [],
    "contents": "\nPlease check your answers against the solutions.\nQuestion 1\n\n\nbypass_n = 539\nangio_n = 847\n\nbypass_sample_mean = 19\nangio_sample_mean = 18\n\nbypass_sample_sd = 10\nangio_sample_sd = 9\n\nbypass_se = bypass_sample_sd/sqrt(bypass_n)\nangio_se = angio_sample_sd/sqrt(angio_n)\n\nbypass_me = qt(0.95, df = bypass_n - 1)*bypass_se\nangio_me = qt(0.95, df = angio_n - 1)*angio_se\n\n\n\nThe confidence intervals:\n\n\nprint(bypass_sample_mean + c(-bypass_me, bypass_me))\n\n\n[1] 18.29029 19.70971\n\nprint(angio_sample_mean + c(-angio_me, angio_me))\n\n\n[1] 17.49078 18.50922\n\nThe size of the confidence intervals, which is twice the margin of error:\n\n\n2 * bypass_me\n\n\n[1] 1.419421\n\n2 * angio_me\n\n\n[1] 1.018436\n\nThe confidence interval for angiography is narrower.\nQuestion 2\none-step solution:\n\n\nn = 1031\nk = 567\nprop.test(k, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  k out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nAlternatively:\n\n\np_hat <- k/n # point estimate\nse = sqrt((p_hat*(1-p_hat))/n) # standard error\ne = qnorm(0.975)*se # margin of error\np_hat + c(-e, e) # confidence interval \n\n\n[1] 0.5195839 0.5803191\n\nAlternatively, we can use the exact binomial test. In large samples like the one we have, the results should essentially be the same as prop.test().\n\n\nbinom.test(k, n)\n\n\n\n    Exact binomial test\n\ndata:  k and n\nnumber of successes = 567, number of trials = 1031, p-value =\n0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515 \n\nQuestion 3\n\n\nrange = 200-30\npopulation_sd = range/4\n\n\n\nRemember:\n\\[CI_{95} = \\bar x \\pm z \\frac{s}{\\sqrt n}\\] (We can use \\(z\\) because we assume population standard deviation is known.)\nWe want the number \\(n\\) that ensures:\n\\[ z \\frac{s}{\\sqrt n} = 5 \\] \\[ zs = 5 \\sqrt n\\] \\[ \\frac{zs}{5} = \\sqrt n\\] \\[  (\\frac{zs}{5})^2 = n\\]\nIn our case:\n\n\nz = qnorm(.975)\ns = population_sd\nn = ((z *s) / 5)^2\nprint(n)\n\n\n[1] 277.5454\n\nRounding up, we need a sample of 278.\nQuestion 4\nWe can write a function to find the t-statistic, and then do all the tests in a, b, and c using that.\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nwhere \\(\\bar x\\) is them sample mean, \\(\\mu\\) is the hypothesizes population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nWriting this in R:\n\n\nget_t_stat <- function(x_bar, mu, sd, n){\n  return((x_bar - mu) / (sd / sqrt(n)))\n}\n\n\n\nFind the t-statistic:\n\n\nt_stat <- get_t_stat(x_bar = 410, mu = 500, sd = 90, n = 9)\n\n\n\nA\nTwo-tailed test\n\n\nn = 9\npval_two_tail = 2*pt(t_stat, df = n-1)\npval_two_tail\n\n\n[1] 0.01707168\n\nWe can reject the hypothesis that population mean is 500.\nB\n\n\npval_lower_tail = pt(t_stat, df = n-1)\npval_lower_tail\n\n\n[1] 0.008535841\n\nWe can reject the hypothesis that population mean is greater than 500.\nC\n\n\npval_upper_tail = pt(t_stat, df = n-1, lower.tail=FALSE)\npval_upper_tail\n\n\n[1] 0.9914642\n\nWe fail to reject the hypothesis that population mean is less than 500.\nAlternatively for C, we could just subtract the answer in B from 1:\n\n\n1 - pval_lower_tail\n\n\n[1] 0.9914642\n\nQuestion 5\n\n\nt_jones = ((519.5 - 500)/ 10)\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\n\nt value for Jones: 1.95 \n\ncat(\"t value for Smith:\", t_smith, '\\n')\n\n\nt value for Smith: 1.97 \n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Jones: 0.0515 \n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Smith: 0.0491 \n\nAt 0.05 level Smith’s result is statistically significant but Jones’s is not. The result show the arbitrariness of the 0.05 demarcation line and the importance of reporting actual p-values to better make sense of results.\nQuestion 6:\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\nIn the one sided test, we are able to reject the null in favor of the alternative that the gas taxes are less than 45 cents.\nNote that a two-sided test at the same level would not have resulted in the rejection of the null.\nHowever, a two-sided 90% confidence interval gives the same upper bound, since now there is a 5% rejection are on two sides:\n\n\nt.test(gas_taxes, mu = 45, alternative = 'two.sided', conf.level = 0.9)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.07654\nalternative hypothesis: true mean is not equal to 45\n90 percent confidence interval:\n 37.04610 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-06T19:21:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/603-homework-1/",
    "title": "603, Homework 1",
    "description": "First homework assignment for DACSS 603",
    "author": [
      {
        "name": "Joe Davis",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nI wanted to do this one a bit longer-form than necessary, but I also appreciate the chance to practice organizing my R code neatly while solving. I need some good repetition to build those good habits.\nFor this question I assigned all of the values in the table to objects to use in the calculations of each treatment’s t-scores, margins of error, and confidence intervals. I did a quick check of the qnorm(.95) to see if the t-scores had converged to the normal distribution given the relatively high sample sizes of each treatment. Surprisingly, to me at least, rounding at the .000 level they still had very slightly different levels both to the normal distribution and to each others’ scores.\nI know I have the standard deviations, wait time would be a continuous variable, and these samples are larger than n = 30 so I can use qnorm and the 1.645 z critical value clearing those assumptions. But, it seems like I should use the distribution with fatter tails relative to the sample size values from qt regardless if I could assume normality from the other conditions, as to err on the side of caution for analyzing and communicating medical procedure data. I have a note in my code on that point, mostly for my own reference and will solve for the normal distribution values as well. After running the calculations the practical effect would be introducing slightly more uncertainty in the mean wait time for the angiography procedure while still keeping that procedures’ interval narrower, as we’d expect, than that of the bypass procedure. Especially if this was to be used to set patient expectations on wait times, the wider range would be the option a hospital or doctor would communicate, but we do appear to be in the range where t and z distributions are becoming quite similar.\n\n\n## Means, Total N,  and SDs from full question text. B = Bypass A = Angiography\n  #mean wait times for each procedure\n  mean_b <- 19 \n  mean_a <- 18 \n  \n  #standard deviations\n  sd_b <- 10\n  sd_a <- 9\n  \n  #total N\n  n_b <- 539\n  n_a <- 847\n\n## 90% CI score finding. .90 = 1 - a and I need  a/2 for the two tails since I have no theory on why the direction of the error would be important. Did a little side exploring of the t distribution and normal distribution scores below.\n  \n\n#Rounded to 1.645 but not used for calculations. I thought it was a sufficiently large sample size that that qt and qnorm should have returned the same values at .000 rounding, but after checking those, each T rounded up differently at the 3rd digits and for medical data I would rather err on the side of caution  \n\n    #normal z score\n  z_heart <- qnorm(.95) \n  round(z_heart, digits = 3)\n\n\n[1] 1.645\n\n  #check this for t scores\nb_t <- round(qt(.95, df = n_b -1), digits = 3)\na_t <-  round(qt(.95, df = n_a -1), digits = 3)\n  \n  #print them\n  b_t\n\n\n[1] 1.648\n\n  a_t\n\n\n[1] 1.647\n\n## 90% CI standard error of mean/ margin of error\n  #Take the score multiplied by standard deviations and sqrt of Ns, t value\n  mofe_bypass <- b_t*(sd_b/sqrt(n_b))\n  mofe_angiography <- a_t*(sd_a/sqrt(n_b))\n  \n  #doing this with z 1.645\n  mofe_bypass_z <- 1.645*(sd_b/sqrt(n_b))\n  mofe_angiography_z <- 1.645*(sd_a/sqrt(n_a))\n  \n  #print the normal distribution score MoE\n  mofe_bypass_z\n\n\n[1] 0.7085517\n\n  mofe_angiography_z\n\n\n[1] 0.5087058\n\n  # Print them using t value\n  mofe_bypass\n\n\n[1] 0.7098439\n\n  mofe_angiography\n\n\n[1] 0.6384718\n\n## +/- from the data set mean for the range\n  \n  #bypass upper and lower\n  bypass_lower <- mean_b - mofe_bypass\n  bypass_upper <- mean_b + mofe_bypass\n  \n  #combine the upper and lower to list interval\n  ci_bypass <- print(c(bypass_lower, bypass_upper))\n\n\n[1] 18.29016 19.70984\n\n  #angiography upper and lower\n  angio_lower <- mean_a - mofe_angiography\n  angio_upper <- mean_a + mofe_angiography\n  \n\n  \n  #combine the upper and lower to list interval\n  ci_angiography <- print(c(angio_lower, angio_upper))\n\n\n[1] 17.36153 18.63847\n\n#Normal distribution 90% CI\n  #bypass Z\n  bypass_lower_z <- mean_b - mofe_bypass_z\n  bypass_upper_z <- mean_b + mofe_bypass_z\n  \n  ci_bypass_z <- (print(c(bypass_lower_z, bypass_upper_z)))\n\n\n[1] 18.29145 19.70855\n\n  #angiography Z\n  angiography_lower_z <- mean_a - mofe_angiography_z\n  angiography_upper_z <- mean_a + mofe_angiography_z\n  \n  ci_angiography_z <- (print(c(angiography_lower_z, angiography_upper_z)))\n\n\n[1] 17.49129 18.50871\n\nThe confidence interval is narrower for angiography, as we have a larger sample size for that procedure’s wait time and a smaller standard deviation. We would expect this as in theory as the larger sample size mean should be closer to the true population mean, and the smaller standard deviation means we have less variation to begin with. That expectation is shown in both the smaller numerator and the larger denominator produced during the margin or error calculation. For comparison on the denominators, since the standard deviations were already listed in the table: 23.2163735 for bypass and 29.1032644 for angiography. For completeness, the normal distribution CI’s are 17.4912942, 18.5087058 for angiography and 18.2914483, 19.7085517 for the bypass procedure.\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nSince we are looking to construct this interval around a proportion, I used the prop.test function to construct the interval using the total sample size as the n input and the 567 raw respondents for college “being essential for success” as the success vector in the function. I decided to use this function versus hand calculating as in question 1.\n\n\nprop_coll_success <- prop.test(567, 1031, conf.level = .95)\nprop_coll_success\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nnames(prop_coll_success)\n\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"   \n[5] \"null.value\"  \"conf.int\"    \"alternative\" \"method\"     \n[9] \"data.name\"  \n\nThe point estimate from the survey data is 0.55 The 95% confidence The interval is 0.52, 0.58. This interval means that we could say we are 95% confident that the proportion of American adults who believe that “college education is essential for success” is somewhere between the lower and upper end of our confidence interval – assuming the initial survey was indeed random and representative. Representativeness (and randomness, but that’s already extremely difficult with surveying) would be especially important for this question depending on what variables were used to determine that the initial sample was representative, as beliefs around college education are increasingly subject to the impacts political polarization and thus we could be breaking some of our assumptions needed for our analysis to be accurate depending on which variables were used.\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nFor this problem, I started off assigning all of the elements of the problem to objects as I did for the prior two. The standard deviation problem is shown in sd_books below, as 1/4 of the difference between $200 and $30. The critical z for 5% significance level is shown in finding the qnorm result for half of the alpha level. The margin of error we need to aim for is 5 dollars and that is assigned to book_moe. I assumed it would take a decent sized sample, at least larger than 30, to get within 5 dollars with that large of a standard deviation and the we would be able to randomly collect this sample, so I used the large sample size for estimating mean equation to find the sample size n. It took too long to figure out the fancy letters in Rmarkdown, so I used abbreviations for standard deviation and such, unfortunately. \\[n =  sd ^ {2} (z /  M) ^ {2}\\]\n\n\n#Assign all of the elements of the problem to objects\nsd_books <- (200-30)*.25\nsd_books\n\n\n[1] 42.5\n\nbook_z <- qnorm(.975) \nbook_z\n\n\n[1] 1.959964\n\nbook_moe <- 5\n\n# Formula for n from margin of error calculation, large sample is n = sd^2 * z a/2 / M. Calculate by hand first.\nbook_n_by_hand <- sd_books ^ 2 * (book_z / book_moe) ^ 2\n\nbook_n_by_hand\n\n\n[1] 277.5454\n\n#Use the samplingbook package to confirm.\nbook_n_package <- sample.size.mean(e = book_moe, S = sd_books, level = .95)\n\nbook_n_package\n\n\n\nsample.size.mean object: Sample size for mean estimate\nWithout finite population correction: N=Inf, precision e=5 and standard deviation S=42.5\n\nSample size needed: 278\n\nAfter finding the answer of 278 –rounded up– students in the sample by hand calculating n, I wanted to check my work using the samplingbook package and putting the same elements from the problem into the function. That answer, shown above in book_n_package matched my “hand” calculation of 278 students in the sample needed to have a mean estimate within 5 dollars of the true population mean of textbook costs.\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s =90. \n-A)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n-B) Report the P-value for Ha : μ < 500. Interpret.\n-C) Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nSince this question has several follow up questions to report out, I’m going to dive right in to the code portion of the work, and I’ll have all of the narrative explanations and steps of work described there.\n\n\n#question elements assigned to objects, will use alpha = .05\n  \n  #population\n  all_union_mean <- 500\n  \n  #sample\n  women_mean <- 410\n  women_sd <- 90\n  women_n <- 9\n\n#get the test statistic t\n\n  #estimated standard error\n  women_se <- women_sd / sqrt(women_n)\n  women_se\n\n\n[1] 30\n\n  #test statistic\n  women_t <- (women_mean - all_union_mean) / women_se\n  women_t\n\n\n[1] -3\n\n#find two tail p-value, round to two digits\n  women_p <- round(2*pt(-abs(women_t), df = 8, lower.tail = FALSE ), digits = 2)\n  women_p\n\n\n[1] 1.98\n\n#p for < 500, round to two digits\n  women_lower_p <- round(pt(-abs(women_t), df = 8, lower.tail = TRUE), digits = 2)\n  women_lower_p\n\n\n[1] 0.01\n\n#p for > 500, round to two digits\n  women_greater_p <- round(pt(-abs(women_t), df = 8, lower.tail = FALSE), digits = 2)\n  women_greater_p\n\n\n[1] 0.99\n\nAssumptions: Because this is a small sample, I’m using the two-sided t-test as it is robust when the data may not clear the normality assumptions. Given the smaller sample size of the study, highly skewed data could impact one-tailed tests and the question didn’t explicitly state that they were looking higher or lower than the overall union average\nHypotheses: Null hypothesis is that the mean wage for women = 500 dollars, the same as the contract required mean for all senior workers at the union. The alternative hypothesis is that the mean wage for women =/= 500 dollars.\nTest statistic: The test statistic women_t has a value of -3. Since this is a negative value due to the sample mean being lower than the population mean, it’s important to remember that the absolute value should be used in calculating the p value.\nP value: The two-sided P value from women_t with 8 degrees of freedom equals 1.98.\nInterpreting the results: Since the two-tailed P value is lower than our pre-selected alpha level of .05 by some distance, we can reject the null hypothesis that the mean wage for women is equal to 500 dollars, and accept the alternative hypothesis that it is not equal to 500. The below data points make it seem as though it is very likely below the overall union mean, which lines up with the mean and sd data from their study. If only the very outer bounds of the deviation hits the overall mean, it seems like this all confirms that the female employee mean is < 500 dollars. I would suggest initiating the process of review, confirmation of the study, and the grievance process.\nQuestion B: The P value for the womens’ mean weekly wage being less than 500 is 0.01. This means that if our null hypothesis was true, we would have a 99% chance of getting a value below 500 for our sample mean. Being that the contract specifies an overall mean of 500 and it’s stated that this is a large company so we can assume normality through the Central Limit Theorem, we have a lot of evidence that the mean weekly wage for women is likely lower than what the contract specifies. Being that we have a small sample size it is possible for the one-tailed value to be off with highly skewed data, however. Given the rejection of the null and the one tail results here and below for greater than 500, I think the women’s group would have a strong case that their mean wage is not 500 and most likely lower than 500 dollars.\nQuestion C: The P value for the mean weekly wage for women being greater than 500 is 0.99. This means that if the null hypothesis was true, we would expect to get a mean weekly wage for women greater than the population mean wage 1% of the time. With the same small sample size caveat as above on one tailed results, this split in P values which would have us fail to reject and then reject the null hypothesis respectively, for above and below the population mean is quite extreme and would be another supporting point in the group filing a grievance.\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519. 7,with se = 10.0.\nA) Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. \nB) Using α = 0.05, for each study indicate whether the result is “statistically significant.” \nC)Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nSame as above, I’m going to assign out the objects and then go through the solution steps below the R code chunk. The answer for question A will be in the R code chunk, while I will answer the narrative aspects of the other questions in the text below.\n\n\n#Question Elements\n  #both studies with same n\n  study_n <- 1000\n\n  #null mean = 500\n  #alt mean =/= 500\n\n  #jones study elements\n  jones_mean <- 519.5\n  jones_se <- 10.0\n\n  #smith study elements\n  smith_mean <- 519.7\n  smith_se <- 10.0\n\n\n#Solve for Smith\n  \n  #t for Smith, use two digits and it must equal 1.97\n  smith_t <- round((smith_mean - 500) / smith_se,  digits = 2)\n  smith_t #It's 1.97 like it's supposed to be,  yay!\n\n\n[1] 1.97\n\n  #P for Smith, use three digits and two-tailed \n  smith_p <- round(2 * pt(-abs(smith_t), df = 999), digits = 3)\n  smith_p # It's .049 like it's supposed to be!\n\n\n[1] 0.049\n\n#Solve for Jones\n  \n  #t for Jones, use two digits and it must equal 1.95.\n  jones_t <- round((jones_mean - 500) / jones_se, digits = 2)\n  jones_t #It's 1.95 like it's suppost to be, yay!\n\n\n[1] 1.95\n\n  #P for Jones, use three digits and it must equal .051\n  jones_p <- round(2 * pt(-abs(jones_t), df = 999), digits = 3)\n  jones_p #It's .051 like it's supposed to be!\n\n\n[1] 0.051\n\nQuestion B and C: I thought it made sense to answer these questions in one response versus splitting them up by bullet point. Technically, both studies would be “statistically significant” at the .05 level as rounding .049 and .051 to two digits would take them both to less than or equal to .05. Rounding them without disclosing that would be no good. This is also quite misleading to attribute practical significance to a difference in P values of .002 to our arbitrarily set level of significance. Practically speaking, there is no real difference in the outcomes of these studies.\nWithout rounding, only Smith’s .049 would be below the alpha level and Jones’ .051 would be above. With these fine margins, reporting only whether the null was rejected or if we failed to reject it, or even just listing the “p less than or equal to .05 or p greater than .05” statement without the raw p values included would also ascribe practical significance in difference to these two studies even though they are very nearly identical. The differences could very well be random noise and chance, and reporting out the statement alone would make it harder to asses that.\nChoosing to publish Smith’s study just because it cleared the threshold and not Jones’ as well could make it harder to see the full picture of the parameter they studied or analyze the overall random variability in the experimental findings in a meta analysis setting.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period.\nThe sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAfter reading the question, I don’t think our initial sample was collected in a way that would allow us to be confident in a confidence interval constructed from it to look at the average price in the nation as a whole in 2005. If the federal tax is constant at 18.4, it looks like most of the variability from city to city in the sample data is driven by state and local tax levy decisions. The mean for the gas_tax data is 40.86, so over half of our mean amount comes from state and local variables that do not look to be accounted for in the sampling description as is. Most Americans in 2005 (and still today), did not live in very large cities, the amount depending of course on the exact definition of “large.”\nOur sample is only from 18 large cities in the country, and I think it’s reasonable to assume large cities have, or the very least could have, systemically different gas tax policies than other population densities. Some states could have caps or specific legislation that could impact the overall national average, and there’s not enough information about how the 18 large cities were selected or sampled to clear all of the assumptions, even if we didn’t think large cities varied from suburban, exurban or rural geographies in a way that would skew the small sample of data that we do have. Exploring bootstrapping or other approaches would also be impacted by this fact.\nNow, if I’m reading entirely too much into this set up and I should just show that I can evaluate if our mean is below a set level using a confidence interval, I’ll proceed to do that, too! To answer this question, I’ll use the psych package to get the descriptive statistics and look those over for fun and possible use to calculate by hand if the t.test result looks funny. Then I’ll use t.test(gas_tax) and look at the 95% CI range.\nIf the entire confidence interval is below 45 cents we would have enough evidence to say we think it’s 95% likely that the mean gas tax is below the 45 cent level, the equivalent of rejecting the null hypothesis and accepting the alternative hypothesis of gas taxes were likely lower than 45 cents. If the interval includes 45 and/or above that level we don’t have enough information, and would be doing the equivalent of failing to reject the null hypothesis.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n#Use describe from the psych package for overview of gas_tax data, could use the variables here to hand calculate the CI.\ngas_tax_summary <- describe(gas_taxes)\ngas_tax_summary\n\n\n   vars  n  mean   sd median trimmed  mad   min   max range  skew\nX1    1 18 40.86 9.31  41.47   41.41 9.72 18.49 54.41 35.92 -0.58\n   kurtosis   se\nX1    -0.32 2.19\n\n#Use t.test on gas_taxes to see the 95% CI\nt.test(gas_taxes)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 18.625, df = 17, p-value = 9.555e-13\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\nThe 95% interval does contain 45 cents, so we cannot say that we have enough evidence at this confidence level and interval to do the equivalent of rejecting the null hypothesis. Since the null value is at the very very top of the interval, and putting aside the other issues with the sample to begin with, it seems like the sort of result where we could say in practice it was likely lower than 45 cents. Since gas taxes and prices go to the third digit at the pump, our result would have a maximum 95% CI level “at the pump” reading of .455. It feels safe to describe in actual practice with that interval that we’re confident it was 45 cents or lower, even if we’d need a bit more data to say it in specific statistical terms.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-27T23:53:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httprpubscomemersonflemi870425/",
    "title": "DACSS 603 HW 1",
    "description": "The following document contains my first homework assignment.",
    "author": [
      {
        "name": "Emerson Fleming",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nAnswer:\nRemember that we are trying to create a confidence interval for the average weight time for each surgical procedure. We are trying to predict the mean in the population ideally. We find the upper and lower bonds in order to be able to find our confidence level between 2 bands if you will.\nFor here, we want to use qt() as it gives us the lower tail value. Remember that you are saying you want R to give you the area underneath of the left OR lower tail.\nt=-qt(0.05, 538)\nRemember, we start here as we are trying to find the area underneath 0.05 and the degrees of freedom would be n-1.\nUB=19+t*10/sqrt(539) 19.7091\nlB= 19-t*10/sqrt(539) 18.2902\nUltimately we get that we are 90% confident that true wait time for bypass patients is between 18.3 and 19.71 days. Remember that there is a chance we are wrong, we are only 90% confident. We are saying that we are 90% confident that you will be waiting between 18.3 and 19.71 days to get your procedure.\nThen we do the same thing for the other surgery\nt=qt(0.05, 846)\nUB=18+t*9/sqrt(847) 18.5092\nLB=18-t*9/sqrt(847) 17.49078\nNow we can say we are 90% confident that the true wait time for aniography patients is between 17.49 and 18.50 days.\nIn order to find which is narrower we simply do: 19.71-18.3 = 1.41 days\n18.51-17.5 = 1.01 days\nAngiography is a more narrow inverval.\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nAnswer\nRemember hear that p hat is the sample proportion. For here we want to say: phat= 567/1031=0.55 n=1031\nRemember that ultimately, we want to construct a confidence interval as this is much more accurate than a point estimate.\nLB < P < UB —->We want to follow this format essentially and plug what we know in\nUB=0.55=1.96+sqrt(0.550.45/1031) LB=0.55-1.96sqrt(0.55*0.45/1031)\n0.52 < P < 0.058\nTrue population proportion of those believing college education is essential for success is in between 0.52 and 0.58 with a 95% confidence level\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nAnswer\nThe important information here is that: Range= 170 Sigma= 170/4 = 42.5 Alpha=5 —->This means that the confidence level = 95% (1-(alpha))\nHere, we are trying to find the size of the sample but remember that we don’t actually know but so much about the data. At this point, we need qnorm() which will give us the z-score of both sides of a normal distribution. We get 0.025 using a z table as this is the number that corresponds to -1.96.\nqnorm(0.025) =-1.96 This tells us that -z = -1.96 and z = 1.96\n21.9642.5/(sqrrtn) = 10 —->We want to solve for n\nWhen we do, we get n=278 We can interpret this as 278 being the ideal sample size based on what we know\nQuestion 4\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha : μ < 500. Interpret. B. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nAnswer\nOur first step is to come up with a test statistic. Then you can take the test statistic and create a t graph where you -t and +t. You then compare your p-value with alpha. Or what you can do is calculate a critical p value or a z value (which corresponds to alpha). Then you would compare your test statistic with your critical t.\nIn this particular case, we will choose the former rather than the latter as we are able to use R to compute the p-value (which are difficult to find by hand).\nHo: M=500 Ha: M(can’t equal)500 n=9 s=90 y(bar)=410 t= y(bar)-M/(s/(root(n))= -3\ndf=8\npt(-3, 8) ^Quantile, remember here that 8 is our degrees of freedom (n-1)\npvalue <- (-3,8)*2 +This gives you area under the curve to the left of -3 pvalue= 0.017 +Here, the p-value is very small. This means that there is no way that you were unlucky and selected a bad sample. This means you can reject your null hypothesis (Ho) We are rejecting M=500 in favor of M(is not equal to)500\n(Alternative way) tc <- qt(0.025, 8) ^probability where degrees of freedom is 8 = -2.31 (critical value) t statistic = -3 Here, we can reject Ho, our t-statistic is in the critical region which means we can reject the null!\nB. Ho = M = 500 Ha = M < 500 P-value = 0.0085 Since this p-value is so small, we can reject the null hypothesis in favor of the alternative hypothesis.\nC. Ho = M < 500 Ha = M > 500 test statistic = -3 pt(-3,8) = 0.99 Here, we fail to reject the null hypothesis. This p-value is huge. This proves that M has to be lower that 500 because we are failing to reject the null hypothesis.\nQuestion 5\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ = 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nAnswer\nA. Jones, t = 519-500/10 = 1.95\nse= s/sqrt(n)\nHere, we are conducting a 2-sided test. We want to find the area to the right of 1.95 and the area to the left of -1.95. These two areas together will give us the t-value. We will try and find the area on the left first\ndf=999\np-value = pt(-1.95, 999)*2 = 0.051\nSmith\nt= 519.7-500/10 = 1.97\np-value = pt(-1.97, 999)*2 = 0.049\nB. For Jones, we fail to reject the null hypothesis as the p-value is above 0.051 which means the results are statistically insignificant. For Smith, we can reject the null hypothesis which means that the results are statistically significant.\nC. The p-values are so similar that the results are not desirable. It is not fair to Jones. The results are so close that the basically got the same results but Jones got the shorter end of the stick and only his results are statistically insignificant.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAnswer\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\nHo: M<45 Ha: M<(less than or equal to) 45\nmean(gas_taxes) sd(gas_taxes)\nt= 40.86-45/9.31/sqrt(18) = -1.89\ndf= pt(-1.89, 17) p-value = 0.038 ^^This is a one-sided test so no need to multiply it\nThe conclusion is that we can reject the null hypothesis which gives us strong evidence to claim that tax rate is less than 45% at 5% significance level.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomalexanderhong86870632/",
    "title": "DACSS 603 HW#1",
    "description": "First homework for DACSS 603.",
    "author": [
      {
        "name": "Alexander Hong",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\n##Question 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\r\nSurgical Procedure\r\nSample Size\r\nMean Wait Time\r\nStandard Deviation\r\nBypass\r\n539\r\n19\r\n10\r\nAngiography\r\n847\r\n18\r\n9\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\n\r\n\r\nbypass_l <- 19 - (1.65 * (10/(539^.5)))\r\nbypass_h <- 19 + (1.65 * (10/(539^.5)))\r\nbypass_diff <- bypass_h - bypass_l\r\n\r\nangio_l <- 18 - (1.65 * (9/(847^.5)))\r\nangio_h <- 18 + (1.65 * (9/(847^.5)))\r\nangio_diff <- angio_h - angio_l\r\n\r\n\r\n\r\nCI for Bypass \\(19 \\pm 1.65 ( 10/sqrt(539) )\\) | Difference = 1.4214106\r\nCI for Angiography \\(18 \\pm 1.65 ( 9/sqrt(847) )\\) | Difference = 1.0205041\r\nThe confidence interval is narrower for angiography surgery.\r\n##Question 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\n\r\n\r\np2 = 567 / 1031\r\nn2 = 1031\r\nz2 = 1.96\r\n\r\np2_l <- p2 - (z2*(p2*(1-p2))^.5) / n2\r\np2_h <- p2 + (z2*(p2*(1-p2))^.5) / n2\r\n\r\n\r\n\r\nCI = \\(.55 \\pm 1.96 * sqrt( .55 / 1 -.55 )\\)\r\n95% of confidence intervals calculated would contain If this survery is repeated as many times, it is expected that 95% of those confidence intervals will contain the proportion that almost 55% of adult Americans believe that college education is essential for success.\r\n##Question 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nz = 1.96\r\nSD = $170 * .25 = $42.5 (The quarter of the range is .25*(200-30) )\r\nMean = Assuming most of the book values of the mean is between $30 and $200, the mean can be derived from ( $200 + $30 / 2 ) = $115\r\n\r\n\r\n\r\nA sample size of 277 textbooks should be needed to estimate the mean cost of textbooks per quarter. Using this sample size, given the standard deviation is $42.5, and the mean price of textbooks is $115, our confidence interval will have a length of $10.01\r\n##Question 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\n\r\n\r\nbar = 410\r\ns = 90\r\nn = 9\r\nmu = 500\r\n\r\ntscore <- (bar - mu) / (s / 9^.5)\r\n\r\np_value_l <- pt(tscore, df = n - 1, lower.tail = TRUE)\r\ncat(\"P-value is:\", p_value_l)\r\n\r\n\r\nP-value is: 0.008535841\r\n\r\np_value_h <- pt(tscore, df = n - 1, lower.tail = FALSE)\r\ncat(\"P-value is:\", p_value_h)\r\n\r\n\r\nP-value is: 0.9914642\r\n\r\nHypothesis 1:\r\nHo: μ = 500\r\nHa: μ < 500\r\nTest Statistic: -3\r\n0.0085358\r\nWe reject the null hypothesis and conclude that the mean salaries of female senior employees are not statistically significantly less than the $500 / week of senior employees.\r\nHypothesis 2:\r\nHo: μ = 500\r\nHa: μ > 500\r\nTest Statistic: -3\r\n0.9914642\r\nWe fail to reject the null hypothesis and conclude that the mean salaries of female senior employees are not statistically significantly higher than the $500 / week of senior employees.\r\n##Question 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05”, or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\n\r\n\r\njones_t <- round(pt(q=1.95, df=999, lower.tail = FALSE) * 2, 3)\r\nsmith_t <- round(pt(q=1.97, df=999, lower.tail = FALSE) * 2, 3)\r\n\r\n\r\n\r\nT Statistic for Jones = (519.5 - 500) / 10 = 1.95, p-value = 0.051\r\nT Statistic for Smith = (519.7 - 500) / 10 = 1.97, p-value = 0.049\r\nUsing an α = 0.05\r\nThe Jones study is not considered to be statistically significant, given the p-value for the test statistic is .051, which is barely above the .05 threshold.\r\nThe Smith study considered to be statistically significant, given the p-value for the test statistic is .049, which is barely above the .05 threshold.\r\nFor this example, if a result was listed as “P ≤ 0.05”, the range of p-values for Smith’s study can range from .05 to almost 0, with the actual p-value being .049.. If a result was listed as “P > 0.05”, the range of p-values for Jones’ study can range from .05 to 1, while the actual p-value was .051. These ranges diminish the reality that these studies were barely statistically significant or not, which would lead to possibly diminishing the motivation to look further into these studies and the nuances of the study which led to producing said result.\r\n##Question 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\ngas_m <- mean(gas_taxes)\r\ngas_sd <- (var(gas_taxes))^.5\r\n\r\ngas_l <- gas_m - (1.96 * (gas_sd/length(gas_taxes)^.5))\r\ngas_g <- gas_m + (1.96 * (gas_sd/length(gas_taxes)^.5))\r\n\r\n\r\n\r\nMean = 40.8627778 Standard Deviation = 9.3083168\r\n\\(40.86 \\pm 1.96 (9.31/sqrt(18))\\) = (36.5625548, 45.1630008)\r\nAt the 95% confidence level, there is not enough evidence to conclude that the average tax per gallon of gas was less than $.45. The upper end of the confidence interval is just over $.45.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomchester870152/",
    "title": "Homework One",
    "description": "DACSS 603",
    "author": [
      {
        "name": "Cynthia Hester",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nQuestion 1\r\nSolution\r\nAnalysis\r\n\r\nQuestion 2\r\nSolution\r\n\r\nQuestion 3\r\nSolution\r\n\r\nQuestion 4\r\nSolution\r\n\r\nQuestion 5\r\nSolution\r\n\r\nQuestion 6\r\nSolution\r\n\r\n\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nSolution\r\nFirst we assign variable names to the sample summary statistics\r\n\r\n\r\nbypass_sample<-539 #bypass_sample\r\nangio_sample<-847  #angiography_sample\r\nbypass_mean<-19    #bypass_sample_wait_time_mean\r\nangio_mean<-18     #angiography_sample_wait_time_mean\r\nbypass_sd<-10      #bypass_standard_deviation\r\nangio_sd<-9        #angiography_standard_deviation\r\n\r\n\r\n\r\nWe then calculate the t-confidence interval or t-score of each sample. To do this we start by calculating the degrees of freedom of each sample (bypass and angiography)\r\nCalculating degrees of freedom of the bypass and angiography samples\r\n\r\n\r\nbypass_df<-bypass_sample - 1  #bypass degrees of freedom\r\nangio_df<-angio_sample - 1    #angiography degrees of freedom\r\n\r\n\r\n\r\nNow that we have degrees of freedom we can calculate the t-critical values or t-score for the respective 90% intervals of each sample.\r\n\r\n\r\n#Calculating the t-critical value for the **angiography** sample\r\n\r\nangio_sample<-847\r\nangio_df<-angio_sample - 1\r\nt_score_angio<-qt(p=0.05, df=angio_df,lower.tail=F)\r\nprint(t_score_angio)\r\n\r\n\r\n[1] 1.646657\r\n\r\n# Calculating the t-critical value for the **bypass** sample\r\n\r\nbypass_sample<-539\r\nbypass_df<-bypass_sample - 1\r\nt_score_bypass<-qt(p=0.05, df=bypass_df,lower.tail=F)\r\nprint(t_score_bypass)\r\n\r\n\r\n[1] 1.647691\r\n\r\n# We now find the margin of error for both samples\r\n\r\n\r\nmargin_angio<- qt(0.05,df=angio_df)*9/sqrt(847)    #margin of error angiography\r\nprint(margin_angio)\r\n\r\n\r\n[1] -0.5092182\r\n\r\nmargin_bypass<-qt(0.05,df=bypass_df)*10/sqrt(539)   #margin of error bypass\r\nprint(margin_bypass)\r\n\r\n\r\n[1] -0.7097107\r\n\r\n# To calculate the lower bound and upper bound of the angiography sample\r\n\r\nlower_bound_angio<-angio_mean-margin_angio\r\nprint(lower_bound_angio)\r\n\r\n\r\n[1] 18.50922\r\n\r\nupper_bound_angio<-angio_mean+margin_angio\r\nprint(upper_bound_angio)\r\n\r\n\r\n[1] 17.49078\r\n\r\n# To calculate the lower bound and upper bound of the bypass sample\r\n\r\nlower_bound_bypass<-bypass_mean-margin_bypass\r\nprint(lower_bound_bypass)\r\n\r\n\r\n[1] 19.70971\r\n\r\nupper_bound_bypass<-bypass_mean+margin_bypass\r\nprint(upper_bound_bypass)\r\n\r\n\r\n[1] 18.29029\r\n\r\nTo determine which confidence interval is narrower I subtract the upper bound from the lower bound of each respective procedure.\r\nAngiography : [17.49,18.51] 18.51-17.49 = 1.02 \r\nBypass : [18.29,19.71] 19.71-18.29 = 1.42 \r\nThe angiography 90% confidence interval is narrower at 1.02 compared to the bypass of 1.42\r\nAnalysis\r\nWe see the mean wait time of the 90% confidence interval is from 17.49 to 18.51 days for the angiography procedure. Whereas the mean wait time of the 90% confidence interval is from 18.29 to 19.71 for the bypass procedure. This results in a narrower wait time of 1.02 days for the angiography compared to the bypass of 1.42 days. This could be attributed to the larger sample size for the angiography of 847 compared to a sample of 539 for the bypass. As well as a smaller standard deviation of 9 for the angiography compared to a standard deviation of 10 for the bypass.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success.\r\nFind the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\r\nConstruct and interpret a 95% confidence interval for p.\r\nSolution\r\nFirst, we find the point estimate,p, of the proportion.\r\n\r\n\r\n# Specify sample occurrences (x), sample size(n), and confidence_level\r\n\r\n\r\nx<- 567                     # survey respondents  (successes)\r\nn<- 1031                    # total surveyed\r\nconfidence_level<-0.95      # confidence level\r\npoint_estimate<-x/n         # the point estimate is the sample proportion\r\n\r\n\r\n\r\nNow to determine the 90% confidence interval I must find the alpha,the critical z-value, standard error and the margin of error.\r\n\r\n\r\nalpha<-(1-confidence_level)\r\ncritical_z<-qnorm(1-alpha/2)\r\nstandard_error<-sqrt(point_estimate*(1-point_estimate)/n)\r\nmargin_of_error<-critical_z*standard_error \r\n\r\n\r\n\r\nThe lower bound and upper bound of the confidence interval are calculated.\r\n\r\n\r\nlower_bound<-point_estimate-margin_of_error \r\nupper_bound<-point_estimate+margin_of_error\r\n\r\n\r\n\r\nResults\r\n\r\n\r\nsprintf(\"Point Estimate: %0.3f\", point_estimate)\r\n\r\n\r\n[1] \"Point Estimate: 0.550\"\r\n\r\nsprintf(\"Critical Z-value: %0.3f\", critical_z)\r\n\r\n\r\n[1] \"Critical Z-value: 1.960\"\r\n\r\nsprintf(\"Margin of Error: %0.3f\", margin_of_error)\r\n\r\n\r\n[1] \"Margin of Error: 0.030\"\r\n\r\nsprintf(\"Confidence Interval: [%0.3f,%0.3f]\", lower_bound,upper_bound)\r\n\r\n\r\n[1] \"Confidence Interval: [0.520,0.580]\"\r\n\r\nsprintf(\"The %0.1f%% confidence interval for the population proportion is:\", confidence_level*100)\r\n\r\n\r\n[1] \"The 95.0% confidence interval for the population proportion is:\"\r\n\r\nsprintf(\"between %0.4f and %0.4f\",lower_bound,upper_bound)\r\n\r\n\r\n[1] \"between 0.5196 and 0.5803\"\r\n\r\nConfidence interval interpretation\r\nWe have 95% confidence that the interval from the lower bound to the upper bound,[0.5196,0.5803] actually contains the true value of the population proportion of those in the sample believing a college education is valuable.\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nSolution\r\nHere’s what we know:\r\nmean population error = +/-5\r\nrange of the data - upper range - lower range = 200-30 = 170\r\npopulation standard deviation = Range/a quarter 170/4=42.5 which is sigma\r\nsignificance level or alpha = 0.05\r\nfrom this we can calculate the z-score or critical value\r\n\r\n\r\ncritical_z<-qnorm(1-0.05/2) #using the significance level or alpha we calculate z-score                 \r\nprint(critical_z)\r\n\r\n\r\n[1] 1.959964\r\n\r\n\r\n\r\nn_sample_size<-((1.96*42.5)/5)**2      #n=(z-score*standard deviation)/margin of error)**2\r\nprint(n_sample_size)                   #sample size\r\n\r\n\r\n[1] 277.5556\r\n\r\nThus, we see that we would need a minimum sample of 277.5556 or 278\r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\na)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses,test statistic, and P-value. Interpret the result.\r\nb)Report the P-value for Ha : μ < 500. Interpret.\r\nc)Report and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nSolution\r\na)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nHere’s what we know:\r\n\\(\\mu\\) mean income for all senior level workers = $500/ week\r\n\\(\\bar{y}\\) random sample of 9 female employees income = $410/week\r\ns standard deviation = 90\r\nn random sample female employees = 9\r\nHypotheses\r\nThe null and alternative hypotheses are:\r\n\\(H_0\\): \\(\\mu\\) = $500 per week\r\nThe null hypothesis weekly income for all senior-level workers is $500 per week\r\n\\(H_a\\): \\(\\mu\\) ≠ $500 per week\r\nThe alternative hypothesis suggests the mean weekly income is ≠ $500 (two-sided)\r\nTest Statistic\r\n\r\n\r\nt_test_income<-(410-500)/(90/sqrt(9))          #Test statistic using t-test\r\nprint(t_test_income)\r\n\r\n\r\n[1] -3\r\n\r\nP-Value\r\n\r\n\r\nn_random_sample<-9                      #random sample female employees\r\ndf_sample<-(n_random_sample-1)          #degrees of freedom\r\nt_test_income<-(410-500)/(90/sqrt(9))   #test statistic\r\np_val<-pt(t_test_income,df_sample)*2    #p-value\r\nprint(p_val)\r\n\r\n\r\n[1] 0.01707168\r\n\r\nInterpretation:\r\npart a\r\nAssuming alpha α = 0.05 and we know the p-value is 0.0171 The p-value 0.0171 < 0.05 , we reject the null hypothesis There is therefore sufficient evidence to claim the mean differs from the weekly income of $500.\r\npart b:\r\nReport the P-value for Ha : μ < 500. Interpret.\r\nHypotheses\r\n\\(H_0\\): \\(\\mu\\) = $500 per week\r\n\\(H_a\\): \\(\\mu\\) < $500 per week (left-tail test)\r\np-value = p(t < t_test_income) p(t < -3)\r\nP-value for \\(H_a\\) < $500 per week (left-tail test)\r\n\r\n\r\n#using the formula: pt(q,df,lower.tail=TRUE,log.p=FALSE) to find the p-value\r\n\r\n\r\nq<-(-3)\r\nn_random_sample<-9\r\ndf_sample<-(n_random_sample-1)                            #degrees of freedom \r\nleft_p_value<-pt(q,df_sample,lower.tail = T,log.p = F)    #p value for alternative hypothesis\r\nprint(left_p_value)         \r\n\r\n\r\n[1] 0.008535841\r\n\r\nInterpretation:\r\npart b\r\nSince the P-value 0.0085 is less than the presumed significance level,alpha α = 0.05 I reject the null hypothesis,\\(H_0\\). What this suggests is that there is sufficient evidence to conclude that the mean is < less than 500.\r\npart c:\r\nReport and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nHypotheses\r\n\\(H_0\\): \\(\\mu\\) = $500 per week\r\n\\(H_a\\): \\(\\mu\\) > $500 per week (right-tail test)\r\np-value = p(t > t_test_income) p(t > -3)\r\nP-value for \\(H_a\\) >$500 per week (right-tail test)\r\n\r\n\r\n#using the formula: pt(q,df,lower.tail=TRUE,log.p=FALSE) to find the p-value\r\n\r\n\r\nq<-(-3)\r\nn_random_sample<-9\r\ndf_sample<-(n_random_sample-1)                                  #degrees of freedom \r\nright_p_value<-pt(q,df_sample,lower.tail = F,log.p = F)         #p-value for alternative hypothesis\r\nprint(right_p_value)   \r\n\r\n\r\n[1] 0.9914642\r\n\r\n#verification sum of left and right tail p-values equal 1\r\n\r\nleft_p_value<-pt(q,df_sample,lower.tail = T,log.p = F) \r\nright_p_value<-pt(q,df_sample,lower.tail = F,log.p = F)\r\nleft_right_sum<-(left_p_value+right_p_value)\r\nprint(left_right_sum)\r\n\r\n\r\n[1] 1\r\n\r\nInterpretation:\r\npart c\r\nSince the p-value 0.9915 is greater than the presumed significance level alpha α = 0.05 \\(H_a\\) > 500,therefore we do not reject the null hypothesis \\(H_0\\). There is insufficient evidence to support the claim \\(\\mu\\) mean is > 500.\r\nQuestion 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7,with se = 10.0.\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nSolution\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nHere’s what we know :\r\nsample_n = 1000 each for Jones and Smith\r\ndf_sample_n<-(1000-1) degrees of freedom\r\nJones\r\n\\(\\bar{y}\\) = 519.5\r\nt = 1.95\r\np-value = 0.051\r\nse = 10.0\r\nSmith\r\n\\(\\bar{y}\\) = 519.7\r\nt = 1.97\r\np-value = 0.049\r\nse = 10.0\r\nHypotheses\r\n\\(H_0\\): \\(\\mu\\) = 500\r\n\\(H_a\\): \\(\\mu\\) ≠ 500\r\nJones We were already given both the test statistic 1.95 and p-value 0.051 for Jones so we are just verifying both.\r\nTest_statistic = (\\(\\bar{y}\\) - \\(\\mu\\))/10\r\n\r\n\r\ndf_sample_n<-(1000-1)  #degrees of freedom\r\n\r\nt_test_jones<-(519.5-500)/10    #Test statistic using t-test\r\nprint(t_test_jones)\r\n\r\n\r\n[1] 1.95\r\n\r\np_value_jones<-pt(t_test_jones,df_sample_n,lower.tail = F,log.p = F)*2\r\nprint(p_value_jones)\r\n\r\n\r\n[1] 0.05145555\r\n\r\nSmith\r\nWe were already given both the test statistic 1.97 and p-value 0.049 for Smith so we are verifying both.\r\nTest_statistic = (\\(\\bar{y}\\) - \\(\\mu\\))/10\r\n\r\n\r\ndf_sample_n<-(1000-1)            #degrees of freedom\r\n\r\nt_test_smith<-(519.7-500)/10    #Test statistic using t-test\r\nprint(t_test_smith)\r\n\r\n\r\n[1] 1.97\r\n\r\np_value_smith<-pt(t_test_smith,df_sample_n,lower.tail = F,log.p = F)*2       #p-value for smith\r\nprint(p_value_smith)                                                         #output smith\r\n\r\n\r\n[1] 0.04911426\r\n\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nFor α = 0.05 we reject the null hypothesis \\(H_0\\) if the p-value is greater than 0.05 and do not reject if the p-value is equal or greater to => \\(H_0\\). So in the case of Jones since the p-value 0.051 is negligibly larger than alpha, it is not statistically significant and we fail to reject the null hypothesis \\(H_0\\). In the case of Smith, the p-value of 0.049 is less than alpha, α = 0.05 which is less than the level of significance, we reject the null hypothesis since a smaller p-value suggests statistical significance.\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nIn this study the p-values are negligibly the same between Jones and Smith. However, in spite of this because our predetermined significance p-value is ≤ 0.05 in the case of Smith, the null hypothesis is rejected and we conclude there is statistical evidence for the alternative hypothesis \\(H_a\\). Conversely, if our predetermined significance p-value is greater than 0.05 as in the case of Jones then we fail to reject the null hypothesis. There is insufficient evidence to draw any conclusions.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nSolution\r\nHere’s what we know:\r\nsample_gas_taxes<-18\r\ndf_gas_taxes<-sample_gas_taxes-1\r\n95% confidence level (presumptive)\r\nsignificance level alpha = 0.05 based on a 95% confidence level\r\nz-score = 1.96 based on 95% confidence level\r\nFrom this information we can determine the following:\r\n\r\n\r\n#manual calculation of t-score used for finding the upper and lower interval of gas_tax_sample\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\ngas_taxes_sample<-18                                           #gas taxes sample size\r\n\r\ndf_gas_taxes<-gas_taxes_sample-1                               #degrees of freedom \r\nmean_gas_taxes<-mean(gas_taxes)                                #mean gas taxes\r\nt_score_gas_taxes<-qt(p = 0.05,df=df_gas_taxes,lower.tail = F) #t_score\r\nsd_gas<-sd(gas_taxes)                                          #standard deviation\r\nm_error_gas_taxes<-qt(0.05,df=df_gas_taxes)*sd_gas/sqrt(18)    #margin of error gas taxes\r\n\r\n\r\n\r\n\r\n \r\n#Now that all of the needed parameters for lower and upper bounds have been calculated, I can find the confidence interval for the gas taxes sample.\r\n\r\n\r\n\r\nmean_gas_taxes<-mean(gas_taxes)  \r\nm_error_gas_taxes<-qt(0.05,df=df_gas_taxes)*sd_gas/sqrt(18)\r\n\r\nlower_gas_tax<-(mean_gas_taxes-m_error_gas_taxes) #lower bound using mean and margin of error\r\nprint(lower_gas_tax)\r\n\r\n\r\n[1] 44.67946\r\n\r\nupper_gas_tax<-(mean_gas_taxes+m_error_gas_taxes) #upper bound using mean and margin of error\r\nprint(upper_gas_tax)\r\n\r\n\r\n[1] 37.0461\r\n\r\nThe 95% confidence interval is [37.0461 ,44.6795] using manual calculations.\r\nBecause the average tax per gallon is less than 45 cents, it is within the lower and upper bounds of the 95% confidence interval.We can therefore reasonably conclude that there is sufficient evidence that the confidence interval contains taxes less than 45 cents.\r\nAlternative outcome using t.test:\r\n\r\n\r\ngas_taxes<- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nmean(gas_taxes)\r\n\r\n\r\n[1] 40.86278\r\n\r\nt.test(gas_taxes,conf.level = 0.95)                            #one sample t-test\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 9.555e-13\r\nalternative hypothesis: true mean is not equal to 0\r\n95 percent confidence interval:\r\n 36.23386 45.49169\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThe 95% confidence interval is [36.23386 ,45.49169]\r\nThere is not enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents because 45 cents is inside the confidence interval. The confidence interval contains taxes greater than 45 cents.\r\nPlease note I am not sure why there is a difference between 95% confidence intervals when calculated manually versus the t.test function. I therefore include both outcomes.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomdacssjoe870700/",
    "title": "603, Homework 1",
    "description": "First homework assignment for DACSS 603",
    "author": [
      {
        "name": "Joe Davis",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nI wanted to do this one a bit longer-form than necessary, but I also appreciate the chance to practice organizing my R code neatly while solving. I need some good repetition to build those good habits.\nFor this question I assigned all of the values in the table to objects to use in the calculations of each treatment’s t-scores, margins of error, and confidence intervals. I did a quick check of the qnorm(.95) to see if the t-scores had converged to the normal distribution given the relatively high sample sizes of each treatment. Surprisingly, to me at least, rounding at the .000 level they still had very slightly different levels both to the normal distribution and to each others’ scores.\nI know I have the standard deviations, wait time would be a continuous variable, and these samples are larger than n = 30 so I can use qnorm and the 1.645 z critical value clearing those assumptions. But, it seems like I should use the distribution with fatter tails relative to the sample size values from qt regardless if I could assume normality from the other conditions, as to err on the side of caution for analyzing and communicating medical procedure data. I have a note in my code on that point, mostly for my own reference and will solve for the normal distribution values as well. After running the calculations the practical effect would be introducing slightly more uncertainty in the mean wait time for the angiography procedure while still keeping that procedures’ interval narrower, as we’d expect, than that of the bypass procedure. Especially if this was to be used to set patient expectations on wait times, the wider range would be the option a hospital or doctor would communicate, but we do appear to be in the range where t and z distributions are becoming quite similar.\n\n\n## Means, Total N,  and SDs from full question text. B = Bypass A = Angiography\n  #mean wait times for each procedure\n  mean_b <- 19 \n  mean_a <- 18 \n  \n  #standard deviations\n  sd_b <- 10\n  sd_a <- 9\n  \n  #total N\n  n_b <- 539\n  n_a <- 847\n\n## 90% CI score finding. .90 = 1 - a and I need  a/2 for the two tails since I have no theory on why the direction of the error would be important. Did a little side exploring of the t distribution and normal distribution scores below.\n  \n\n#Rounded to 1.645 but not used for calculations. I thought it was a sufficiently large sample size that that qt and qnorm should have returned the same values at .000 rounding, but after checking those, each T rounded up differently at the 3rd digits and for medical data I would rather err on the side of caution  \n\n    #normal z score\n  z_heart <- qnorm(.95) \n  round(z_heart, digits = 3)\n\n\n[1] 1.645\n\n  #check this for t scores\nb_t <- round(qt(.95, df = n_b -1), digits = 3)\na_t <-  round(qt(.95, df = n_a -1), digits = 3)\n  \n  #print them\n  b_t\n\n\n[1] 1.648\n\n  a_t\n\n\n[1] 1.647\n\n## 90% CI standard error of mean/ margin of error\n  #Take the score multiplied by standard deviations and sqrt of Ns, t value\n  mofe_bypass <- b_t*(sd_b/sqrt(n_b))\n  mofe_angiography <- a_t*(sd_a/sqrt(n_b))\n  \n  #doing this with z 1.645\n  mofe_bypass_z <- 1.645*(sd_b/sqrt(n_b))\n  mofe_angiography_z <- 1.645*(sd_a/sqrt(n_a))\n  \n  #print the normal distribution score MoE\n  mofe_bypass_z\n\n\n[1] 0.7085517\n\n  mofe_angiography_z\n\n\n[1] 0.5087058\n\n  # Print them using t value\n  mofe_bypass\n\n\n[1] 0.7098439\n\n  mofe_angiography\n\n\n[1] 0.6384718\n\n## +/- from the data set mean for the range\n  \n  #bypass upper and lower\n  bypass_lower <- mean_b - mofe_bypass\n  bypass_upper <- mean_b + mofe_bypass\n  \n  #combine the upper and lower to list interval\n  ci_bypass <- print(c(bypass_lower, bypass_upper))\n\n\n[1] 18.29016 19.70984\n\n  #angiography upper and lower\n  angio_lower <- mean_a - mofe_angiography\n  angio_upper <- mean_a + mofe_angiography\n  \n\n  \n  #combine the upper and lower to list interval\n  ci_angiography <- print(c(angio_lower, angio_upper))\n\n\n[1] 17.36153 18.63847\n\n#Normal distribution 90% CI\n  #bypass Z\n  bypass_lower_z <- mean_b - mofe_bypass_z\n  bypass_upper_z <- mean_b + mofe_bypass_z\n  \n  ci_bypass_z <- (print(c(bypass_lower_z, bypass_upper_z)))\n\n\n[1] 18.29145 19.70855\n\n  #angiography Z\n  angiography_lower_z <- mean_a - mofe_angiography_z\n  angiography_upper_z <- mean_a + mofe_angiography_z\n  \n  ci_angiography_z <- (print(c(angiography_lower_z, angiography_upper_z)))\n\n\n[1] 17.49129 18.50871\n\nThe confidence interval is narrower for angiography, as we have a larger sample size for that procedure’s wait time and a smaller standard deviation. We would expect this as in theory as the larger sample size mean should be closer to the true population mean, and the smaller standard deviation means we have less variation to begin with. That expectation is shown in both the smaller numerator and the larger denominator produced during the margin or error calculation. For comparison on the denominators, since the standard deviations were already listed in the table: 23.2163735 for bypass and 29.1032644 for angiography. For completeness, the normal distribution CI’s are 17.4912942, 18.5087058 for angiography and 18.2914483, 19.7085517 for the bypass procedure.\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nSince we are looking to construct this interval around a proportion, I used the prop.test function to construct the interval using the total sample size as the n input and the 567 raw respondents for college “being essential for success” as the success vector in the function. I decided to use this function versus hand calculating as in question 1.\n\n\nprop_coll_success <- prop.test(567, 1031, conf.level = .95)\nprop_coll_success\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nnames(prop_coll_success)\n\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"   \n[5] \"null.value\"  \"conf.int\"    \"alternative\" \"method\"     \n[9] \"data.name\"  \n\nThe point estimate from the survey data is 0.55 The 95% confidence The interval is 0.52, 0.58. This interval means that we could say we are 95% confident that the proportion of American adults who believe that “college education is essential for success” is somewhere between the lower and upper end of our confidence interval – assuming the initial survey was indeed random and representative. Representativeness (and randomness, but that’s already extremely difficult with surveying) would be especially important for this question depending on what variables were used to determine that the initial sample was representative, as beliefs around college education are increasingly subject to the impacts political polarization and thus we could be breaking some of our assumptions needed for our analysis to be accurate depending on which variables were used.\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nFor this problem, I started off assigning all of the elements of the problem to objects as I did for the prior two. The standard deviation problem is shown in sd_books below, as 1/4 of the difference between $200 and $30. The critical z for 5% significance level is shown in finding the qnorm result for half of the alpha level. The margin of error we need to aim for is 5 dollars and that is assigned to book_moe. I assumed it would take a decent sized sample, at least larger than 30, to get within 5 dollars with that large of a standard deviation and the we would be able to randomly collect this sample, so I used the large sample size for estimating mean equation to find the sample size n. It took too long to figure out the fancy letters in Rmarkdown, so I used abbreviations for standard deviation and such, unfortunately. \\[n =  sd ^ {2} (z /  M) ^ {2}\\]\n\n\n#Assign all of the elements of the problem to objects\nsd_books <- (200-30)*.25\nsd_books\n\n\n[1] 42.5\n\nbook_z <- qnorm(.975) \nbook_z\n\n\n[1] 1.959964\n\nbook_moe <- 5\n\n# Formula for n from margin of error calculation, large sample is n = sd^2 * z a/2 / M. Calculate by hand first.\nbook_n_by_hand <- sd_books ^ 2 * (book_z / book_moe) ^ 2\n\nbook_n_by_hand\n\n\n[1] 277.5454\n\n#Use the samplingbook package to confirm.\nbook_n_package <- sample.size.mean(e = book_moe, S = sd_books, level = .95)\n\nbook_n_package\n\n\n\nsample.size.mean object: Sample size for mean estimate\nWithout finite population correction: N=Inf, precision e=5 and standard deviation S=42.5\n\nSample size needed: 278\n\nAfter finding the answer of 278 –rounded up– students in the sample by hand calculating n, I wanted to check my work using the samplingbook package and putting the same elements from the problem into the function. That answer, shown above in book_n_package matched my “hand” calculation of 278 students in the sample needed to have a mean estimate within 5 dollars of the true population mean of textbook costs.\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s =90. \n-A)Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n-B) Report the P-value for Ha : μ < 500. Interpret.\n-C) Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nSince this question has several follow up questions to report out, I’m going to dive right in to the code portion of the work, and I’ll have all of the narrative explanations and steps of work described there.\n\n\n#question elements assigned to objects, will use alpha = .05\n  \n  #population\n  all_union_mean <- 500\n  \n  #sample\n  women_mean <- 410\n  women_sd <- 90\n  women_n <- 9\n\n#get the test statistic t\n\n  #estimated standard error\n  women_se <- women_sd / sqrt(women_n)\n  women_se\n\n\n[1] 30\n\n  #test statistic\n  women_t <- (women_mean - all_union_mean) / women_se\n  women_t\n\n\n[1] -3\n\n#find two tail p-value, round to two digits\n  women_p <- round(2*pt(-abs(women_t), df = 8, lower.tail = FALSE ), digits = 2)\n  women_p\n\n\n[1] 1.98\n\n#p for < 500, round to two digits\n  women_lower_p <- round(pt(-abs(women_t), df = 8, lower.tail = TRUE), digits = 2)\n  women_lower_p\n\n\n[1] 0.01\n\n#p for > 500, round to two digits\n  women_greater_p <- round(pt(-abs(women_t), df = 8, lower.tail = FALSE), digits = 2)\n  women_greater_p\n\n\n[1] 0.99\n\nAssumptions: Because this is a small sample, I’m using the two-sided t-test as it is robust when the data may not clear the normality assumptions. Given the smaller sample size of the study, highly skewed data could impact one-tailed tests and the question didn’t explicitly state that they were looking higher or lower than the overall union average\nHypotheses: Null hypothesis is that the mean wage for women = 500 dollars, the same as the contract required mean for all senior workers at the union. The alternative hypothesis is that the mean wage for women =/= 500 dollars.\nTest statistic: The test statistic women_t has a value of -3. Since this is a negative value due to the sample mean being lower than the population mean, it’s important to remember that the absolute value should be used in calculating the p value.\nP value: The two-sided P value from women_t with 8 degrees of freedom equals 1.98.\nInterpreting the results: Since the two-tailed P value is lower than our pre-selected alpha level of .05 by some distance, we can reject the null hypothesis that the mean wage for women is equal to 500 dollars, and accept the alternative hypothesis that it is not equal to 500. The below data points make it seem as though it is very likely below the overall union mean, which lines up with the mean and sd data from their study. If only the very outer bounds of the deviation hits the overall mean, it seems like this all confirms that the female employee mean is < 500 dollars. I would suggest initiating the process of review, confirmation of the study, and the grievance process.\nQuestion B: The P value for the womens’ mean weekly wage being less than 500 is 0.01. This means that if our null hypothesis was true, we would have a 99% chance of getting a value below 500 for our sample mean. Being that the contract specifies an overall mean of 500 and it’s stated that this is a large company so we can assume normality through the Central Limit Theorem, we have a lot of evidence that the mean weekly wage for women is likely lower than what the contract specifies. Being that we have a small sample size it is possible for the one-tailed value to be off with highly skewed data, however. Given the rejection of the null and the one tail results here and below for greater than 500, I think the women’s group would have a strong case that their mean wage is not 500 and most likely lower than 500 dollars.\nQuestion C: The P value for the mean weekly wage for women being greater than 500 is 0.99. This means that if the null hypothesis was true, we would expect to get a mean weekly wage for women greater than the population mean wage 1% of the time. With the same small sample size caveat as above on one tailed results, this split in P values which would have us fail to reject and then reject the null hypothesis respectively, for above and below the population mean is quite extreme and would be another supporting point in the group filing a grievance.\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519. 7,with se = 10.0.\nA) Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. \nB) Using α = 0.05, for each study indicate whether the result is “statistically significant.” \nC)Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nSame as above, I’m going to assign out the objects and then go through the solution steps below the R code chunk. The answer for question A will be in the R code chunk, while I will answer the narrative aspects of the other questions in the text below.\n\n\n#Question Elements\n  #both studies with same n\n  study_n <- 1000\n\n  #null mean = 500\n  #alt mean =/= 500\n\n  #jones study elements\n  jones_mean <- 519.5\n  jones_se <- 10.0\n\n  #smith study elements\n  smith_mean <- 519.7\n  smith_se <- 10.0\n\n\n#Solve for Smith\n  \n  #t for Smith, use two digits and it must equal 1.97\n  smith_t <- round((smith_mean - 500) / smith_se,  digits = 2)\n  smith_t #It's 1.97 like it's supposed to be,  yay!\n\n\n[1] 1.97\n\n  #P for Smith, use three digits and two-tailed \n  smith_p <- round(2 * pt(-abs(smith_t), df = 999), digits = 3)\n  smith_p # It's .049 like it's supposed to be!\n\n\n[1] 0.049\n\n#Solve for Jones\n  \n  #t for Jones, use two digits and it must equal 1.95.\n  jones_t <- round((jones_mean - 500) / jones_se, digits = 2)\n  jones_t #It's 1.95 like it's suppost to be, yay!\n\n\n[1] 1.95\n\n  #P for Jones, use three digits and it must equal .051\n  jones_p <- round(2 * pt(-abs(jones_t), df = 999), digits = 3)\n  jones_p #It's .051 like it's supposed to be!\n\n\n[1] 0.051\n\nQuestion B and C: I thought it made sense to answer these questions in one response versus splitting them up by bullet point. Technically, both studies would be “statistically significant” at the .05 level as rounding .049 and .051 to two digits would take them both to less than or equal to .05. Rounding them without disclosing that would be no good. This is also quite misleading to attribute practical significance to a difference in P values of .002 to our arbitrarily set level of significance. Practically speaking, there is no real difference in the outcomes of these studies.\nWithout rounding, only Smith’s .049 would be below the alpha level and Jones’ .051 would be above. With these fine margins, reporting only whether the null was rejected or if we failed to reject it, or even just listing the “p less than or equal to .05 or p greater than .05” statement without the raw p values included would also ascribe practical significance in difference to these two studies even though they are very nearly identical. The differences could very well be random noise and chance, and reporting out the statement alone would make it harder to asses that.\nChoosing to publish Smith’s study just because it cleared the threshold and not Jones’ as well could make it harder to see the full picture of the parameter they studied or analyze the overall random variability in the experimental findings in a meta analysis setting.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period.\nThe sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAfter reading the question, I don’t think our initial sample was collected in a way that would allow us to be confident in a confidence interval constructed from it to look at the average price in the nation as a whole in 2005. If the federal tax is constant at 18.4, it looks like most of the variability from city to city in the sample data is driven by state and local tax levy decisions. The mean for the gas_tax data is 40.86, so over half of our mean amount comes from state and local variables that do not look to be accounted for in the sampling description as is. Most Americans in 2005 (and still today), did not live in very large cities, the amount depending of course on the exact definition of “large.”\nOur sample is only from 18 large cities in the country, and I think it’s reasonable to assume large cities have, or the very least could have, systemically different gas tax policies than other population densities. Some states could have caps or specific legislation that could impact the overall national average, and there’s not enough information about how the 18 large cities were selected or sampled to clear all of the assumptions, even if we didn’t think large cities varied from suburban, exurban or rural geographies in a way that would skew the small sample of data that we do have. Exploring bootstrapping or other approaches would also be impacted by this fact.\nNow, if I’m reading entirely too much into this set up and I should just show that I can evaluate if our mean is below a set level using a confidence interval, I’ll proceed to do that, too! To answer this question, I’ll use the psych package to get the descriptive statistics and look those over for fun and possible use to calculate by hand if the t.test result looks funny. Then I’ll use t.test(gas_tax) and look at the 95% CI range.\nIf the entire confidence interval is below 45 cents we would have enough evidence to say we think it’s 95% likely that the mean gas tax is below the 45 cent level, the equivalent of rejecting the null hypothesis and accepting the alternative hypothesis of gas taxes were likely lower than 45 cents. If the interval includes 45 and/or above that level we don’t have enough information, and would be doing the equivalent of failing to reject the null hypothesis.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n#Use describe from the psych package for overview of gas_tax data, could use the variables here to hand calculate the CI.\ngas_tax_summary <- describe(gas_taxes)\ngas_tax_summary\n\n\n   vars  n  mean   sd median trimmed  mad   min   max range  skew\nX1    1 18 40.86 9.31  41.47   41.41 9.72 18.49 54.41 35.92 -0.58\n   kurtosis   se\nX1    -0.32 2.19\n\n#Use t.test on gas_taxes to see the 95% CI\nt.test(gas_taxes)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 18.625, df = 17, p-value = 9.555e-13\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\nThe 95% interval does contain 45 cents, so we cannot say that we have enough evidence at this confidence level and interval to do the equivalent of rejecting the null hypothesis. Since the null value is at the very very top of the interval, and putting aside the other issues with the sample to begin with, it seems like the sort of result where we could say in practice it was likely lower than 45 cents. Since gas taxes and prices go to the third digit at the pump, our result would have a maximum 95% CI level “at the pump” reading of .455. It feels safe to describe in actual practice with that interval that we’re confident it was 45 cents or lower, even if we’d need a bit more data to say it in specific statistical terms.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-27T23:56:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomegeeslindacss-603-hw1/",
    "title": "HW1_DACSS603",
    "description": "DACSS 603 Homework 1",
    "author": [
      {
        "name": "Eliza Geeslin",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n  surgical_procedure sample_size mean_wait_time standard_deviation\n1            Bypasss         539             19                 10\n2        Angiography         847             18                  9\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nI will do the following to calculate the confidence intervals:\nCalculate the mean\nCalculate the standard error of the mean\nCalculate n\nDetermine a confidence level\nFind the t-score\nCalculate interval\nI will start with the values we know.\n\n\n# we already know the mean, sample size, and standard deviation\n# creating variables for all the values we do know\nbypass_n <- 539\nbypass_mean_wait_time <- 19\nbypass_sd <- 10\nangio_n <- 847\nangio_mean_wait_time <- 18\nangio_sd <- 9\n\n\n\nNext, I will specify the confidence level and use that the calculate the tail area.\n\n\n# specify confidence level\n# calculate tail area - we can use this for both the angio and bypass confidence intervals\n\nconfidence_level <- 0.90\ntail_area <- (1 - confidence_level)/2 # divide by two because we care about both sides.\n\ntail_area\n\n\n[1] 0.05\n\nThen I will use the tail areas to calculate the t-scores and confidence intervals. I will start with the bypass surgery.\n\n\n# bypass\n# calculate t-score\nbypass_t_score <- qt(p = 1 - tail_area, df = bypass_n - 1)\n\nbypass_t_score\n\n\n[1] 1.647691\n\n\n\n# bypass \n# calculate confidence internal\n\nbypass_lower <- bypass_mean_wait_time - bypass_t_score * bypass_sd / sqrt(bypass_n)\nbypass_upper <- bypass_mean_wait_time + bypass_t_score * bypass_sd / sqrt(bypass_n)\n\nprint(c(bypass_lower, bypass_upper))\n\n\n[1] 18.29029 19.70971\n\nThe confidence interval for the bypass surgery is between 18.29029 and 19.70971 days.\nNext, I will do the same for the angio surgery.\n\n\n# angio\n# calculate t-score\n\nangio_t_score <- qt(p = 1 - tail_area, df = angio_n - 1)\n\nangio_t_score\n\n\n[1] 1.646657\n\n\n\n# margin of error and confidence interval - angio\n\nangio_lower <- angio_mean_wait_time - angio_t_score * angio_sd / sqrt(angio_n)\nangio_upper <- angio_mean_wait_time + angio_t_score * angio_sd / sqrt(angio_n)\n\nprint(c(angio_lower, angio_upper))\n\n\n[1] 17.49078 18.50922\n\nThe confidence interval for the angiography surgery is between 17.49078 and 18.50922 days.\nWe can calculate that the confidence interval for mean days waiting is narrower for the angiography surgery than for the bypass surgery, but it also may be easier to see in graph form:\n\n\n# add confidence intervals to df\nsurgical_procedure = c('Bypasss', 'Angiography')\nsample_size = c(bypass_n, angio_n)\nmean_wait_time = c(bypass_mean_wait_time, angio_mean_wait_time)\nstandard_deviation = c(bypass_sd, angio_sd)\nlower = c(bypass_lower, angio_lower)\nupper = c(bypass_upper, angio_upper)\n\ndf <- data.frame(surgical_procedure, sample_size, mean_wait_time, standard_deviation, lower, upper)\n\n# compare confidence intervals - plot\n\nggplot(df) +   \n  geom_point(aes(x = surgical_procedure, y = mean_wait_time), color = \"#9784c2\", size = 3) +\n  geom_errorbar(aes(x = surgical_procedure, ymin = lower, ymax = upper), color = \"#9784c2\", width = 0.5) +\n  labs(x = \"Surgical Procedure\", y = \"Mean Wait Time (Days)\") +\n  geom_text(aes(x = surgical_procedure, y = upper, label = round(upper, digits = 2)), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -3) +\n  geom_text(aes(x = surgical_procedure, y = lower, label = round(lower, digits = 2)), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -3) +\n  geom_text(aes(x = surgical_procedure, y = mean_wait_time, label = mean_wait_time), \n            family = \"Avenir\", size=3, color = \"#33475b\", hjust = -1) +\n  theme(axis.text.x = element_text(family = \"Avenir\", color = \"#33475b\", size=10),\n        axis.text.y = element_text(family = \"Avenir\", color = \"#33475b\", size=8),\n        axis.title.y = element_text(family = \"Avenir\", color = \"#33475b\", size=13),\n        axis.title.x = element_text(family = \"Avenir\", color = \"#33475b\", size=13))\n\n\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nFirst, we want to find the point estimate (p) and then construct the confidence interval because that will be much more accurate than a single point.\n\n\n#find the point estimate\n\ncollege_education_essential <- 567\nsurvey_n <- 1031\n\npoint_estimate <- college_education_essential/survey_n\n\npoint_estimate\n\n\n[1] 0.5499515\n\nThe next thing I am going to do is calculate the margin of error on either side of the point estimate. For a 95% confidence interval, the alpha is 0.05, which means that the z-score is 1-(0.05/2) = 0.975). We can use the z-score because we are assuming a normal distribution and the sample size is greater than 30.\n\n\n# calculate the error\n\nerror <- qnorm(0.975)*sqrt(point_estimate*(1-point_estimate)/survey_n)\n\nerror\n\n\n[1] 0.03036761\n\n\n\n# calculate the confidence interval\n\nupper2 <- point_estimate + error\nlower2 <- point_estimate - error\n\nprint(c(lower2, upper2))\n\n\n[1] 0.5195839 0.5803191\n\nprint(c(round(lower2, digits = 3), round(upper2, digits = 3))) # round\n\n\n[1] 0.52 0.58\n\nHere we can see for that our sample proportion our point of estimate is 0.5499515. The 95% confidence interval indicates that the population mean is between .520 and .580. In other words, the percentage of Americans who believe college is important is between 52% and 58%. This means that when a a series of representative samples are created, 95% of the time the true mean should be between .520 and .580 (the result of % of Americans who believe college is important should be between 52% and 58%).\nAlternative Approach\nWe could also use prop.test() using the same numbers that would tell us automatically what the point of estimate and confidence are.\nconf.level = 0.95 (this is also the default for prop.test() but we will still specify)\nx = the number of “successes” (in this case it is the number of survey respondents who say that college education is needed)\nn = number of survey respondents.\n\n\n# calculate the confidence interval\n\nprop.test(x = 567, n = 1031, conf.level = 0.95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\nNOTE: There is a slight difference in the margin of error (less than .001). I suspect this has to do with how standard deviation is calculated (rounded) by the prop.test() in r. If we assume that we are rounding to the nearest hundredth this might not even be noticed.\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nRight away we know a few key things:\nrange = the difference between $30 and $200 (170)\nz-value = significance level is 5%, the alpha is 0.05, which means that the z-score is 1-(0.05/2) = 0.975)\nmargin = the estimate is useful within $5 on either side, so the margin is 5.\nThe first thing we will do is calculate the standard deviation, which we know is a quarter of the range.\n\n\n#range - difference between $30 and $200\n\nrange <- 170\n\n#significance level is 5%, alpha is 0.5\nz <- qnorm(1-(0.05/2))\n\n#margin within $5 of the true population mean - margin = 5\n\nmargin <- 5\n\n#calculate sd (\"standard deviation is a quarter of the range\")\n\nsd <- range*0.25\n\nsd\n\n\n[1] 42.5\n\nNow that I have the standard deviation, I can calculate the sample size.\n\n\n#Now I can calculate the sample size with the formula n = (z-value/margin)^2.\n\nsample_size <- ((1.96*sd)/margin)^2\n\nsample_size\n\n\n[1] 277.5556\n\nround(sample_size, digits = 0) #round to the nearest whole person\n\n\n[1] 278\n\nRounding to the nearest whole person, we get 278. Interpreting that, in order for the financial aid office to estimate the mean cost of textbooks (+ or - $5) with a significance level of 5%, they should sample 278 students.\nQuestion 4\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410’and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nReport the P-value for Ha : μ < 500. Interpret.\nReport and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nAssumptions:\nnull hypothesis (H0) is that the mean weekly earnings for the population of women at the company is $500 per week: μ = 500\nalternative hypothesis (Ha) is that the mean weekly earnings for the population of women at the company is not $500 per week: μ =/= 500\nsample size = 9\nsample mean = 410\nsample standard deviation = 90\nFirst, I will find the t-score and then I will calculate the p-value. I will assume a 95% confidence level. t-statistic and use it to find the p-value. We can use t = (sample mean - hypothesized mean)/ (sample standard deviation / sqrt(n))\n\n\n# start with what we know\n\nsalary_mean <- 410\nsalary_sd <- 90\nsalary_n <- 9\n\n# find the t-score t = (sample mean - hypothesized mean) / (sample standard deviation / sqrt(n))\n# hypothesized mean = null hypothesis = 500\n\nsalary_t_score <- (salary_mean - 500)/(salary_sd/sqrt(salary_n))\n\nsalary_t_score\n\n\n[1] -3\n\n# now I will find the p-value using pt()\n\npt(q = salary_t_score, df = salary_n-1)*2 # multiplied by two because this is a two-tailed test\n\n\n[1] 0.01707168\n\nAt the 95% confidence interval we can reject the null hypothesis that mean income is $500 based on the alternate hypothesis that mean income is not $500 because the p value is less than .05. Additionally, we see that, at the 95% confidence interval, the null hypothesis ($500) falls outside of the interval.\nNow we’ll look at the p-value if the alternative hypothesis is μ < 500.\n\n\n#p-value of the right side only (less than 500)\n\npt(q = salary_t_score, df = salary_n-1)\n\n\n[1] 0.008535841\n\nAt the 95% confidence interval we can reject the null hypothesis that mean income is $500 based on the alternate hypothesis that mean income is less than $500 because the p value 0.001, which is less than 0.05.\nNow we’ll look at if the alternative hypothesis is μ < 500.\n\n\n#p-value of the left side only\n\npt(q = salary_t_score, df = salary_n-1, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\nAt the 95% confidence interval we cannot reject the null hypothesis that mean income is $500 based on the alternate hypothesis that mean income is greater than $500 because the p value 0.99, which is greater than 0.05 and is not significant at that level.\nQuestion 5\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519. 7,with se = 10.0.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nStarting with Jones, we will find the t and p values:\nt = (sample mean - null hypothesis)/(sample standard error).\nse = 10\nsample mean = 519.5\nnull hypothesis = 500\nWe will find the p-value using pt(). Since this is a two-tailed test, we will multiply the result by two.\n\n\n# t = (y-hat - H0)/se\n#Jones got population mean of 519.5 with standard error of 10.0\n\njones_t <- (519.5-500)/10\n\n# now we are conducting a 2-sided test. Find the area to the right of 1.95 and the area to the left of -1.95 to get p-value\n# use pt(); degrees of freedom are n-1 = 999\n# pt() finds the area to the left of a value\ndf <- 999\n\njones_p <- pt(q = -1.95, df = 999) + pt(q = 1.95, df = 999, lower.tail = FALSE) #lower tail + upper tail\n\njones_t\n\n\n[1] 1.95\n\njones_p\n\n\n[1] 0.05145555\n\nWe get the same results as the ones we are given; t = 1.95, p = 0.51.\nNow we will verify the t and p values for Smith:\nt = (sample mean - null hypothesis)/(sample standard error).\nse = 10\nsample mean = 519.7\nnull hypothesis = 500\nWe will find the p-value using pt(). Since this is a two-tailed test, we will multiply the result by two.\n\n\n# Smith got population mean of 519.7 with standard error of 10.0\n\nsmith_t <- (519.7-500)/10\n\n# now we are conducting a 2-sided test. Find the area to the right of 1.97 and the area to the left of -1.97 to get p-value\n\nsmith_p <- pt(q = -1.97, df = 999) + pt(q = 1.97, df = 999, lower.tail = FALSE) #lower tail + upper tail\n\nsmith_t\n\n\n[1] 1.97\n\nsmith_p\n\n\n[1] 0.04911426\n\nWe get the same results as the ones we are given; t = 1.97, p = 0.49.\nUsing a significance level of 0.05, Jones will not be able to reject the null hypothesis because his p-value is greater than 0.05 (0.051), and his results are deemed “not statistically significant. Smith, with a p-value of 0.049 is able to reject the null hypothesis and say his results are”statistically significant.\"\nThe issue is that both of these studies are have such similar results, but because one p-value is less than 0.05, that study is deemed significant and would get published, while the other one may not. Additionally, Smith reporting a result of “p < 0.05” rather than the actual p-value can be misleading because it is so close to 0.05, and the reader of the study may assume that the result is more significant than it actual is if they don’t know the actual p-value.\nQuestion 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nFirst we calculate what we can and use those values to calculate the t-score. We will assume that p = 0.05 because we are looking at a 95% confidence interval.\n\n\n# calculate values that we can\n\nmean_gt <- mean(gas_taxes) # mean\nsd_gt <- sd(gas_taxes) # standard deviation\nn_gt <- 18 # sample size\nse_gt <-(sd_gt/sqrt(18)) # standard error\n\n# calculate t-score using qt()\n# we know that the p = 0.05 because we are looking for the 95% confidence interval.\n# df = 18 - 1\n# we are looking for the lower.tail, which is the default (\"less than 45%\")\n\nt_gt <- qt(p = 0.05, df = 17)\n\nt_gt\n\n\n[1] -1.739607\n\nFrom here we can calculate the error margin on each side of the mean: error = t * sd/sqrt(n)\n\n\n# calculate margin of error t * sd/sqrt(n)\n\nme_gt <- t_gt * sd_gt/sqrt(n_gt)\n\n# then we get the upper and lower bounds of the confidence interval\n\nupper3 <- mean_gt + me_gt\nlower3 <- mean_gt - me_gt\n\nprint(c(upper3, lower3))\n\n\n[1] 37.04610 44.67946\n\nBased on the sample from the 18 cities, the 95% confidence interval for the average tax per gallon of gas in the US is between 37.05 cents and 44.68 cents.\nIf the null hypothesis is the average tax per gallon of gas is the US in 2005 45 cents (μ = 45), we reject the null hypothesis because the upper bound of the 95% confidence interval is 44.68 cents. If the alternate hypothesis is that the average tax per gallon of gas in the US in 2005 was less than 45 cents (μ < 45) than we accept the alternate hypothesis.\n\n\n\n",
    "preview": "posts/httpsrpubscomegeeslindacss-603-hw1/distill-preview.png",
    "last_modified": "2022-02-27T23:55:46-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomektracy866318/",
    "title": "HW1 V2",
    "description": "603 Homework 1",
    "author": [
      {
        "name": "Erin Tracy",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\nQUESTION 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\r\n\r\n\r\nconfidence_level <- 0.9\r\ns_size_bypass <- 539\r\ns_size_angio <- 847\r\nS_mean_bypass <- 19\r\ns_mean_angio <- 18\r\ns_sd_bypass <- 10\r\ns_sd_angio <- 9\r\n\r\ntail_area <- (1-confidence_level)/2\r\ntail_area\r\n\r\n\r\n[1] 0.05\r\n\r\nt_score_bypass <- qt(p = 1-tail_area, df = s_size_bypass-1)\r\nt_score_bypass\r\n\r\n\r\n[1] 1.647691\r\n\r\nt_score_angio <- qt(p = 1-tail_area, df = s_size_angio-1)\r\nt_score_angio\r\n\r\n\r\n[1] 1.646657\r\n\r\nCI_bypass <- c(S_mean_bypass - t_score_bypass * s_sd_bypass / sqrt(s_size_bypass),\r\n        S_mean_bypass + t_score_bypass * s_sd_bypass / sqrt(s_size_bypass))\r\n\r\nCI_bypass\r\n\r\n\r\n[1] 18.29029 19.70971\r\n\r\nCI_angio <- c(s_mean_angio - t_score_angio * s_sd_angio / sqrt(s_size_angio),\r\n       s_mean_angio + t_score_angio * s_sd_angio / sqrt(s_size_angio))\r\n\r\nprint(CI_bypass)\r\n\r\n\r\n[1] 18.29029 19.70971\r\n\r\nprint(CI_angio)\r\n\r\n\r\n[1] 17.49078 18.50922\r\n\r\n19.70971-18.29029\r\n\r\n\r\n[1] 1.41942\r\n\r\n18.50922-17.49078\r\n\r\n\r\n[1] 1.01844\r\n\r\nI followed the instructions to “Calculate Confidence Interval Manually” from the Tutorial to get to the following conclusions:\r\nThe 90% Confidence Interval for Bypass Surgery Wait Time Mean is 18.29029 to 19.70971 days\r\nThe 90% Confidence Interval for Angiography Surgery Wait Time Mean is 17.49078 to 18.50922 days\r\nIs the confidence interval narrower for angiography or bypass surgery?\r\nThe confidence interval is more narrow for angiography.\r\nQUESTION 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\n\r\n\r\nconfidence_level <- 0.95\r\nx <- 567\r\ns_size2 <- 1031\r\np <- x/s_size2\r\np\r\n\r\n\r\n[1] 0.5499515\r\n\r\n?prop.test\r\nprop.test(x,s_size2)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  x out of s_size2, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nBased on the sample, approximately 55% of adult Americans believe that college education is essential for success. p=0.55\r\nThe 95% confidence interval is 0.5189682 to 0.5805580.\r\nWe are 95% confident that between 51.9% and 58.1% of adult Americans believe that college education is essential for success.\r\nQUESTION 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range.\r\nAssuming the significance level to be 5%, what should be the size of the sample?\r\n\r\n\r\n#n=?\r\n\r\nz <- 1.96\r\nM <-5\r\nsd <- 170/4\r\nsd\r\n\r\n\r\n[1] 42.5\r\n\r\n(sd)*(sd)*(z/M)*(z/M)\r\n\r\n\r\n[1] 277.5556\r\n\r\nI found this formula in section 5.4 of the Agresti test book.\r\nSample size should be at least 278 books.\r\nQUESTION 4 (Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nH0= Mean=500 HA= Mean does not equal 500\r\n\r\n\r\ns_mean4 <- 500\r\ns_mean_alt <- 410\r\ns <- 90\r\nn4 <- 9\r\ndf4<- n4-1\r\ndf4\r\n\r\n\r\n[1] 8\r\n\r\nt.score4 <- (s_mean_alt - s_mean4)/(s*(sqrt(9)))\r\nt.score4\r\n\r\n\r\n[1] -0.3333333\r\n\r\np.value <- 2*pt(-abs(t.score4),df4)\r\np.value\r\n\r\n\r\n[1] 0.747451\r\n\r\ngreater_p.value <- pt(t.score4, df4, lower= FALSE)\r\ngreater_p.value\r\n\r\n\r\n[1] 0.6262745\r\n\r\nless_p.value <- pt(t.score4, df4, lower= TRUE)\r\nless_p.value\r\n\r\n\r\n[1] 0.3737255\r\n\r\ngreater_p.value+ less_p.value\r\n\r\n\r\n[1] 1\r\n\r\nI started by calculating T Score = (sample mean - population mean)/ (Standard Deviation/ Square Root of Sample Size). The T Score is -0.33333\r\nI got a little lost and used google to help my find the formulat to use the T Score to find the (2 sided) P Value , specifically https://www.cyclismo.org/tutorial/R/pValues.html and followed the example in section 10.2 since it seemed similar, to calculate the p.value.\r\nI multiplied 2 (since 2 sided) by the pt calculation using the T Score and Degrees of Freedom. The result was a p value of 0.7475. This is a large P Value, so this result suggests that we should not reject the original hypothesis that $500 is the mean income.\r\nI then calculated the p value in the case that the hypothesis is that the Mean income is greater than $500, the result was 0.6268. I then calculated the p value in the case that the hypothesis is that the Mean income is less than $500, the result was 0.3737. In both of these situations, there is not enough information to reject the original hypothesis. (However it seems more likely that the mean is actuallyless than $500 than it is more than $500.)\r\nThe sum of both p-values in this calculation is 1.\r\nQUESTION 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nH0 = Mean = 500 Ha = Mean does not equal 500 Sample Size = 1000\r\n\r\n\r\npopulation_mean <- 500\r\nn5 <- 1000\r\njones_mean <- 519.5\r\njones_Se <- 10\r\n\r\n\r\njones_t <- (jones_mean - population_mean)/(jones_Se)\r\njones_t\r\n\r\n\r\n[1] 1.95\r\n\r\n?pt\r\njones_pvalue <-2*pt(jones_t,(n5-1), lower.tail= FALSE)\r\njones_pvalue\r\n\r\n\r\n[1] 0.05145555\r\n\r\nsmith_mean <- 519.7\r\nsmith_se <- 10\r\n\r\nsmith_t <- (smith_mean - population_mean)/smith_se\r\nsmith_t\r\n\r\n\r\n[1] 1.97\r\n\r\nsmith_pvalue <-2*pt(smith_t,(n5-1), lower.tail= FALSE)\r\nsmith_pvalue\r\n\r\n\r\n[1] 0.04911426\r\n\r\nThe formulas above do indicate that t = 1.95 and P-value = 0.051 for Jones and t = 1.97 and P-value = 0.049 for Smith.\r\nTechnically the Jones p-value indicates that the Jones results are statistically significant, however they are very close to being insignificant- that should be noted. The Smith p-value indicates that the Smith results are not statistically significant since they are less than 0.05\r\nQUESTION 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nn6 <- 18\r\n\r\nsummary(gas_taxes)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  18.49   35.95   41.48   40.86   47.95   54.41 \r\n\r\ngas_taxes_mean <- mean(gas_taxes)\r\ngas_taxes_mean\r\n\r\n\r\n[1] 40.86278\r\n\r\ngas_taxes_sd <- sd(gas_taxes)\r\ngas_taxes_sd\r\n\r\n\r\n[1] 9.308317\r\n\r\nconfidence_level6 <- 0.95\r\n\r\ntail_area6 <- (1 - confidence_level6)/2\r\ntail_area6\r\n\r\n\r\n[1] 0.025\r\n\r\nt_score6 <- qt(p = 1-tail_area, df = n6-1)\r\nt_score6\r\n\r\n\r\n[1] 1.739607\r\n\r\nCI <- c(gas_taxes_mean - t_score6 * gas_taxes_sd / sqrt(n6),\r\n        gas_taxes_mean + t_score6 * gas_taxes_sd / sqrt (n6))\r\n\r\nCI\r\n\r\n\r\n[1] 37.04610 44.67946\r\n\r\nI’m assuming this data is from 2005, though that’s not incredibly obvious. I’m assuming the sample of gas prices is the sum of local, state and federal.\r\nThe mean gas tax from the sample group is 40.86 cents.\r\nI again used the formula from our tutorial for calculating Confidence Interval Manually.\r\nI calculated the Confidence Interval of 95% based on the sample data provided. The CI is between 36.23 and 45.49\r\nSo I am 95% confident that the average gas tax per gallon in the US for the given time period was between 36.23 cents and 45.49 cents.\r\nBeing conservative, I don’t think it’s correct to say that I am 95% confident that the mean for all of the US gas taxes is less than 45 cents.\r\nR Markdown\r\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\r\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\r\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:56:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscommegangeorgesdacss603-hw1/",
    "title": "DACSS 603: Homework 1",
    "description": "Homework # 1 questions and answers for DACSS 603: Introduction to Quantitative Analysis",
    "author": [
      {
        "name": "Megan Georges",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\nQuestion 1:\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\n\r\n\r\nprocedure <- c('Bypass', 'Angiography')\r\nsamplesize <- c(539, 847)\r\nmeanwait <- c(19, 18)\r\nstandev <- c(10, 9)\r\n\r\nsurgdata <- data.frame(procedure, samplesize, meanwait, standev)\r\n\r\nkable(surgdata, col.names = c(\"Surgical Procedure\", \"Sample Size\", \"Mean Wait Time\", \"Standard Deviation\"), \r\n      align = c('c', 'c', 'c', 'c')) %>%\r\n    kable_styling(fixed_thead = TRUE)%>%\r\n  scroll_box(width = \"100%\", height = \"100%\")\r\n\r\n\r\n\r\n\r\nSurgical Procedure\r\n\r\n\r\nSample Size\r\n\r\n\r\nMean Wait Time\r\n\r\n\r\nStandard Deviation\r\n\r\n\r\nBypass\r\n\r\n\r\n539\r\n\r\n\r\n19\r\n\r\n\r\n10\r\n\r\n\r\nAngiography\r\n\r\n\r\n847\r\n\r\n\r\n18\r\n\r\n\r\n9\r\n\r\n\r\n\r\nAnswer:\r\nWe have the sample size, mean, and standard deviation and can assume the sample is representative of the Ontario population. We do not know the population mean or standard deviation. We will use the t-distribution to produce an interval estimate for the true mean wait times of the two procedures. According to the text (SMSS, section 5.6), “confidence intervals using the t-distribution apply with any n but assume a normal population distribution.”.\r\nUsing formula to calculate confidence intervals:\r\nI will use qt() to calculate the t-score since we know the sample distribution\r\nI will set p to .05 which accounts for .05 right tail and .05 left tail calculations to achieve the 90% confidence interval\r\ndf=n-1 for t-score\r\n\\(\\bar{y}\\) ± t(se)\r\n\r\n\r\n# Bypass procedure\r\nybarB <- 19\r\ntB <- qt(.05, df=539-1)\r\nseB <- 10/sqrt(539)\r\n\r\nybarB + tB*seB\r\n\r\n\r\n[1] 18.29029\r\n\r\nybarB - tB*seB\r\n\r\n\r\n[1] 19.70971\r\n\r\n# Angiography procedure\r\nybarA <- 18\r\ntA <- qt(.05, df=847-1)\r\nseA <- 9/sqrt(847)\r\n\r\nybarA + tA*seA\r\n\r\n\r\n[1] 17.49078\r\n\r\nybarA - tA*seA\r\n\r\n\r\n[1] 18.50922\r\n\r\nReporting the confidence intervals:\r\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 minutes.\r\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 minutes.\r\nWhich confidence interval is narrower?\r\n\r\n\r\n# Bypass confidence interval difference\r\n(ybarB - tB*seB)-(ybarB + tB*seB)\r\n\r\n\r\n[1] 1.419421\r\n\r\n# Angiography confidence interval difference\r\n(ybarA - tA*seA)-(ybarA + tA*seA)\r\n\r\n\r\n[1] 1.018436\r\n\r\nThe confidence interval for the angiography procedure is narrower than the confidence interval for the bypass procedure.\r\nQuestion 2:\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\nAnswer:\r\nSince the data is that of proportions, we will use prop.test() to calculate p and the 95% confidence interval.\r\n\r\n\r\nprop.test(567, 1031, conf.level = .95)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  567 out of 1031, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515.\r\nWe can be 95% confident that the population proportion who believe that a college education is essential for success is between 0.5189682 and 0.5805580.\r\nQuestion 3:\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer:\r\nThe formula used to determine the sample size for estimating mean is n=\\(σ^{2}\\) (\\(\\frac{z}{M})^{2}\\).\r\nThe financial aid office estimates the population standard deviation to be about a quarter or the range, which is \\(\\frac{(200-30)}{4}\\).\r\nThe office wants the confidence interval to have a length of 10 dollars or less. Since confidence interval = point estimate ± margin of error, the margin of error in this case will be \\(\\frac{10}{2}\\).\r\nWith the significance level set at 5%, z=1.96.\r\n\r\n\r\n# Computing sample size\r\nstdevBooks <- (200-30)/4\r\nmargerrorBooks <- (10/2)\r\nzBooks <- 1.96\r\n\r\nstdevBooks^2 * (zBooks/margerrorBooks)^2\r\n\r\n\r\n[1] 277.5556\r\n\r\nTo achieve an estimate of the mean cost of books with the range of a 95% confidence interval equal to or less than $10, the sample size should be at least 278.\r\nQuestion 4:\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nReport the P-value for Ha : μ < 500. Interpret.\r\nReport and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nAnswer:\r\na.\r\nTo test whether the mean income of female employees differs from $500, we will perform a one-sample two-sided significance test for a mean (which uses t-statistic).\r\nWe assume that the sample is random and that the population has a normal distribution.\r\nNull hypothesis: \\(H_{0}\\): μ = 500\r\nAlternative hypothesis: \\(H_{a}\\): μ ≠ 500\r\nThe test statistic is t=\\(\\frac{ȳ-μ_{0}}{se}\\), where se=\\(\\frac{s}{\\sqrt{n}}\\)\r\nWe will reject the null hypothesis at a p-value less than or equal to α=.05\r\n\r\n\r\n# Calculate t-statistic:\r\n(410-500)/(90/sqrt(9))\r\n\r\n\r\n[1] -3\r\n\r\n# Calculate p-value\r\npt(-3, 8)*2 \r\n\r\n\r\n[1] 0.01707168\r\n\r\n# Multiply by 2 to account for two-tails\r\n\r\n\r\n\r\nThe test statistic is t=-3 and the p-value is P=0.01707168. With an α-level of .05, the p-value is substantially less than .05, thus we will reject the null hypothesis. There is strong evidence that the mean income of female employees is not equal to $500.\r\nb.\r\n\r\n\r\n# Calculate p-value for Ha: μ < 500\r\npt(-3, 8, lower.tail = TRUE)\r\n\r\n\r\n[1] 0.008535841\r\n\r\nThe p-value for \\(H_{a}\\): μ < 500 is P=0.008535841. With an α-level of .05, the p-value is substantially less than .05, thus we will reject the null hypothesis. There is evidence that the mean income of female employees is less than $500.\r\nc.\r\n\r\n\r\n# Calculate p-value for Ha: μ > 500\r\npt(-3, 8, lower.tail = FALSE)\r\n\r\n\r\n[1] 0.9914642\r\n\r\nThe p-value for \\(H_{a}\\): μ > 500 is P=0.9914642. With an α-level of .05, we fail to reject the null hypothesis. It is highly unlikely that the mean income of female employees is greater than $500.\r\nQuestion 5:\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nUsing α = 0.05, for each study indicate whether the result is ‘statistically significant.’\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer:\r\na.\r\n\r\n\r\n# Jones t=1.95, P=.051\r\nJonesT <- (519.5-500)/10\r\nJonesT\r\n\r\n\r\n[1] 1.95\r\n\r\nJonesP <- pt(1.95, 999, lower.tail = FALSE)*2\r\nJonesP\r\n\r\n\r\n[1] 0.05145555\r\n\r\n\r\n\r\n# Smith t=1.97, P=.049\r\nSmithT <- (519.7-500)/10\r\nSmithT\r\n\r\n\r\n[1] 1.97\r\n\r\nSmithP <- pt(1.97, 999, lower.tail = FALSE)*2\r\nSmithP\r\n\r\n\r\n[1] 0.04911426\r\n\r\nb.\r\nWith an α-level of .05, the p-values that both Jones (P=.051) and Smith (P=.049) found are very close to equivalent. Although Jones’ P-value is slightly greater than α=.05 and Smith’s P-value is slightly less than α=.05, the proximity of the results should yield the same conclusion. Both P-values provide moderate evidence to reject the null hypothesis and indicate that the mean is not equal to 500. If we were to technically interpret the P-values, then Jones’ test would fail to reject the null hypothesis, and Smith’s test would reject the null hypothesis.\r\nc.\r\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. For example, a P-value of .009 for a significance level of .05 provides much stronger evidence to reject the null than a P-value of .045, however both values allow for rejection of the null at the significance level .05. In the Jones/Smith example, reporting the results only as “P ≤ 0.05” versus “P > 0.05” will lead to different conclusions about very similar results (rejecting versus failing to reject the null).\r\nQuestion 6:\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\n\r\n\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer:\r\n\r\n\r\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 10.238, df = 17, p-value = 1.095e-08\r\nalternative hypothesis: true mean is not equal to 18.4\r\n95 percent confidence interval:\r\n 36.23386 45.49169\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents (up to 45.49169).\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomtpaske870606/",
    "title": "Homework 1",
    "description": "Q1-Q6",
    "author": [
      {
        "name": "Tyler Paske",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nAnswer\r\nTo construct the 90% confidence interval, we first need one piece of missing information, the “Z” value. A 90% confidence interval corresponds to a z value of 1.645 (within 2 standard deviations of the mean by convention). With all the needed pieces of the formula we can now compute the formula in R as shown with the code provided.\r\n                                              Bypass: [18.29,19.71] \r\nDifference = 1.42\r\nR-Code:\r\n\r\nqnorm(.95)\r\n\r\nHow I got .95 was by knowing the corresponding z value. A 90% confidence interval provides tails of .95, .10/2 = .5+.90 = .95 = z value of 1.645\r\n1.644854\r\ncenter <- 19\r\nstddev <-10\r\nn <- 539\r\nerror <- qnorm(0.95)*stddev/sqrt(n)\r\nerror 0.7084886\r\nlower_bound <- center - error\r\nlower_bound 18.29151\r\nupper_bound <- center + error\r\nupper_bound 19.70849\r\n                                           Angiography: [17.49,18.51]\r\nDifference = 1.02\r\nR-Code:\r\ncenter <-18\r\nstddev <-9\r\nn <- 847\r\nerror <- qnorm(0.95)*stddev/sqrt(n)\r\nerror 0.5086606\r\nlower_bound <- center - error\r\nlower_bound 17.49134\r\nupper_bound <- center + error\r\nupper_bound 18.50866\r\n• As we can see the confidence interval is narrower for Angiography. Bypass surgy being .4 more in difference than Angiography makes the confidence interval narrower for Angiography.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\nAnswer\r\nUsing division (567/1031), I found that the point estimate “p” is 0.54995 of American Adults believed that college is essential for success while those that don’t are 45% of the population. To construct and interpret a 95% confidence interval for “p” requires further research using the following elements.\r\nSample proportion; 0.54995 = p\r\nThe sample size; 567 = x\r\nTotal Adults; 1031 = n\r\n\r\nprop.test(x=567, n=1031, p=0.54995, correct=FALSE)\r\n\r\n1-sample proportions test without continuity correction\r\ndata: 567 out of 1031,null probability 0.54995 X-squared = 9.415e-09, df = 1, p-value = 0.9999 alternative hypothesis: true p is not equal to 0.54995 95 percent confidence interval: [0.52, 0.58] sample estimates: p = 0.5499515\r\n                              Lower <- 0.52 Upper <-  0.58\r\nThe confidence interval at 95% has an interval of [.52, .58] which contains the difference between proportions. To further interpret, there are between 51-59% Americans who believe that a college education is essential for success when estimating the proportion.\r\n• The difference in this question from question 1 is that we didn’t have the standard deviation. Not having the standard deviation, we use the Confidence Interval for a Proportion Formula.\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within 5 dollars of the true population mean (i.e. they want the confidence interval to have a length of 10 dollars or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between 30 dollars and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer\r\nTo find the size of the sample you need the following.\r\nN = population size = 170\r\nz = z-score = 1.96\r\ne = margin of error = 95% or 10 as they want a confidence interval of 10\r\np = standard of deviation = 170/4 = 42.5\r\nSample mean = 170*.5 = 85\r\nNext we use the following standard formula to solve for the Sample Size.\r\n                             Sample Size = {(z*sd)/5}2 = 277.5556 \r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals 500 dollars per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\n##PART A Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\n##Answer Assumptions\r\nType of data: Quantitative as the question represents amounts rather than groupings\r\nRandomization: As the question stands our random sample stands to hold a random sample of 9 female employees.\r\nPopulation distribution: Normal distribution.\r\nSample size “n”: 9 Population Mean: 500 Standard Deviation “s”: 90 Sample mean “y”:410\r\nHypothesis The hypothesis is as follows.\r\nHo : μ=$500 H1: μ≠$500\r\nHo is the null hypothesis and represents the population mean which is $500 H1 is the alternative hypothesis and represents the population proportion not equal to $500\r\n                                    Test Statistic & P – Value\r\n• To find the answer I turn to the one sample t-Test.\r\nPopulation Mean = 500 Sample Mean = 410 Sample Size = 9 Sample Standard Deviation = 90\r\n• Assuming the data is normally distributed and the significance of the test is .05\r\nT = (Xbar – μ) / sd / sqrt(n)\r\n(410-500) / 90 / sqrt(9) = -3\r\nThe probability the t value being -3 is less than 1 percent; .85% and since the alternative hypothesis (isn’t equal), we’ll need a two-tailed probability.\r\nq = t-score = -3\r\ndf = degrees of freedom (sample size “n”-1) = 9-1 = 8\r\nLeft-tailed test in R:\r\n                        p_value=pt(q=-3, df=8, lower.tail = TRUE)\r\n0.00853584\r\nThe P-value is as follows due to that we’re calculating a two tailed test,\r\n                                     2 * 0.0085 = 0.0171\r\nWith the P-Value of 0.0171 (lower than the significance level of .05) we can say that there is significance at p <.05 between women and senior level workers making the seniors pay “rejected”. There is enough evidence to say that the women aren’t paid as much.\r\nQuestion 4 B\r\nReport the P-value for Ha : μ < 500. Interpret.\r\nAnswer\r\nRemembering that this is calculated assuming the null hypothesis is true, we look further to the left sided tail test. The P-Value itself validates if a null hypothesis is true or not. The p value is a kind of “credibility rating” of a null hypothesis in light of evidence.\r\n• To find the p-value we must first get the following values\r\nq = t-score = -3 df = degrees of freedom (sample size “n”-1) = 9-1 = 8 Left-tailed test in R:\r\n                              p_value=pt(q=-3, df=8, lower.tail = TRUE)\r\n\r\np_value P-Value: 0.008535841\r\n\r\nIn this case, the p-value is significant. This shows that we can reject the null hypothesis or in other words reject the hypothesis that was claimed.\r\nQuestion 4 C\r\nReport and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\n##Answer Right-tailed test in R:\r\n                            p_value=pt(q=-3,df=8, lower.tail=FALSE)\r\n\r\np_value P-Value: 0.9914642\r\n\r\n                                  0.008535841 + 0.9914642 = 1\r\n    \r\nHaving the P-values for the two possible one-sided tests must sum to 1 I know that I came to right conclusion.\r\nQuestion 5A\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0\r\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nAnswer\r\nTo first show the t test I used the test statistic formula: t = ȳ - μ / se\r\nJones <- (519.5 - 500) / 10\r\n                                   T-Test for Jones = 1.95\r\nSmith <- (519.7 – 500)/10\r\n                                   T-Test for Smith = 1.97 \r\nNow that we have the T-Tests for both Jones and smith we can solve for the P-Value. Remembering the formula used in question 4 we need the degrees of freedom and p = standard error. Degrees of freedom being n – 1 we find that df = 999.\r\np_value=pt(q=1.95, df=999, lower.tail = FALSE) Multiplied by 2 = .051\r\n                                    Jones P Value = 0.051\r\nPerforming the same operation for Smith we find the following, p_value=pt(q=1.97, df=999, lower.tail = FALSE) Multiplid by 2 = 0.049\r\n                                  Smiths P Value = 0.049\r\nQuestion 5B\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nAnswer\r\nWith a significance level α = 0.05, we can not reject the null hypothesis for Jones as his P Value is .051. The P Value being larger for Jones than the significance level also shows that the results are not statistically significant. Smith on the other hand is the opposite where his P-Value is less than the significance level and can reject the null hypothesis and are statistically significant.\r\nQuestion 5C\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer\r\nThe misleading aspects of reporting the results of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value is that they are based on arbitrary evaluations. Being presented with statistics that are based on random choice only shows one aspects of a situation where there can be an infinite number of outcomes. As an example, and to conclude my explanation we can’t have a binary justification as to reject or not reject a hypothesis as statistics can take a quick snapshot of a scenario in time but does not in any way describe the entire story. However, should we decide to look at a situation, snapshot or circumstance in time, the p-value does provide the best insight for the outcomes significance.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer\r\n95% Confidence interval has a 5% significance level and 1.960 Z Value\r\n• We use the Z value amongst the following values to solve for the confidence interval\r\nConfidence interval Formula: X (+or–) Z s/sqrt(n)\r\n• X is the mean = 40.86278 • Z is the chosen Z-value from the table above = 1.960 • s is the standard deviation = 9.308317 • n is the number of observations = 18\r\nR CODE used\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\n\r\nmean(gas_taxes) [1] 40.86278\r\n\r\n• The last thing that we need in order to find the confidence interval is the standard deviation.\r\n\r\nsd(gas_taxes) [1] 9.308317\r\n\r\nThe value we derived from the formula for the confidence interval is the margin or error. [36.6, 45.2] We can conclude that yes, with 95% confidence the population mean is between 36.6 and 45.2, based on only 18 samples.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-27T23:55:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomclairebattagliahomework-1-603/",
    "title": "Homework 1",
    "description": "Statistical Inference",
    "author": [
      {
        "name": "Claire Battaglia",
        "url": "https://rpubs.com/clairebattaglia"
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\n\nContents\nQuestion 1\nQuestion\nAnswer\nSolution\n\nQuestion 2\nQuestion\nAnswer\nSolution\n\nQuestion 3\nQuestion\nAnswer\nSolution\n\nQuestion 4\nQuestion\nAnswer\nSolution\n\nQuestion 5\nQuestion\nSolution\n\nQuestion 6\nQuestion\nAnswer\nSolution\n\n\nQuestion 1\nQuestion\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nSurgical Procedure\nSample Size (\\(n\\))\nMean Wait Time (\\(\\overline{x}\\))\nStandard Deviation (\\(s\\))\nbypass\n539\n19\n10\nangiography\n847\n18\n9\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nAnswer\nFor those undergoing bypass surgery the estimated mean wait time is between 18.29 and 19.71 days (19 \\(\\pm\\) .71 days).\nFor those undergoing angiography surgery the estimated mean wait time is between 17.49 and 18.51 days (18 \\(\\pm\\) .51 days).\nThe confidence interval is narrower for the population of patients undergoing angiography surgery.\nSolution\nThere are two populations: patients who have undergone bypass surgery and those who have undergone angiography surgery. I have a sample from each population and the mean and standard deviation of each sample.\nWhile I don’t know the sampling method(s), I am told each sample is representative of its corresponding population and I can see that each is large (\\(n>30\\)). Because the variable wait time is measured in days, I am working with discrete interval data, which I need to know in order to construct the confidence interval correctly.\nA confidence interval is essentially a “best guess” of a population parameter (called the point estimate), plus or minus a margin of error. The confidence level, which is frequently expressed as a percentage, is the proportion of trials in which the confidence interval would contain the true population mean. As a statement of proportion in the long run, it is only meaningful within the frequentist approach to statistics.\nThe confidence interval (\\(CI\\)) is a \\[{point\\;estimate}\\pm{margin\\;of\\;error}\\] where the \\[{margin\\;of\\;error} = t*{standard\\;error}\\] and the \\[{standard\\;error} = \\frac{\\sigma}{\\sqrt{n}}\\] Because I don’t know the true population mean (\\(\\mu\\)), I’ll use the sample mean (\\(\\overline{x}\\)) as my estimator (\\(\\hat{\\mu}\\)) of the population mean. Similarly, because I don’t know the true population standard deviation (\\(\\sigma\\)), I’ll use the sample standard deviation (\\(s\\)) as my estimator (\\(\\hat{\\sigma}\\)) of the population standard deviation. Because I don’t know the population standard deviation and am instead using the sample standard deviation as an estimate, I’m going to use the \\(t\\) distribution, which means I’ll be using a \\(t\\)-score instead of a \\(z\\)-score. It’s worth noting, however, that both samples are large enough that the \\(t\\) distribution will be essentially identical to the standard normal distribution.\nThis finally gives me the formula \\[CI=\\overline{x}\\;\\pm\\;\\left(t*\\frac{s}{\\sqrt{n}}\\right)\\] To calculate the \\(t\\)-score for a 90% confidence interval, I’m going to use R. Because the confidence interval is 90% (.9), the error probability (\\(\\alpha\\)) is .1. \\(\\alpha/2=5\\)%, or .05 on the right tail and .05 on the left tail.\n\n\nShow code\n\n# calculate quantile for t distribution\ntScoreBypass <- qt(p = .95, df = 538)\n\n# view\ntScoreBypass\n\n\n[1] 1.647691\n\nSo the final equation is \\[CI_{90}={19}\\;\\pm\\;\\left(1.647691*\\frac{10}{\\sqrt{539}}\\right)\\]\n\n\nShow code\n\n# calculate lower bound\nlowerBypass <- 19 - (1.647691*(10/sqrt(539)))\n\n# calculate upper bound\nupperBypass <- 19 + (1.647691*(10/sqrt(539)))\n\n# calculate range\nrangeBypass <- upperBypass - lowerBypass\n\n\n\nThe lower bound is 18.29029 and upper bound is 19.70971, making the range 1.41942.\nI’ll do the same calculations for my second population (patients who have undergone angiography surgery).\n\n\nShow code\n\n# calculate quantile for t distribution\ntScoreAngio <- qt(p = .95, df = 846)\n\n# view\ntScoreAngio\n\n\n[1] 1.646657\n\nSo the final equation is \\[CI_{90}={18}\\;\\pm\\;\\left(1.646657*\\frac{9}{\\sqrt{847}}\\right)\\]\n\n\nShow code\n\n# calculate lower bound\nlowerAngio <- 18 - (1.646657*(9/sqrt(847)))\n\n# calculate upper bound\nupperAngio <- 18 + (1.646657*(9/sqrt(847)))\n\n# calculate range\nrangeAngio <- upperAngio - lowerAngio\n\n\n\nThe lower bound is 17.49078 and the upper bound is 18.50922, making the range 1.01844.\nThis means that if I were to sample repeatedly from these two populations, 90% of the intervals created with these models would contain the true populations means.\nThe confidence interval is narrower for the angiography population (1.01844 \\(<\\) 1.41942). This is to be expected because the angiography sample size is larger and the standard deviation is smaller.\nQuestion 2\nQuestion\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nAnswer\nThe point estimate is 0.55 and the 95% confidence interval is 0.52 to 0.58. That is, the estimated proportion of adult Americans who believe that college education is essential for success is 55% \\(\\pm\\) 3%.\nSolution\nThe population is all adult Americans and I have a representative sample (\\(n\\)) of 1031. Of those sampled, 567 believe that a college education is essential for success. This is enough information to calculate the sample proportion (\\(\\hat{\\pi}\\)), which I’ll then use to construct a 95% confidence interval for the true population proportion.\nTo construct the confidence interval, I’ll be using the formula \\[CI=\\hat{\\pi}\\;\\pm\\;z\\sqrt{\\left(\\frac{\\hat{\\pi}\\;(1-\\hat{\\pi})}{n}\\right)}\\] First, I’ll calculate the sample proportion, which will serve as my estimator for the true population proportion.\n\n\nShow code\n\n# calculate sample proportion\nsampProp <- 567/1031\n\n\n\nThus the sample proportion is 0.5499515\nIn a standard normal distribution 95% of all values fall within 1.96 standard deviations of the mean. Because I’m constructing a 95% confidence interval, then, the \\(z\\)-score I’ll be using is 1.96.\nThus \\[CI_{95}=0.5499515\\;\\pm\\;1.96\\sqrt{\\left(\\frac{0.5499515\\;(1-0.5499515)}{1031}\\right)}\\]\n\n\nShow code\n\n# calculate lower bound\nlowerBound <- sampProp - (1.96 * sqrt(((sampProp * (1-sampProp)/1031))))\n\n# calculate upper bound\nupperBound <- sampProp + (1.96 * sqrt(((sampProp * (1-sampProp)/1031))))\n\n\n\nThus the lower bound of the confidence interval is 0.52 and the upper bound is 0.58.\nThis means that using the sample proportion of 0.5499515 as the estimator (\\(\\hat{\\pi}\\)) for the true population proportion and constructing a 95% confidence interval, I can expect that 95% of trials would contain the true population mean within the interval of 0.52 and 0.58 (\\(\\pi\\pm.0304\\)).\nAlternatively, I can use the R function prop.test to calculate everything needed to answer this question. Since I’ve already calculated everything, I’ll simply use it to validate my calculations.\n\n\nShow code\n\n# calculate prop test\nprop.test(567, 1031, p = 0.5499515)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5499515\nX-squared = 0, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\nQuestion 3\nQuestion\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\nAnswer\nThe sample size should be 278.\nSolution\nThe formula for determining the sample size for estimating a population mean (\\(\\mu\\)) is \\[n=\\sigma^2\\left(\\frac{z}{M}\\right)^2\\] where \\(n\\) is the sample size (what I’m trying to find), \\(\\sigma\\) is the population standard deviation, \\(z\\) is the \\(z\\)-score for the chosen confidence level, and \\(M\\) is the margin of error.\nWhile I don’t know the true population standard deviation (\\(\\sigma\\)), I am given an estimate, which will suffice. The suspected range is 170 and I am told the population standard deviation is estimated to be about a quarter of that, which is 42.50.\nIf the significance level (\\(\\alpha\\)) is 5%, then the confidence level is 95% (\\(1-{confidence\\;level}=\\alpha\\)). Assuming a normal distribution, the \\(z\\)-score for a 95% confidence level is 1.96.\nThe margin of error is 5.\nThus \\[n=42.5^2\\left(\\frac{1.96}{5}\\right)^2\\]\n\n\nShow code\n\n# calculate sample size\nn <- (42.5^2)*((1.96/5)^2)\n\n\n\nwhich indicates that a sample size of 277.5556 is required to estimate the population mean with a 95% confidence interval.\nBecause the units in the sample are people (i.e. a discrete unit), I’ll round up to the nearest whole number, giving me a total of 278.\n\nBecause I need a minimum of 277.5556, I would round up to the nearest whole number regardless of what the normal rule for rounding would indicate.\nThis means that I would need to sample a minimum of 278 people to estimate the mean cost of textbooks per student per quarter within plus or minus $5. If I repeatedly sample at least this number of people, 95% of the intervals constructed would contain the true population mean.\nQuestion 4\nQuestion\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nReport the P-value for Ha : μ < 500. Interpret.\nReport and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nAnswer\nThere is sufficient evidence to reject the null hypothesis and thereby accept the alternative hypothesis that the mean income for female employees does not equal $500 per week. The test statistic is -3. The \\(P\\)-value is .017.\nFor \\(H_a:\\mu<500\\), the \\(P\\)-value is .009. Because the \\(P\\)-value is less than the most stringent significance level of .01, we can reject the null hypothesis (\\(H_0:\\mu\\ge500\\)) and thereby accept the alternative hypothesis.\nFor \\(H_a:\\mu>500\\), the \\(P\\)-value is .991. Because the \\(P\\)-value is greater than the significance level of .01 (and even the more lenient .05), we cannot reject the null hypothesis (\\(H_0:\\mu\\le500\\)) at this time.\nSolution\nTo solve this problem, I’m going to assume:\nA normal population distribution.\nThe sample of female employees is sufficiently large.\nTo begin, I’ll identify the null hypothesis (\\(H_0\\)) and alternative hypothesis (\\(H_a\\)).\nThe null hypothesis is that there is no difference between the mean for female employees and the mean for all employees. That is, the mean for female employees is also 500.\n\\[{H_0:}\\;\\mu = 500\\] The alternative hypothesis is that there is a difference. That is, the mean for female employees is not 500. Because I’m interested in any kind of difference (\\(\\mu < 500\\) or \\(\\mu > 500\\)), I’ll be using a two-sided test.\n\\[{H_a:}\\;\\mu\\neq 500\\] Conducting a hypothesis test asks whether what we’ve observed (\\(\\overline{y}=410\\)) would be so unlikely if the null hypothesis (\\(H_0=500\\)) were true that we are obligated to reject it. If we find that what we observed is not that unlikely and could reasonably be explained by sample variability, we will not be able to reject the null hypothesis at this time.\nThe formula for calculating the test statistic is \\[t=\\frac{\\overline{y}-\\mu_0}{se}\\] where \\[se=\\frac{s}{\\sqrt{n}}\\] Thus the standard error is \\[se=\\frac{90}{\\sqrt{9}}\\] and \\[t=\\frac{410-500}{30}\\]\n\n\nShow code\n\n# calculate estimated standard error\nse <- 90/sqrt(9)\n\n# calculate test statistic\ntestStat <- (410-500)/se\n\n\n\n\\(t=\\) -3 and \\(|t|=\\) 3\nNext I’ll calculate the \\(P\\)-value.\n\n\nShow code\n\n# calculate 2-sided P value\npValue <- 2 * (1-pt(q = 3, df = 8))\n\n\n\nThus the \\(P\\)-value \\(=\\) 0.0170717\nThis indicates a 0.0170717 probability that we would observe the sample mean (\\(\\overline{y}\\)) of 410 if the null hypothesis were true. That is, if the mean for female employees (\\(\\mu\\)) were really 500. While this finding isn’t significant at the .01 level, it is significant at the .05 level and I can conclude that if the mean for female employees were 500, I’d be rather unlikely to end up with a sample mean of 410. I feel comfortable, then, in rejecting the null hypothesis and accepting the alternative hypothesis.\nNow I’ll look at the probabilities that the sample mean would be above or below the population mean separately.\n\n\nShow code\n\n# calculate P value for Ha > 500, right-tail\npValueG <- 1-pt(q = 3, df = 8, lower.tail = FALSE)\n\n# calculate P value for Ha < 500, left-tail\npValueL <- 1-pValueG\n\n\n\nFor \\(H_a:\\mu<500\\), \\(P=\\) 0.009. This indicates a 0.009 probability that we would have observed a \\(t\\)-score equal to or lesser than what we what we did in fact observe if the null hypothesis (\\(H_0:\\mu\\ge500\\)) were true. Put more simply, if the null hypothesis were true, it is highly unlikely we would have observed what we observed. We can reject the null hypothesis.\nFor \\(H_a:\\mu>500\\), \\(P=\\) 0.991. This indicates a 0.991 probability that we would have observed a \\(t\\)-score equal to or greater than what we did in fact observe if the null hypothesis (\\(H_0:\\mu\\le500\\)) were true. That is, if the null hypothesis were true, it is highly likely we would have observed what we observed. We can accept the null hypothesis.\nTaken together these lend strong support to the claim that the mean income for female employees is less than the mean for all employees.\nQuestion 5\nQuestion\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nSolution\nThe formula for calculating the test statistic is \\[t=\\frac{\\overline{y}-\\mu_0}{se}\\] Since they both assume the population mean (\\(\\mu\\)) to be 500 (the null hypothesis) and they both got a standard error of 10, the only difference between their tests is the mean of each of their samples (\\(\\overline{y}\\)).\nThus Jones’s test statistic is \\[t=\\frac{{519.5}-{500}}{10}\\] and Smith’s is \\[t=\\frac{{519.7}-{500}}{10}\\]\n\n\nShow code\n\n# calculate test stat Jones\ntStatJ <- (519.5-500)/10\n\n# calculate test stat Smith\ntStatS <- (519.7-500)/10\n\n\n\nFor Jones, \\(t=\\) 1.95\nFor Smith, \\(t=\\) 1.97\nNow I’ll calculate the \\(P\\)-value for each of them. This will tell me the probability of observing the data they actually observed if the null hypothesis is true. Since the alternative hypothesis is non-directional, I’ll calculate the two-sided probability.\n\n\nShow code\n\n# calculate P value Jones\npJones <- 2*pt(q = tStatJ, df = 999, lower.tail=FALSE)\n\n# calculate P value Smith\npSmith <- 2*pt(q = tStatS, df = 999, lower.tail=FALSE)\n\n\n\nFor Jones, \\(P=\\) 0.051\nFor Smith, \\(P=\\) 0.049\nSince the significance level (\\(\\alpha\\)) is .05, Jones is unable to reject the null hypothesis at this time (\\(.051>.05\\)) and his/her results are not statistically significant. Smith, however, is able to reject the null hypothesis (\\(.049<.05\\)) and can claim that his/her results are statistically significant (at the level of .05).\nThis example illustrates the danger of living and dying by whether or not the \\(P\\)-value is statistically significant. For example, if a statistically significant finding is requisite for publishing, then only Smith’s finding would make its way to a larger audience. If the \\(P\\)-value were not included, the reader might wrongly assume that the evidence for rejecting the null hypothesis is strong, when in reality it only nudges us towards that conclusion.\nAlternatively, if both findings were published and the \\(P\\)-values were not included, the reader would see that Jones does not reject the null hypothesis but that Smith does and might wrongly believe that their findings were contradictory. Including the \\(P\\)-values, however, would allow the reader to see that the findings are actually not contradictory.\nIt’s important to remember that the significance level is an arbitrary demarcation. This is ultimately a problem of using a binary framework (reject/fail to reject) in a world in which very few things (if any) are actually binary. Maintaining a larger perspective is paramount—statistics can and should inform our understanding of the world but significance testing is not a substitute for critical thinking.\nQuestion 6\nQuestion\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAnswer\nYes, there is sufficient evidence to reject the null hypothesis and thereby accept the alternative hypothesis that in 2005 the average gas tax in the United States was less than 45.00 cents.\nSolution\nTo answer this question, I’m going to conduct a one-sided significance test. To do so, I’m going to assume a random sample and normal distribution.\nThe null hypothesis is that the average tax per gallon is 45.00 cents. \\[H_0:\\mu=45.00\\] The alternative hypothesis is that the average tax per gallon is less than 45.00 cents. \\[H_a:\\mu<45.00\\] Conducting a hypothesis test asks whether the observed data (the gas taxes for the 18 cities in our sample) would be so unlikely if the null hypothesis were true that we are forced to reject it.\nFirst I’ll load the given sample data and calculate some summary statistics.\n\n\nShow code\n\n# create vector of sample data\ngasTaxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# calculate mean\nsampMean <- mean(gasTaxes)\n\n# calculate sd\nsampSD <- sd(gasTaxes)\n\n\n\nThe sample mean (\\(\\overline{y}\\)) is 40.8627778 and the sample standard deviation (\\(s\\)) is 9.3083168.\nThe formula for calculating the test statistic is \\[t=\\frac{\\overline{y}-\\mu_0}{se}\\] where \\[se=\\frac{s}{\\sqrt{n}}\\] Thus \\[se=\\frac{9.3083168}{\\sqrt{18}}\\] and \\[t=\\frac{40.8627778-45.00}{2.193991}\\]\n\n\nShow code\n\n# calculate standard error\nse <- 9.3083168/sqrt(18)\n\n# calculate t statistic\ntStatGas <- (40.8627778-45.00)/se\n\n\n\nThus the test statistic is -1.8857058.\nFinally, I’ll calculate the \\(P\\)-value.\n\n\nShow code\n\n# calculate P value\npGas <- pt(q = tStatGas, df = 17, lower.tail=TRUE)\n\n\n\nThus the \\(P\\)-value is 0.038\nThis means that if the null hypothesis were true there is a 0.038 probability of observing the data we observed or data more extreme.\nA 95% confidence interval means a significance level of .05 (\\(1-{confidence\\;level}=\\alpha\\)). Given that the \\(P\\)-value is less than the significance level of .05, I can say that, yes, given a 95% confidence interval there is sufficient evidence to reject the null hypothesis and accept the alternative hypothesis that in 2005 the average gas tax in the United States was less than 45.00 cents.\nAlternatively, I can use the R function t.test to calculate everything needed to answer this question. Since I’ve already calculated everything, I’ll simply use it to validate my calculations.\n\n\nShow code\n\nt.test(gasTaxes, mu = 45.00, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  gasTaxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\nAnother approach to hypothesis testing is constructing the appropriate confidence interval and determining whether the mean of the null hypothesis is contained within that interval or not.\nThe t.test function has returned the confidence interval and since I’m looking at it only to confirm my decision to reject the null hypothesis, I won’t calculate it manually. Because the alternative hypothesis is directional, the confidence interval is likewise one-sided. In this case it is [-\\(\\infty\\), 44.67946]. This interval does not contain the mean of the null hypothesis, which confirms my decision to reject the null hypothesis.\n\nIt’s worth noting that the upper bound of the interval is close to 45.00. This raises the question of whether the difference has any practical significance. I don’t follow conversations about gas tax policy so I can’t comment on how meaningful it is to say that the average tax is less than 45.00 cents if it could reasonably be 44.68 cents.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomjflattery870090/",
    "title": "Homework 1",
    "description": "My HW 1 for 603",
    "author": [
      {
        "name": "Justin Flattery",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nQuestion 1\nFor each procedure we will construct a confidence interval around our estimate of the actual mean by fitting the sample about a t distribution. We can do this since we have the sample mean, sample size (thus degrees of freedom) and sample standard deviation\nBYPASS Confidence Interval\nFirst we will find the T score associated with the degrees of freedom and level of confidence\n\n\nqt(p = .9, df = 538)\n\n\n[1] 1.283127\n\nWe will now plug this t score into the formula to find the standard error,\n\n\n(1.283127 * (10/(sqrt(539))))\n\n\n[1] 0.5526819\n\nWe will now add/subtract the sample mean from the standard error to produce our 90% confidence interval\n\n\n19 + (1.283127 * (10/(sqrt(539))))\n\n\n[1] 19.55268\n\n\n\n19 - (1.283127 * (10/(sqrt(539))))\n\n\n[1] 18.44732\n\nTherefore the 90% confidence interval for bypass surgery is [18.45,19.55]\nAngiography Procedure\nAgain we will find the T score associated with the degrees of freedom and level of confidence)\n\n\nqt(p = .9, df = 846)\n\n\n[1] 1.282553\n\nWe will now plug this t score into the formula to find the standard error\n\n\n(1.282553 * (9/(sqrt(847))))\n\n\n[1] 0.3966214\n\nWe will now add/subtract the sample mean from the standard error to produce our 90% confidence interval\n\n\n18 + (1.282553 * (9/(sqrt(847))))\n\n\n[1] 18.39662\n\n18 - (1.282553 * (9/(sqrt(847))))\n\n\n[1] 17.60338\n\nTherefore the 90% confidence interval for angiography surgery is [17.6,18.40]\nWhich is narrower?\nAngiography - 17.60 - 18.39 Bypass - 18.45 - 19.55\nAngiography surgery because it has a smaller range to its interval (0.79 vs 1.1)\nQuestion 2 n=1031 567=success 95% confidence\n\n\n567/1031\n\n\n[1] 0.5499515\n\nPoint estimate: 0.55 - 55% of americans believe\nI will use r’s prop test formula to find the 95% confidence interval for p, using the knowledge of 567 believed in a 1031 sample.\n\n\nprop.test(567,1031,conf.level = 0.95)$conf.int\n\n\n[1] 0.5189682 0.5805580\nattr(,\"conf.level\")\n[1] 0.95\n\nQuestion 3 First I will find the population standard deviation based on information about range\n\n\nsigma = (200-30)/4\n\n\n\nNext, given the information above, assuming we have the population standard deviation, we can assume a normal distribution of the sample. So I will find the z value based on a significance level of 5%. Since this will be a two tailed sample, this will leave 2.5% on each side, so I will find for (0.975)\n\n\nzstar = qnorm(.975)\n#I will input 42.5 as my sigma (population sd variable)\nsigma = 42.5\n#E is my desired width/range of my confidence interval\nE = 10\n\n\n\nI will now input these variables into the appropriate formula - z^2 * sigma^2 / (E^2) to find the necessary sample size\n\n\n(zstar*zstar) * (sigma*sigma)/ (E*E)\n\n\n[1] 69.38635\n\nSince the value is 69.38, we will round up to the next whole number, 70 in order to ensure we have a large enough sample to estimate the mean\nQuestion 4 In order to evaluate if the mean income is different for female employees, we must set up a hypothesis test. In order to evaluate whether it is NOT $500 we will set a null hypothesis that the mean income = $500 per week. The alternative hypothesis will be the opposite of this, that it is different from $500 a week so:\nH0: mu= $500 Ha mu not = 500\nTest assumes data obtained through randomization and sample is representative of larger population.\nFirst we will find the value of the test statistic (tscore), (how far the sample distance is from the mean in a t distribution)\n\n\n(410 - 500)/((90/3))\n\n\n[1] -3\n\nWe will next use r to find the value of p (the probability of observing this occurrence assuming the null hypothesis)for a two tailed test, inputting our test statistic of -3 and df (9-1)\n\n\n2*pt(q=-3, df=8, log = FALSE)\n\n\n[1] 0.01707168\n\nThis produces a p value of 0.017. Since this is a very small number ( less than 0.05), it indicates a very small probability of occurrence and that we can reject the null hypothesis that the population mean is 500 at the 5% significance level.\nB For part b) We will set up a different hypothesis: H0: mu> $500 Ha mu < $500\nWe will plug in the same values (t statistic, df) but use a one tailed test:\n\n\npt(q=-3, df=8,lower.tail = TRUE)\n\n\n[1] 0.008535841\n\nThis produces a p value of 0.008. Since this is a very small number ( less than 0.05), it indicates a very small probability of occurrence and that we can reject the null hypothesis that the population mean is greater than 500 at the 5% significance level.\nC\nFor part c) We will set up a different hypothesis:\nH0: mu< $500 Ha mu > $500\n\n\npt(q=-3, df=8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\nThis produces a p value of 0.99. Since this is a very large number/close to one and greater than significance level of 0.05, it indicates a very high probability of occurrence and therefore we cannot reject the null hypothesis that the population mean is less than 500 at the 5% significance level.\nQuestion 5 H0: μ = 500 Ha : μ = 500, each with n = 1000. Jones has ȳ = 519.5, with se = 10.0. Smith has ȳ = 519.7, with se = 10.0.\nFirst I will find the t statistic for Jones using formula used in previous questions\n\n\n(519.5 - 500)/(10)\n\n\n[1] 1.95\n\nJones calculating p value\n\n\n2*pt(q=1.95, df=499, lower.tail=FALSE)\n\n\n[1] 0.05173574\n\nNow I will find the t statistic for Smith using formula used in previous questions\n\n\n(519.7 - 500)/(10)\n\n\n[1] 1.97\n\nSmithh calcultaing p value\n\n\n2*pt(q=1.97, df=499, lower.tail=FALSE)\n\n\n[1] 0.04939092\n\nThe result for Smith is statistically significant at the 5% level (since the value of p is 0.049 <.05), the result for Jones it is not statistically significant at 5% level (p = 0.051>.05)\nWithout reporting the actual p value , data can be manipulated to state the sample is significant for example, for Jones, it would be misleading to round down 0.051 to 0.05 and report that p is less than or equal to 0.05, and thus statistically significant. By reporting instead as the reality that P>0.05 (with p included) is much more transparent. Additionally, the p value should be reported along with the significance level when making a statement around rejecting the null hypothesis. For example, we could reject the null hypothesis at the 10% level for Smith - but this would be manipulating the data from our original planned significance level.\nQuestion 6\nFirst I will load in the dataset of gas_taxes based on the sample\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\nNext, I used r to calculate a t test of the data set under a 95% confidence level\n\n\nt.test(gas_taxes, conf.level = 0.95)$conf.int\n\n\n[1] 36.23386 45.49169\nattr(,\"conf.level\")\n[1] 0.95\n\nConclusion - No - the upper bound of a 95% confidence interval is over 45 cents, therefore we can not definitively say that the average tax per gallon was less than 45C.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomkbec864954/",
    "title": "Homework 1 DACSS 603",
    "description": "Descriptive Statistics, Probability, Statistical Inference, and Comparing Two Means",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nSurgical Procedure - Representative Sample\r\nSurgical Procedure\r\nSample Size\r\nMean Wait Time\r\nStandard Deviation\r\nBypass\r\n539\r\n19\r\n10\r\nAngiography\r\n847\r\n18\r\n9\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\r\nIs the confidence interval narrower for angiography or bypass surgery?\r\nAnswer 1\r\nI calculated the answer by first calculating the standard error for each procedure given the mean, standard deviation, and sample size for each. I do so using 0.95 for the qnorm function so that I can determine the 5% confidence level for both the right and left side of the normal distribution, since the sample is larger than n=30. By calculating the 5% margin for each side of the distribution, this gives me the effective 90% confidence interval overall.\r\n\r\n\r\n#Calculate the actual mean wait time for the bypass:\r\n\r\nxbar1 <- 19 #sample mean\r\nsd1a <- 10 #sample standard deviation\r\nn1a <- 539 #sample size\r\n\r\nerror1a <- qnorm(0.95)*sd1a/sqrt(n1a)\r\nerror1a\r\n\r\n\r\n[1] 0.7084886\r\n\r\nlower1a <- xbar1-error1a\r\nupper1a <- xbar1+error1a\r\n\r\nlower1a\r\n\r\n\r\n[1] 18.29151\r\n\r\nupper1a\r\n\r\n\r\n[1] 19.70849\r\n\r\n\r\n\r\n#Calculate the actual mean wait time for the angiography:\r\n\r\nxbar1b <- 18 #sample mean\r\nsd1b <- 9 #sample standard deviation\r\nn1b <- 847 #sample size\r\n\r\nerror1b <- qnorm(0.95)*sd1b/sqrt(n1b)\r\nerror1b\r\n\r\n\r\n[1] 0.5086606\r\n\r\nlower1b <- xbar1b-error1b\r\nupper1b <- xbar1b+error1b\r\n\r\nlower1b\r\n\r\n\r\n[1] 17.49134\r\n\r\nupper1b\r\n\r\n\r\n[1] 18.50866\r\n\r\nNext, I created a data frame with the information.\r\n\r\n\r\nShow code\r\n\r\n#Create a data frame with the information:\r\n\r\ndf1 <- data.frame( c('Bypass', 'Angiography')\r\n                   ,c(19, 18)\r\n                   ,c(539, 847)\r\n                   ,c(10, 9)\r\n                   ,c(18.29151, 17.49134)\r\n                   ,c(19.70849, 18.50866))\r\nnames(df1) <- c('Procedure', 'Mean', 'Sample', 'SD', 'Lower', 'Upper')\r\n\r\ndf1\r\n\r\n\r\n    Procedure Mean Sample SD    Lower    Upper\r\n1      Bypass   19    539 10 18.29151 19.70849\r\n2 Angiography   18    847  9 17.49134 18.50866\r\n\r\nFinally, I created a plot to visualize the results. The visualization communicates that for each procedure, there is a range of values where we can expect 90% of the estimates to include the population mean given the sample mean and standard deviation.\r\n\r\n\r\nShow code\r\n\r\ngg1 <- ggplot(data = df1)\r\ngg1 <- gg1 + geom_point(aes(x = Procedure, y = Mean), size = 5, color = \"blue\")\r\ngg1 <- gg1 + geom_errorbar(aes(x = Procedure, y = Mean, ymin=Lower, ymax=Upper), width=.1, color = \"blue\")\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Lower, label = round(Lower, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Upper, label = round(Upper, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Mean, label = round(Mean, 2)), hjust = -0.5)\r\ngg1 <- gg1 + labs(x = \"Procedure\", y = \"Mean\")\r\ngg1 <- gg1 + labs(title = \"Chart Showing Mean With Upper and Lower Confidence Intervals at 90%\")\r\ngg1 <- gg1 + theme_classic()\r\n\r\ngg1\r\n\r\n\r\n\r\n\r\nFor the angiography, we can be 90% sure that the population mean for the wait time to the procedure falls between 17.49 and 18.51 days.\r\nFor the bypass, we can be 90% sure that the population mean for the wait time to the procedure falls between 18.29 and 19.71 days.\r\nThe confidence interval was narrower for the angiography surgery.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success.\r\nFind the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\r\nConstruct and interpret a 95% confidence interval for p.\r\nAnswer 2\r\nTo construct the 95% confidence interval, I began by calculating the point sample estimate of the population proportion:\r\n\r\n\r\n#Calculate the point sample estimate of the population proportion using (p = x/n)\r\n\r\nx2 = 567 #affirmative response size\r\nn2 = 1031 #sample size - survey participants\r\n\r\np2 <- x2/n2\r\np2\r\n\r\n\r\n[1] 0.5499515\r\n\r\nThis tells me that the sample proportion of those who believe a college education is essential for success is ~45%.\r\nSince np >= 5 and n(1-p) >= 5, I know that I will calculate the confidence interval of that population proportion as follows: p +/- z * square root of (p) x (1-p)/n\r\n\r\n\r\n#Calculate the confidence interval for p:\r\n\r\nerror2 <-qnorm(0.975)*sqrt(p2*(1-p2)/n2)\r\nerror2\r\n\r\n\r\n[1] 0.03036761\r\n\r\nlower2 <- p2-error2\r\nupper2 <- p2+error2\r\n\r\nlower2\r\n\r\n\r\n[1] 0.5195839\r\n\r\nupper2\r\n\r\n\r\n[1] 0.5803191\r\n\r\nThis tells me that we can be 95% confident that the proportion of adult Americans who believe that a college education is essential for success lies between 51.96% and 58.03% of the population.\r\nAlternatively, using the R prop.test function, I can compare the calculation to my manual calculation and find it is the same for finding the point, but slightly different (at 4 decimal points) on the calculation of the confidence interval.\r\n\r\n\r\nprop.test(x2, n2)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  x2 out of n2, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range.\r\nAssuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer 3\r\nI will start by taking the information given and calculating the variables I need to know.\r\nIf the aid office believes the amount spent on books is between $30 and $200, I have a range of $170 to consider.\r\nI want to be 95% confident that the interval estimate contains the population mean, so my z = 1.96.\r\nI also know that the margin of error should be no more than +/- $5 on each end of the estimate, so my margin of error = 5\r\n\r\n\r\nerror3 <- (5)\r\n\r\n#I need to calculate the standard deviation at the given estimate that it is a quarter of the range:\r\n\r\nsd3 <- 170*0.25 #range * 25%\r\nsd3 #standard deviation = 42.5\r\n\r\n\r\n[1] 42.5\r\n\r\n#Now I can calculate the sample size with the formula n=(zσ/M)2.\r\n\r\nss3 <- ((1.96*sd3)/error3)^2\r\nss3 #sample size\r\n\r\n\r\n[1] 277.5556\r\n\r\nUsing these calculations, I can estimate that the financial aid office will need to use a sample size of 278 people.\r\nQuestion 4\r\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nB. Report the P-value for Ha : μ < 500. Interpret.\r\nC. Report and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nAnswer 4\r\nA. I will start by taking the information given and determining my hypotheses:\r\nH0: The mean weekly earnings for the population of women at the company is μ=$500#\r\nHa: The mean weekly earnings for the population of women at the company is μ≠$500\r\nI cannot assume that the population distribution is normal as I have a low sample size of 9 and it is not stated it is a random sample, but a representative sample.\r\nMy other assumptions are:\r\npopulation mean (mu4) = 500\r\nsample size (n4) = 9\r\nsample mean (xbar4) = 410\r\nsample standard deviation (sd4) = 90\r\nI also need to make a decision about using a significance level of 5%.\r\nI will use the test statistic formula to find the t-value: [t = (x̄) - (μ) / (sd/sqrt(n)]\r\n\r\n\r\na4 <- 500\r\nn4 <- 9\r\nxbar4 <- 410\r\nsd4 <- 90\r\n\r\n#Test statistic:\r\n\r\nt4 <-(xbar4-a4)/(sd4/sqrt(n4))\r\n\r\nt4\r\n\r\n\r\n[1] -3\r\n\r\nGiven that my test statistic = (-3), I can determine the p-value is .00135*2 (to get the sum of both tail probabilities) or 0.0027.\r\nSince my confidence level is 0.05 and my p-value of 0.0027 < 0.05, I can reject the null hypothesis that the mean weekly earnings for the population of women at the company is μ=$500.\r\n\r\n\r\n#Then I take the t-statistic result (-3) and the degrees of freedom by taking \"sample size - 1\" or (\"9\" - 1) = 8.\r\n\r\npval4a <- pt(-3, 8)\r\n\r\npval4a\r\n\r\n\r\n[1] 0.008535841\r\n\r\nB. For the alternative hypothesis Ha: μ < 500:\r\n\r\n\r\npval4b <- pt(-3, 8, lower.tail=FALSE)\r\n\r\npval4b\r\n\r\n\r\n[1] 0.9914642\r\n\r\nPer the hint, I can confirm that these are logical answers by adding the two probabilities together and confirming they equal 1:\r\n\r\n\r\npval4a+pval4b\r\n\r\n\r\n[1] 1\r\n\r\nQuestion 5\r\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000.\r\nJones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7.\r\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer 5\r\nA. To show the t-scores, I will use the test statistic [t = (ybar) - (μ) / (se)] given the results for each:\r\n\r\n\r\n#Test statistic for Jones:\r\n\r\nybar5a <- 519.5\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5a <-(ybar5a-a5)/(se5)\r\n\r\nt5a\r\n\r\n\r\n[1] 1.95\r\n\r\n#Test statistic for Smith:\r\n\r\nybar5b <- 519.7\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5b <-(ybar5b-a5)/(se5)\r\n\r\nt5b\r\n\r\n\r\n[1] 1.97\r\n\r\nTo show the p-values, I will use the pt() function and use the t-statistic results from each of the tests and the degrees of freedom by taking “sample size - 1” or (“100” - 1) = 999. I will need to multiply each result by 2 to account for the probabilities in each tail of the normal distribution.\r\n\r\n\r\n#For Jones' results:\r\n\r\npval5a <- pt(1.95, 999, lower.tail = FALSE) * 2\r\n\r\npval5a\r\n\r\n\r\n[1] 0.05145555\r\n\r\n#For Smith's results:\r\n\r\npval5b <- pt(1.97, 999, lower.tail = FALSE) * 2\r\n\r\npval5b\r\n\r\n\r\n[1] 0.04911426\r\n\r\nB. To use α = 0.05 and look at each result and whether it is “statistically significant”, I can compare the p-values directly to the confidence level of 0.5. Jones’ results gave a p-value of 0.5145, which is just over the threshold of the confidence level given of 0.5. Smith’s results gave a p-value of 0.4911, which is just under the threshold of the confidence level given of 0.5.\r\nHypothesis tests tell us that if the p-value < α, we reject H0 and if p-value ≥ α, we do not reject H0. Given this general statistical guidance, only Smith’s results would be considered “statistically significant”.\r\nC. The results in this example could be very misleading if only whether the results were reported as simply “rejecting” or “not rejecting” H0 or being “statistically significant” or not because that is leaving out vital information on how close the results were to the confidence level used. If the actual p-values were reported instead, they would reflect how marginally “significant” the results really were. The confidence is an artificial threshold that is subjectively applied, and in this case, the results were close enough that they should not be statistically reported as significally different. This is why we need to be sure we report the full p-values and confidence intervals.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer\r\nTo answer whether there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents, we need to use a left-tailed t-test. We will use the sample data in the variable “gas_taxes” in the t.test function. We know that the t.test() function uses the 95% confidence interval as a default, but it also uses a two-sided test as the default, so we need to provide the alternative argument “less”, indicating a left-sided test.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nt.test(gas_taxes, alternative = \"less\")\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 1\r\nalternative hypothesis: true mean is less than 0\r\n95 percent confidence interval:\r\n     -Inf 44.67946\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThis t-test gives us a confidence interval of [inf - 44.67946]. Since the confidence interval includes amounts that are all less than 45 cents, we have enough evidence to conclude that, at that confidence level, the average tax was less than 45 cents.\r\n\r\n\r\n\r\n",
    "preview": "posts/httpsrpubscomkbec864954/distill-preview.png",
    "last_modified": "2022-02-24T15:25:24-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/httpsrpubscomowenvespa865400/",
    "title": "Statistical Inference II & Comparing two means",
    "description": "DACSS 603 Homework 1",
    "author": [
      {
        "name": "Rhowena Vespa",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\r\n\r\n\r\nlibrary(distill)\r\nlibrary(dplyr)\r\nlibrary(tidyverse)\r\nProblem1<- read.csv('homework1_prob1.csv',TRUE,',',na.strings = \"N/A\")\r\nProblem1\r\n\r\n\r\n  ï..Surgical.Procedure Sample.Size Mean.Wait.Time Standard.Deviation\r\n1                Bypass         539             19                 10\r\n2           Angiography         847             18                  9\r\n\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\r\nFor Bypass group:\r\n\r\n\r\n#Our values are: \r\nBypass.mean<- 19\r\nBypass.sd<-10\r\nBypass.n<-539\r\nBypass.se <- Bypass.sd/sqrt(Bypass.n) #This is standard error of the mean\r\n\r\n\r\n\r\n\r\n\r\n#Find the t.score for CI 90%\r\nalpha = 0.1\r\ndegrees.freedom = Bypass.n - 1\r\nBypass.t.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\r\nprint(Bypass.t.score)\r\n\r\n\r\n[1] 1.647691\r\n\r\n\r\n\r\n#Calculate margin of error\r\nBypass.margin.error <- Bypass.t.score * Bypass.se\r\nprint(Bypass.margin.error)\r\n\r\n\r\n[1] 0.7097107\r\n\r\n\r\n\r\n#Calculate the 90% confidence interval for Bypass\r\n  lower.bound <- Bypass.mean - Bypass.margin.error\r\n  upper.bound <- Bypass.mean + Bypass.margin.error\r\n  print(c(lower.bound,upper.bound))\r\n\r\n\r\n[1] 18.29029 19.70971\r\n\r\nFor Angiography group:\r\n\r\n\r\n#Our values are: \r\nAngio.mean<- 18\r\nAngio.sd<-9\r\nAngio.n<-847\r\nAngio.se <- Angio.sd/sqrt(Angio.n) #This is standard error of the mean\r\n\r\n\r\n\r\n\r\n\r\n#Find the t.score for CI 90%\r\nalpha = 0.1\r\ndegrees.freedomA = Angio.n - 1\r\nAngio.t.score = qt(p=alpha/2, df=degrees.freedomA,lower.tail=F)\r\nprint(Angio.t.score)\r\n\r\n\r\n[1] 1.646657\r\n\r\n\r\n\r\n#Calculate margin of error\r\nAngio.margin.error <- Angio.t.score * Angio.se\r\nprint(Angio.margin.error)\r\n\r\n\r\n[1] 0.5092182\r\n\r\n\r\n\r\n#Calculate the 90% confidence interval for Angiography\r\n  lower.boundA <- Angio.mean - Angio.margin.error\r\n  upper.boundA <- Angio.mean + Angio.margin.error\r\n  print(c(lower.boundA,upper.boundA))\r\n\r\n\r\n[1] 17.49078 18.50922\r\n\r\nAnswer for Question #1-Is the confidence interval narrower for angiography or bypass surgery?:\r\nAngiography patients, at 90% confidence interval, had between (17.49078 and 18.50922) wait time in days which is NARROWER compared to the bypass patients wait time which is between (18.29029 and 19.70971)\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\r\n\r\n\r\n#Point Estimate, p\r\nn<-1031\r\nk<-567\r\np<-k/n\r\np\r\n\r\n\r\n[1] 0.5499515\r\n\r\nInterpretation of point estimate p:\r\nThe sample proportion of adult Americans who believed that college education is essential for success is 0.5499515 or 55%. This represents our point estimate for the population (adult Americans) proportion.\r\n\r\n\r\n#Construct 95% confidence interval for p\r\n\r\nS.margin <- qnorm(0.975)*sqrt(p*(1-p)/n)  #calculate margin of error\r\n  S.lower.bound <- p-S.margin\r\n  S.upper.bound <- p+S.margin\r\n  print(c(S.lower.bound,S.upper.bound))\r\n\r\n\r\n[1] 0.5195839 0.5803191\r\n\r\nInterpretation of 95% confidence interval for p:\r\nThe 95% confidence interval for the population (adult Americans) proportion is [0.5195839 0.5803191]. This means between 51.9% to 58% of adult Americans believed that college education is essential for success.\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range. Assuming the significance level to be 5%, what should be the size of the sample?\r\nn= square of ((Z 0.05/2 * sd of pop)/within $5 of true pop mean\r\n\r\n\r\nsd_of_pop =(200-30)/4 #This is standard deviation of population\r\nsd_of_pop\r\n\r\n\r\n[1] 42.5\r\n\r\nsample_size=((1.96*sd_of_pop)/5)**2  #Using Z score 1.96 for significance level 5%\r\nsample_size\r\n\r\n\r\n[1] 277.5556\r\n\r\nANSWER: Sample size needed to achieve significance level of 95% is 278.\r\nQuestion 4\r\n(Exercise 6.7, Chapter 6 of SMSS, Agresti 2018) According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nassumption: seed set at 123, using rnorm\r\nHo mu=500\r\nHa mu≠500\r\nMean y_hat =410\r\nsd = 90\r\nn =9\r\n\r\n\r\nset.seed(123)\r\nMean_income <- c(rnorm(9, mean = 410, sd = 90)) \r\nMean_income\r\n\r\n\r\n[1] 359.5572 389.2840 550.2837 416.3458 421.6359 564.3558 451.4825\r\n[8] 296.1445 348.1832\r\n\r\n\r\n\r\nt.test(Mean_income, mu = 500) # Ho: mu=500\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  Mean_income\r\nt = -2.6213, df = 8, p-value = 0.03059\r\nalternative hypothesis: true mean is not equal to 500\r\n95 percent confidence interval:\r\n 353.2313 490.6071\r\nsample estimates:\r\nmean of x \r\n 421.9192 \r\n\r\nInterpretation of one sample t-test result:\r\nAt p-value 0.03, considered statistically significant, we reject the null hypothesis. We can conclude that mean income for female employees is not 500.\r\nReport the P-value for Ha : μ < 500. Interpret.\r\n\r\n\r\nt.test(Mean_income, mu=500, alternative = 'less') # Ha: mu<500\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  Mean_income\r\nt = -2.6213, df = 8, p-value = 0.01529\r\nalternative hypothesis: true mean is less than 500\r\n95 percent confidence interval:\r\n     -Inf 477.3087\r\nsample estimates:\r\nmean of x \r\n 421.9192 \r\n\r\nInterpretation of Ha : μ < 500:\r\nAt p-value 0.01529, considered statistically significant, we reject the null hypothesis and accept the alternative hypothesis. We can conclude that mean income for female employees is less than 500.\r\nReport and interpret the P-value for Ha: μ > 500.\r\n\r\n\r\nt.test(Mean_income, mu=500, alternative = \"greater\") # Ha: mu>500\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  Mean_income\r\nt = -2.6213, df = 8, p-value = 0.9847\r\nalternative hypothesis: true mean is greater than 500\r\n95 percent confidence interval:\r\n 366.5297      Inf\r\nsample estimates:\r\nmean of x \r\n 421.9192 \r\n\r\nInterpretation of Ha: μ > 500:\r\nAt p-value 0.9847, considered statistically NOT significant, we fail to reject the null hypothesis. We can reject the alternative hypothesis that mean income for female employees is greater than 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\n\r\n\r\ntotal_p_value=0.9847+0.01529\r\nprint(c(\"Total p-values for the two possible one-sided tests is\",total_p_value))\r\n\r\n\r\n[1] \"Total p-values for the two possible one-sided tests is\"\r\n[2] \"0.99999\"                                               \r\n\r\nQuestion 5\r\n(Exercise 6.23, Chapter 6 of SMSS, Agresti 2018) Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\r\n\r\n\r\ntab <- matrix(c(519.5, 519.7, 10, 10), ncol=2, byrow=TRUE)\r\ncolnames(tab) <- c(\"Jones\",\"Smith\")\r\nrownames(tab) <- c(\"y_hat\",\"se\")\r\ntab <- as.table(tab)\r\nprint(tab)\r\n\r\n\r\n      Jones Smith\r\ny_hat 519.5 519.7\r\nse     10.0  10.0\r\n\r\nCalculate t and p-value\r\nt.stat <- (y_hat - mu)/sample.se\r\np.value = pt(q=abs(t.stat), df=degrees.freedom,lower.tail=F) 2 *\r\nShow that t = 1.95 and P-value = 0.051 for Jones.\r\n\r\n\r\nt.stat <- (519.5 - 500)/10\r\nprint(c(\"t.stat\",t.stat))\r\n\r\n\r\n[1] \"t.stat\" \"1.95\"  \r\n\r\n\r\n\r\ndegrees.freedom = 1000 - 1\r\np.value = pt(q=abs(t.stat), df=degrees.freedom,lower.tail=F) * 2\r\nprint(c(\"Two-sided p-value\",p.value))\r\n\r\n\r\n[1] \"Two-sided p-value\"  \"0.0514555476459477\"\r\n\r\nShow that t = 1.97 and P-value = 0.049 for Smith.\r\n\r\n\r\nSmith.t.stat <- (519.7 - 500)/10\r\nprint(c(\"t.stat\",Smith.t.stat))\r\n\r\n\r\n[1] \"t.stat\" \"1.97\"  \r\n\r\n\r\n\r\ndegrees.freedom = 1000 - 1\r\nSmith.p.value = pt(q=abs(Smith.t.stat), df=degrees.freedom,lower.tail=F) * 2\r\nprint(c(\"Two-sided p-value\",Smith.p.value))\r\n\r\n\r\n[1] \"Two-sided p-value\"  \"0.0491142565416521\"\r\n\r\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\r\n  α = 0.05\r\n  H0: μ = 500\r\n  Ha : μ ≠ 500\r\n  \r\nFor Jones Data: Since p=0.051, we fail to reject the null hypothesis (H0: μ = 500) and reject the alternative hypothesis (Ha : μ ≠ 500). The p-value is NOT statistically significant.\r\nFor Smith Data: Since p=0.049, we reject the null hypothesis (H0: μ = 500) and accept the alternative hypothesis. We conclude that μ ≠ 500. P-value is statistically significant.\r\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\n\r\n\r\nprint(tab)\r\n\r\n\r\n      Jones Smith\r\ny_hat 519.5 519.7\r\nse     10.0  10.0\r\n\r\n#α = 0.05\r\n#H0: μ = 500\r\n#Ha : μ ≠ 500\r\n#Zc= 1.96\r\n\r\n\r\n\r\nCalculate z-score= (y_hat -μ )/ se\r\n\r\n\r\nJones.z <- (519.5-500)/10\r\nJones.z\r\n\r\n\r\n[1] 1.95\r\n\r\n\r\n\r\nSmith.z <- (519.7-500)/10\r\nSmith.z\r\n\r\n\r\n[1] 1.97\r\n\r\nInterpretation without reporting the actual P-value:\r\nReporting the result of a test using p-values could be misleading. We can avoid this by using z-score to report the results, using z-score =1.96 as the same 95% confidence level. Jones z-score of 1.95 < 1.96 means we fail to reject the null hypothesis (H0: μ = 500) and reject the alternative hypothesis (Ha : μ ≠ 500). Smith z-score of 1.97 > 1.96 we reject the null hypothesis (H0: μ = 500) and accept the alternative hypothesis(Ha : μ ≠ 500).\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\n#H0: μ = 45\r\n#Ha : μ ≠ 45, specifically μ < 45 assuming one-sided using argument alternative= \"lesser\"\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nt.test(gas_taxes,mu=45, alternative = 'less')\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = -1.8857, df = 17, p-value = 0.03827\r\nalternative hypothesis: true mean is less than 45\r\n95 percent confidence interval:\r\n     -Inf 44.67946\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nInterpretation of one sample t-test:\r\nYes, there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents. With the p-value of 0.03827 < α = 0.05, we reject the null hypothesis that μ = 45 and accept the alternative hypothesis that μ < 45 cents.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/httpsrpubscomrhyslong96870303/",
    "title": "Homework 1",
    "description": "Here is my submission for homework 1.",
    "author": [
      {
        "name": "Rhys Long",
        "url": {}
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\n\n\n##QUESTION 1##\n#Sample Size\nAngiography_Sample=847\nBypass_Sample=539\n\n#Mean\nAngiography_Mean=18\nBypass_Mean=19\n\n#Standard Deviation\nAngiography_STD=9\nBypass_STD=10\n\n#Standard Error\nAngiography_SE=Angiography_STD/sqrt(Angiography_Sample)\nBypass_SE=Bypass_STD/sqrt(Bypass_Sample)\n\n#Tail Area\nConfidence_Level=0.9\nTail_Area=(1-Confidence_Level)/2\n\n#T Scores\nAngiography_T=qt(p=0.95,df=(Angiography_Sample-1))\nBypass_T=qt(p=0.95,df=(Bypass_DF=Bypass_Sample-1))\n\n#Margin Of Error\nAngiography_MOE=Angiography_T*Angiography_SE\nBypass_MOE=Bypass_T*Bypass_SE\n\n#Angiography Confidence Interval\nLower_Angiography=Angiography_Mean-Angiography_MOE\nUpper_Angiography=Angiography_Mean+Angiography_MOE\nAngiography_CI=c(Lower_Angiography, Upper_Angiography)\nc(\"Angiography Confidence Interval:\", Angiography_CI)\n\n\n[1] \"Angiography Confidence Interval:\"\n[2] \"17.4907818376895\"                \n[3] \"18.5092181623105\"                \n\n#Bypass Confidence Interval\nLower_Bypass=Bypass_Mean-Bypass_MOE\nUpper_Bypass=Bypass_Mean+Bypass_MOE\nBypass_CI=c(Lower_Bypass, Upper_Bypass)\nc(\"Bypass Confidence Interval:\", Bypass_CI)\n\n\n[1] \"Bypass Confidence Interval:\" \"18.2902893200424\"           \n[3] \"19.7097106799576\"           \n\n#Comparing The Confidence Interval Widths\nAngiography_Width=Upper_Angiography-Lower_Angiography\nBypass_Width=Upper_Bypass-Lower_Bypass\nc(\"Angiography Confidence Interval Width:\", Angiography_Width)\n\n\n[1] \"Angiography Confidence Interval Width:\"\n[2] \"1.01843632462099\"                      \n\nc(\"Bypass Confidence Interval Width:\", Bypass_Width)\n\n\n[1] \"Bypass Confidence Interval Width:\"\n[2] \"1.41942135991513\"                 \n\nThe actual mean wait for angiographies is around 17.4907818376895 to 18.5092181623105 days and the actual mean wait for bypasses is 18.2902893200424 to 19.7097106799576. To find out the confidence interval for both procedures, I first found the standard error, since I already knew the sample sizes and standard deviations. From there I determined the t scores for both procedures. Even though the samples for both procedures exceeded 30, I used a t score instead of a z score because the sample standard deviations were different and the population standard deviation was unspecified. In order to figure out the correct t score, I used qt() with p set to 0.95 because each tail has an area of 0.05 when the confidence level is 90%. Once I had the t scores, I determined the margin of error and confidence intervals. To figure out which procedure has the narrower confidence interval, I subtracted the upper bound value by the lower bound value for each respective procedure. The angiography has a narrower confidence interval than the bypass.\n\n\n##QUESTION 2##\n#Sample Size\nSample=1031\n\n#Pro-College vs Anti-College Proportions\nPro_College=567/Sample\nAnti_College=1-Pro_College\n\n#Z Score, Standard Error, And Margin Of Error\nZ=1.96\nSE=sqrt((Pro_College*Anti_College)/Sample)\nMOE=Z*SE\n\n#Confidence Interval\nLower_Bound=Pro_College-MOE\nUpper_Bound=Pro_College+MOE\nConfidence_Interval <- c(Lower_Bound,Upper_Bound)\nConfidence_Interval \n\n\n[1] 0.5195833 0.5803197\n\nThe 95% confidence interval for p is 51.95833-58.03197%. Based on the fact that 567/1031 adults in the survey believed that college education is essential for success, I was able to conclude that 464/1031 adults in the survey disagreed with that mindset. From there, I used sqrt((Pro College Proportion*Anti College Proportion)/Sample Size) to find the standard error since I don’t need a standard deviation to find the standard error of a proportion. Next, I multiplied the standard error by 1.96, otherwise known as the 95% confidence level z score, to find the margin of error. The reason why I used a z score instead of a t score is because the instructions specify that I should assume that the results are reflective of the entire population. Finally, I subtracted the margin of error from the pro-college proportion to find the lower bound confidence interval limit and I added the margin of error to the pro-college proportion to find the upper bound confidence interval limit.\n\n\n##QUESTION 3##\n#Determining Standard Deviation\nRange=200-30\nSTD=Range/4\nSTD\n\n\n[1] 42.5\n\n#Determnining Margin Of Error And Z Score\nM=5\nZ=1.96\n\n#Determnining Sample Size\nZ_Div_M=Z/M\nSize=(STD*STD)*(Z_Div_M*Z_Div_M)\nSize\n\n\n[1] 277.5556\n\n#Checking Work\nM_Check=Z*(STD/sqrt(278))\nM_Check\n\n\n[1] 4.996002\n\nThe ideal sample size for an experiment with the specifications of question 3 is at least 278. Before figuring out the ideal sample size, I had to find the standard deviation and the margin of error. Based on the question 3 instructions, I could conclude that the standard deviation is 42.5 the spending range is 170 (200-30) and the population standard deviation is the spending range divided by 4. I could also conclude that the margin of error is 5 because the confidence interval length should be 10 dollars or less and the margin of error is half the confidence interval length. In order to determine the ideal sample size, I first divided the margin of error by 1.96, otherwise known as the 5% significance level z score. I used a z score instead of a t score because the population standard deviation is known. From there, I multiplied the standard deviation squared by (Margin of Error/Z Score) squared and got 277.5556, which can be rounded up to 278. To check my work, I used the M=Z*(STD/sqrt(N)).\n\n\n##QUESTION 4##\n#Mean And Standard Deviation\nPopulation_Mean=500\nWomen=9\nWoman_Mean=410\nSTD=90\nSE=STD/sqrt(Women)\n\n#T-Score\nT_Score=(Woman_Mean-Population_Mean)/SE\nT_Score\n\n\n[1] -3\n\n#A: Find Two Tail P Value\nTwo_Tail_P=2*pt(q=T_Score,df=(Women-1))\nTwo_Tail_P\n\n\n[1] 0.01707168\n\n#B: Left Tail P Value\nLeft_Tail_P=pt(q=T_Score,df=(Women-1),lower.tail=TRUE)\nLeft_Tail_P\n\n\n[1] 0.008535841\n\n#C: Right Tail P Value\nRight_Tail_P=pt(q=T_Score,df=(Women-1),lower.tail=FALSE)\nRight_Tail_P\n\n\n[1] 0.9914642\n\nFor question 4 part A, I concluded that there is a statistically significant difference between the income of women and the mean income. The null hypothesis is that the income of women is equal to that of everyone else. The alternative hypothesis is that hypothesis is that the income of women isn’t equal to that of everyone else. In order to figure out whether there is a statistically significant difference, I had to figure out the T-Score. The T-Score I got from dividing the difference between the women’s mean and the population mean by the standard error is -3. Even though I know that a T-Score of -3 is probably reflective of statistically significant differences, I still determined the two tailed p value by using 2*pt(q=T_Score,df=(Women-1)). I multiplied pt(q=T_Score,df=(Women-1)) by 2 because the pt() function is used for determining 1 tail p values and I was interested in finding the 2 tail p value. The p value I got was p=0.01707168, which is considered statistically significant when using the p<0.05 significance level. To find the left tail P value, I used pt(q=T_Score,df=(Amount of Women-1),lower.tail=TRUE) and I got p=0.008535841, which supports the alternative hypothesis of mu<500 to a highly significant extent. To find the right tail P value, I used Right_Tail_P=pt(q=T_Score,df=(Women-1),lower.tail=FALSE) and I got 0.9914642, which is indicative of mu>500 being false.\n\n\n##QUESTION 5##\n#Null Mean And Sample Size\nNull_Mean=500\nN=1000\n\n#Jones Mean And Standard Error\nJones_Mean=519.5\nJones_SE=10.0\n\n#Jones T\nJones_T=(Jones_Mean-Null_Mean)/Jones_SE\nJones_T\n\n\n[1] 1.95\n\n#Jones P\nJones_P=2*pt(q=Jones_T,df=(N-1),lower.tail=FALSE)\nJones_P\n\n\n[1] 0.05145555\n\n#Smith Mean And Standard Error\nSmith_Mean=519.7\nSmith_SE=10.0\n\n#Smith T\nSmith_T=(Smith_Mean-Null_Mean)/Jones_SE\nSmith_T\n\n\n[1] 1.97\n\n#Smith P\nSmith_P=2*pt(q=Smith_T,df=(N-1),lower.tail=FALSE)\nSmith_P\n\n\n[1] 0.04911426\n\nAccording to alpha=0.05, Smith’s results are statistically significant, but Jones’s results are not. The reason why neglecting to report the p value is so misleading when reporting results is because there is no indication of how close the results are to being statistically significant. Jones’s p value of 0.05145555 comes extremely close to being statistically significant when the significance level is set to p<=0.05 and if the significance level was set to p<=0.1, Jones’s results would undoubtedly be classified as statistically significant. If I say Jones’s results are not statistically significant with p<=0.05 and that I can’t reject the null hypothesis, nobody will know that Jones’s results come extremely close to being statistically significance and that could lead to type 2 errors.\n\n\n##QUESTION 6##\n#Dataset\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n#T-Test\nt.test(gas_taxes, mu=45, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\nEven though the sample size is only 18, there is enough evidence to conclude that the gas prices in 2005 were less than 45 cents with a 95% confidence interval. Given that the data is provided, I was able to use the t test function to figure out the answer. In this context, the null hypothesis is that the average tax per gallon is equal to 45 cents, so I set mu to mu=45. For the alternative= component, I used alternative=“less” because the alternative hypothesis is that the true mean of the gas prices is less than 45 cents. When I ran the t-test, I got a p value of 0.03827, which is considered statistically significant because a 95% confidence interval calls for a p value that is less than or equal to 0.05.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:25:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to DACSS 601",
    "description": "Welcome to DACSS 603! We hope you enjoy the class!",
    "author": [
      {
        "name": "Omer Yalcin",
        "url": "http://umass.edu/sbs/dacss"
      }
    ],
    "date": "2022-02-24",
    "categories": [
      "welcome"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-24T15:14:40-05:00",
    "input_file": {}
  }
]
